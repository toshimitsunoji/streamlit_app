# -*- coding: utf-8 -*-
"""
ã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ« + Outlookã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« é›†ä¸­ãƒ»ç–²åŠ´äºˆæ¸¬ã‚¢ãƒ—ãƒª (Streamlitç‰ˆ)
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import lightgbm as lgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score, log_loss
from sklearn.model_selection import TimeSeriesSplit
import google.generativeai as genai
import shap
import warnings
import math
import plotly.graph_objects as go
import plotly.express as px

# --- Streamlit ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="é›†ä¸­ãƒ»ç–²åŠ´äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ", layout="wide")

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
try:
    import japanize_matplotlib
    japanize_matplotlib.japanize()
except ImportError:
    st.warning("âš ï¸ japanize-matplotlib ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã€‚ã‚°ãƒ©ãƒ•ã®æ—¥æœ¬èªãŒæ–‡å­—åŒ–ã‘ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚GitHubä¸Šã§ `requirements.txt` ãŒ `streamlit_app.py` ã¨å…¨ãåŒã˜ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆç¬¬ä¸€éšå±¤ï¼‰ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

warnings.filterwarnings('ignore')

st.title("ğŸ§  ã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ«Ã—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« é›†ä¸­äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ")
st.markdown("""
ã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ«ãƒ‡ãƒã‚¤ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã¨äºˆå®šè¡¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ„ã¿åˆã‚ã›ã¦ã€æ•°æ™‚é–“å¾Œã®é›†ä¸­ã‚¹ã‚³ã‚¢ã‚’äºˆæ¸¬ã—ã€æ¨å¥¨ã™ã‚‹åƒãæ–¹ã‚’ææ¡ˆã—ã¾ã™ã€‚
""")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ (è¨­å®š) ---
st.sidebar.header("âš™ï¸ è¨­å®š")
api_key = st.sidebar.text_input("Gemini APIã‚­ãƒ¼ (çœç•¥æ™‚ã¯å›ºå®šãƒ«ãƒ¼ãƒ«ã§å‡ºåŠ›)", type="password")

st.sidebar.subheader("åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
RESAMPLE_FREQ = st.sidebar.selectbox("åˆ†æå˜ä½", ['10T', '30T', '1H'], index=1)
PREDICT_AHEAD = st.sidebar.selectbox("äºˆæ¸¬å…ˆ", ['10T', '30T', '1H'], index=1)
LOOKBACK_PERIOD = st.sidebar.selectbox("éå»å‚ç…§", ['1H', '2H', '3H'], index=1)
INTERPOLATE_LIMIT = st.sidebar.selectbox("è£œå®Œé™ç•Œ", ['10T', '30T', '1H'], index=1)

st.sidebar.subheader("äºˆæ¸¬åŸºæº–æ—¥æ™‚ (ä»»æ„)")
TARGET_DATETIME_STR = st.sidebar.text_input("ä¾‹ï¼‰2026-01-01 16:00 (ç©ºæ¬„ã§æœ€æ–°ãƒ‡ãƒ¼ã‚¿)")
TARGET_DATETIME = TARGET_DATETIME_STR if TARGET_DATETIME_STR.strip() != "" else None

freq_td = pd.Timedelta(RESAMPLE_FREQ)
ahead_td = pd.Timedelta(PREDICT_AHEAD)
lookback_td = pd.Timedelta(LOOKBACK_PERIOD)
interp_td = pd.Timedelta(INTERPOLATE_LIMIT)

ahead_steps = max(1, int(ahead_td / freq_td))
lookback_steps = max(2, int(lookback_td / freq_td))
interp_steps = max(1, int(interp_td / freq_td))

# --- äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®é¸æŠæ©Ÿèƒ½ã‚’è¿½åŠ  ---
TARGET_OPTIONS = {
    'é›†ä¸­åˆ¤å®š': 'é›†ä¸­åˆ¤å®š(1=é›†ä¸­)',
    'ç–²åŠ´åˆ¤å®š': 'ç–²åŠ´åˆ¤å®š(1=ç–²åŠ´)'
}
st.sidebar.subheader("ğŸ¯ äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ")
selected_target_name = st.sidebar.selectbox("äºˆæ¸¬ã™ã‚‹æŒ‡æ¨™ã‚’é¸æŠ", list(TARGET_OPTIONS.values()), index=0)
target_col = [k for k, v in TARGET_OPTIONS.items() if v == selected_target_name][0]

# --- æ–°è¦: é•·æœŸåˆ†æç”¨ãƒ•ã‚£ãƒ«ã‚¿ ---
st.sidebar.subheader("ğŸ“… é•·æœŸåˆ†æãƒ•ã‚£ãƒ«ã‚¿ (ç‰¹æ€§ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”¨)")
dow_options = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]
selected_dows = st.sidebar.multiselect("å¯¾è±¡æ›œæ—¥", dow_options, default=dow_options)
time_range = st.sidebar.slider("å¯¾è±¡æ™‚é–“å¸¯", 0, 23, (9, 19))

# --- ç‰¹å¾´é‡åæ—¥æœ¬èªåŒ–ãƒ˜ãƒ«ãƒ‘ãƒ¼ ---
def jp_feat_name(col_name: str) -> str:
    mapping = {
        'CVRR_SCORE_NEW': 'é›†ä¸­ã‚¹ã‚³ã‚¢', 'SkinTemp': 'çš®è†šæ¸©åº¦', 'LP_SCORE_NEW': 'ãƒªãƒ©ãƒƒã‚¯ã‚¹ã‚¹ã‚³ã‚¢',
        'LFHF_SCORE_NEW': 'LF/HF(è‡ªå¾‹ç¥çµŒãƒãƒ©ãƒ³ã‚¹)', 'TP': 'TP(è‡ªå¾‹ç¥çµŒãƒˆãƒ¼ã‚¿ãƒ«ãƒ‘ãƒ¯ãƒ¼)', 'NEMUKE_SCORE_NEW': 'ä½è¦šé†’ã‚¹ã‚³ã‚¢',
        'PR_SCORE_NEW': 'è„ˆæ‹', 'RMSSD_SCORE_NEW': 'ç–²åŠ´ãƒ»å›å¾©ã‚¹ã‚³ã‚¢', '1åˆ†é–“æ­©æ•°': 'æ­©æ•°', 'accDeviation': 'æ´»å‹•é‡(åŠ é€Ÿåº¦)',
        'has_schedule': 'äºˆå®šã®æœ‰ç„¡', 'is_meeting': 'ä¼šè­°ä¸­ã‹ã©ã†ã‹', 'schedule_density_2h': 'æœ€è¿‘ã®äºˆå®šã®è©°ã¾ã‚Šå…·åˆ',
        'time_to_next_event_min': 'æ¬¡ã®äºˆå®šã¾ã§ã®æ™‚é–“', 'time_since_prev_event_min': 'å‰ã®äºˆå®šã‹ã‚‰ã®çµŒéæ™‚é–“',
        'daily_schedule_hours': '1æ—¥ã®ç·äºˆå®šæ™‚é–“', 'consecutive_schedules': 'é€£ç¶šäºˆå®šãƒ–ãƒ­ãƒƒã‚¯æ•°',
        'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°', 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“_åˆ†': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“(åˆ†)',
        'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“(åˆ†)', 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“(åˆ†)',
        'é›†ä¸­åˆ¤å®š': 'é›†ä¸­åˆ¤å®š', 'ç–²åŠ´åˆ¤å®š': 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š': 'å¼·ã„ç–²åŠ´åˆ¤å®š',
        'é›†ä¸­çŠ¶æ…‹': 'é›†ä¸­çŠ¶æ…‹', 'çœ æ°—çŠ¶æ…‹': 'çœ æ°—çŠ¶æ…‹', 'ç–²åŠ´çŠ¶æ…‹': 'ç–²åŠ´çŠ¶æ…‹',
        'é›†ä¸­ç¶™ç¶šæ™‚é–“': 'é›†ä¸­ç¶™ç¶šæ™‚é–“', 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“': 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“',
        'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'ç–²åŠ´ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“': 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“'
    }
    
    base_jp = col_name
    remainder = ""
    for k, v in mapping.items():
        if col_name.startswith(k):
            base_jp = v
            remainder = col_name[len(k):]
            break
            
    if remainder == "": return base_jp
    elif remainder == "_roll_mean": return f"æœ€è¿‘ã®ã€Œ{base_jp}ã€ã®å¹³å‡çš„ãªé«˜ã•"
    elif remainder == "_roll_slope": return f"æœ€è¿‘ã®ã€Œ{base_jp}ã€ã®æ€¥ãªå¤‰åŒ–(ãƒˆãƒ¬ãƒ³ãƒ‰)"
    elif remainder == "_diff1": return f"å‰å›ã‹ã‚‰ã®ã€Œ{base_jp}ã€ã®å¤‰å‹•å¹…"
    elif remainder.startswith("_lag"): return f"å°‘ã—å‰ã®ã€Œ{base_jp}ã€ã®çŠ¶æ…‹"
    elif remainder == "_is_missing": return f"ã€Œ{base_jp}ã€ãŒæœªè¨ˆæ¸¬ã§ã‚ã‚‹ã“ã¨"
    else: return f"{base_jp}{remainder}"

def get_base_feature_name(feat: str) -> str:
    mapping = {
        'CVRR_SCORE_NEW': 'é›†ä¸­ã‚¹ã‚³ã‚¢', 'SkinTemp': 'çš®è†šæ¸©åº¦', 'LP_SCORE_NEW': 'ãƒªãƒ©ãƒƒã‚¯ã‚¹ã‚¹ã‚³ã‚¢',
        'LFHF_SCORE_NEW': 'LF/HF(è‡ªå¾‹ç¥çµŒãƒãƒ©ãƒ³ã‚¹)', 'TP': 'TP(è‡ªå¾‹ç¥çµŒãƒˆãƒ¼ã‚¿ãƒ«ãƒ‘ãƒ¯ãƒ¼)', 'NEMUKE_SCORE_NEW': 'ä½è¦šé†’ã‚¹ã‚³ã‚¢',
        'PR_SCORE_NEW': 'è„ˆæ‹', 'RMSSD_SCORE_NEW': 'ç–²åŠ´ãƒ»å›å¾©ã‚¹ã‚³ã‚¢', '1åˆ†é–“æ­©æ•°': 'æ­©æ•°', 'accDeviation': 'æ´»å‹•é‡(åŠ é€Ÿåº¦)',
        'has_schedule': 'äºˆå®š', 'is_meeting': 'ä¼šè­°', 'schedule_density_2h': 'äºˆå®šã®å¯†åº¦',
        'time_to_next_event_min': 'æ¬¡ã®äºˆå®šã¾ã§ã®æ™‚é–“', 'time_since_prev_event_min': 'å‰ã®äºˆå®šã‹ã‚‰ã®çµŒéæ™‚é–“',
        'daily_schedule_hours': '1æ—¥ã®ç·äºˆå®šæ™‚é–“', 'consecutive_schedules': 'é€£ç¶šäºˆå®šãƒ–ãƒ­ãƒƒã‚¯æ•°',
        'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°', 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“_åˆ†': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“',
        'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“', 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“',
        'é›†ä¸­åˆ¤å®š': 'é›†ä¸­åˆ¤å®š', 'ç–²åŠ´åˆ¤å®š': 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š': 'å¼·ã„ç–²åŠ´åˆ¤å®š',
        'é›†ä¸­çŠ¶æ…‹': 'é›†ä¸­çŠ¶æ…‹', 'çœ æ°—çŠ¶æ…‹': 'çœ æ°—çŠ¶æ…‹', 'ç–²åŠ´çŠ¶æ…‹': 'ç–²åŠ´çŠ¶æ…‹',
        'é›†ä¸­ç¶™ç¶šæ™‚é–“': 'é›†ä¸­ç¶™ç¶šæ™‚é–“', 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“': 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“',
        'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'ç–²åŠ´ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“': 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“'
    }
    for k, v in mapping.items():
        if feat.startswith(k): return v
    return feat

def get_factor_direction_text(feat: str, val: float, df_all: pd.DataFrame) -> str:
    base_jp = get_base_feature_name(feat)
    remainder = feat[len([k for k,v in {**locals()}.items() if base_jp == v] or [feat])[0]:] if base_jp != feat else ""
    
    if "_is_missing" in feat: return f"ã€Œ{base_jp}ã€ãŒæœªè¨ˆæ¸¬ã§ã‚ã‚‹ã“ã¨"
    elif feat in ["has_schedule", "is_meeting"]: return f"ã€Œ{base_jp}ã€ãŒå…¥ã£ã¦ã„ã‚‹ã“ã¨" if val > 0 else f"ã€Œ{base_jp}ã€ãŒå…¥ã£ã¦ã„ãªã„ã“ã¨"
    elif feat in ["é›†ä¸­çŠ¶æ…‹", "çœ æ°—çŠ¶æ…‹", "ç–²åŠ´çŠ¶æ…‹"]: return f"ã€Œ{base_jp}ã€ãŒã€Œ{val}ã€ã§ã‚ã‚‹ã“ã¨"
            
    if "_roll_slope" in feat or "_diff1" in feat:
        direction = "ã®å¢—åŠ " if val > 0 else "ã®ä½ä¸‹" if val < 0 else "ã®å¤‰åŒ–ãªã—"
        return f"æœ€è¿‘ã®ã€Œ{base_jp}ã€ã®ãƒˆãƒ¬ãƒ³ãƒ‰{direction}" if "_roll_slope" in feat else f"å‰å›ã‹ã‚‰ã®ã€Œ{base_jp}ã€{direction}"
    else:
        if "_roll_mean" in feat: return f"æœ€è¿‘ã®ã€Œ{base_jp}ã€"
        elif "_lag" in feat: return f"å°‘ã—å‰ã®ã€Œ{base_jp}ã€"
        else: return f"ã€Œ{base_jp}ã€"

# --- åˆ†æãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def run_analysis(df_ts, df_sched, use_gemini=False):
    with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ãƒ»å‰å‡¦ç†ã—ã¦ã„ã¾ã™..."):
        if 'timestamp' in df_ts.columns:
            df_ts['timestamp_clean'] = df_ts['timestamp'].astype(str).str.split(' GMT').str[0]
            df_ts['datetime'] = pd.to_datetime(df_ts['timestamp_clean'], errors='coerce')
            df_ts = df_ts.dropna(subset=['datetime'])
            df_ts.set_index('datetime', inplace=True)
            df_ts.drop(columns=['timestamp', 'timestamp_clean'], inplace=True, errors='ignore')
            df_ts = df_ts.sort_index()

        base_agg_dict = {
            'SkinTemp': 'mean', 'CVRR_SCORE_NEW': 'mean', 'LP_SCORE_NEW': 'mean',
            'LFHF_SCORE_NEW': 'mean', 'TP': 'mean', 'NEMUKE_SCORE_NEW': 'mean',
            'PR_SCORE_NEW': 'mean', 'RMSSD_SCORE_NEW': 'mean', '1åˆ†é–“æ­©æ•°': 'sum', 'accDeviation': 'mean',
            'é›†ä¸­åˆ¤å®š': 'mean', 'ç–²åŠ´åˆ¤å®š': 'mean', 'å¼·ã„ç–²åŠ´åˆ¤å®š': 'mean',
            'é›†ä¸­ç¶™ç¶šæ™‚é–“': 'mean', 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“': 'mean', 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“': 'mean', 'ç–²åŠ´ç¶™ç¶šæ™‚é–“': 'mean', 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“': 'mean'
        }
        
        cat_agg_dict = {'é›†ä¸­çŠ¶æ…‹': 'last', 'çœ æ°—çŠ¶æ…‹': 'last', 'ç–²åŠ´çŠ¶æ…‹': 'last'}
        
        agg_dict = {col: func for col, func in base_agg_dict.items() if col in df_ts.columns}
        for col, func in cat_agg_dict.items():
            if col in df_ts.columns: agg_dict[col] = func
        
        if target_col not in agg_dict:
            st.error(f"ã‚¨ãƒ©ãƒ¼: äºˆæ¸¬ã«å¿…è¦ãªç›®çš„å¤‰æ•°ã€Œ{selected_target_name}ã€ãŒãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return

        for col in agg_dict.keys():
            if col in base_agg_dict.keys():
                df_ts[col] = pd.to_numeric(df_ts[col], errors='coerce')

        df_resampled = df_ts.resample(RESAMPLE_FREQ).agg(agg_dict)

        if df_sched is not None:
            df_sched = df_sched[df_sched['çµ‚æ—¥ã‚¤ãƒ™ãƒ³ãƒˆ'].astype(str).str.upper() != 'TRUE']
            df_sched['start_dt'] = pd.to_datetime(df_sched['é–‹å§‹æ—¥'].astype(str) + ' ' + df_sched['é–‹å§‹æ™‚åˆ»'].astype(str), errors='coerce')
            df_sched['end_dt']   = pd.to_datetime(df_sched['çµ‚äº†æ—¥'].astype(str) + ' ' + df_sched['çµ‚äº†æ™‚åˆ»'].astype(str), errors='coerce')
            df_sched = df_sched.dropna(subset=['start_dt', 'end_dt']).sort_values('start_dt')

            df_resampled['has_schedule'] = 0
            df_resampled['is_meeting'] = 0
            meeting_keywords = ['ä¼šè­°', 'æ‰“åˆã›', 'æ‰“ã¡åˆã‚ã›', 'MTG', 'é¢è«‡', 'å•†è«‡', 'æ¥å®¢', 'è¨ªå•']

            for _, row in df_sched.iterrows():
                mask = (df_resampled.index < row['end_dt']) & ((df_resampled.index + freq_td) > row['start_dt'])
                df_resampled.loc[mask, 'has_schedule'] = 1
                subject = str(row.get('ä»¶å', ''))
                if any(kw in subject for kw in meeting_keywords):
                    df_resampled.loc[mask, 'is_meeting'] = 1

            s = df_resampled['has_schedule']
            df_resampled['consecutive_schedules'] = s.groupby((s != s.shift()).cumsum()).cumsum()

            df_resampled['date'] = df_resampled.index.date
            df_resampled = df_resampled.join(df_resampled.groupby('date')['has_schedule'].sum().rename('daily_schedule_hours'), on='date').fillna({'daily_schedule_hours': 0})
            df_resampled.drop(columns=['date'], inplace=True)

            event_starts = df_sched['start_dt'].to_numpy(dtype='datetime64[ns]')
            event_ends   = df_sched['end_dt'].to_numpy(dtype='datetime64[ns]')
            t = df_resampled.index.to_numpy(dtype='datetime64[ns]')

            next_start_idx = np.searchsorted(event_starts, t, side='left')
            has_next = next_start_idx < len(event_starts)
            next_idx_safe = np.clip(next_start_idx, 0, max(len(event_starts) - 1, 0))
            next_start = np.full(t.shape, np.datetime64('NaT'), dtype='datetime64[ns]')
            if len(event_starts) > 0: next_start[has_next] = event_starts[next_idx_safe[has_next]]

            prev_end_idx = np.searchsorted(event_ends, t, side='right') - 1
            has_prev = prev_end_idx >= 0
            prev_idx_safe = np.clip(prev_end_idx, 0, max(len(event_ends) - 1, 0))
            prev_end = np.full(t.shape, np.datetime64('NaT'), dtype='datetime64[ns]')
            if len(event_ends) > 0: prev_end[has_prev] = event_ends[prev_idx_safe[has_prev]]

            df_resampled['time_to_next_event_min'] = (next_start - t) / np.timedelta64(1, 'm')
            df_resampled['time_since_prev_event_min'] = (t - prev_end) / np.timedelta64(1, 'm')

            win_steps = max(1, int(pd.Timedelta('2H') / freq_td))
            df_resampled['schedule_density_2h'] = df_resampled['has_schedule'].rolling(win_steps, min_periods=1).mean()

    with st.spinner("ç‰¹å¾´é‡ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™..."):
        df_features = df_resampled.copy()
        df_features['hour'] = df_features.index.hour.astype('category')
        df_features['dayofweek'] = df_features.index.dayofweek.astype('category')
        
        for c in ['é›†ä¸­çŠ¶æ…‹', 'çœ æ°—çŠ¶æ…‹', 'ç–²åŠ´çŠ¶æ…‹']:
            if c in df_features.columns: df_features[c] = df_features[c].astype('category')
        
        numeric_cols = df_resampled.select_dtypes(include=[np.number]).columns.tolist()
        win = lookback_steps
        x = np.arange(win, dtype=float)
        x_mean = x.mean()
        x_var = ((x - x_mean) ** 2).sum()
        def rolling_slope(arr):
            y = arr.astype(float)
            if x_var == 0: return 0.0
            return ((x - x_mean) * (y - y.mean())).sum() / x_var

        for col in numeric_cols:
            df_features[f'{col}_is_missing'] = df_resampled[col].isna().astype(int)
            r = df_features[col].rolling(win, min_periods=win)
            df_features[f'{col}_roll_mean'] = r.mean()
            df_features[f'{col}_roll_slope'] = r.apply(rolling_slope, raw=True)
            df_features[f'{col}_diff1'] = df_features[col] - df_features[col].shift(1)

        df_features['date'] = df_features.index.date
        if '1åˆ†é–“æ­©æ•°' in df_features.columns:
            df_features['ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°'] = df_features.groupby('date')['1åˆ†é–“æ­©æ•°'].cumsum()
        if 'is_meeting' in df_features.columns:
            df_features['ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“_åˆ†'] = df_features.groupby('date')['is_meeting'].cumsum() * (freq_td.total_seconds() / 60)
        
        def calc_duration_mins(series):
            group_id = (series != series.shift()).cumsum()
            return (series.groupby(group_id).cumcount() + 1) * (freq_td.total_seconds() / 60)
            
        if 'é›†ä¸­åˆ¤å®š' in df_features.columns:
            focus_mask = (df_features['é›†ä¸­åˆ¤å®š'] >= 0.5).astype(int)
            df_features['ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“_åˆ†'] = calc_duration_mins(focus_mask) * focus_mask
        if 'ç–²åŠ´åˆ¤å®š' in df_features.columns:
            fatigue_mask = (df_features['ç–²åŠ´åˆ¤å®š'] >= 0.5).astype(int)
            df_features['ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“_åˆ†'] = calc_duration_mins(fatigue_mask) * fatigue_mask
            
        df_features.drop(columns=['date'], inplace=True)

        target_threshold = 0.5 if target_col in ['é›†ä¸­åˆ¤å®š', 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š'] else df_features[target_col].median()
        df_features['target_ahead_class'] = (df_features[target_col].shift(-ahead_steps) >= target_threshold).astype(int)

    with st.spinner("LightGBMåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¦ã„ã¾ã™..."):
        drop_cols = ['target_ahead_class']
        df_all = df_features.copy()
        split_idx = int(len(df_all) * 0.8)
        
        df_imp = df_all.copy()
        for col in df_imp.columns:
            if col not in drop_cols:
                df_imp[col] = df_imp[col].ffill(limit=interp_steps).bfill(limit=interp_steps)
        
        train_df = df_imp.iloc[:split_idx].dropna(subset=drop_cols + [target_col])
        test_df  = df_imp.iloc[split_idx:].dropna(subset=drop_cols + [target_col])

        X_train = train_df.drop(columns=drop_cols)
        y_train_class = train_df['target_ahead_class']
        X_test  = test_df.drop(columns=drop_cols)
        y_test_class = test_df['target_ahead_class']
        
        cat_cols = [c for c in X_train.columns if str(X_train[c].dtype) == 'category']
        model = lgb.LGBMClassifier(objective='binary', n_estimators=500, learning_rate=0.03, random_state=42)
        model.fit(X_train, y_train_class, categorical_feature=cat_cols if cat_cols else 'auto')
        preds_proba = model.predict_proba(X_test)[:, 1]
        try:
            auc_test = roc_auc_score(y_test_class, preds_proba)
            logloss_test = log_loss(y_test_class, preds_proba)
        except ValueError:
            auc_test = np.nan
            logloss_test = np.nan


    # =========================================================================
    # ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ç‰¹æ€§ã‚¤ãƒ³ã‚µã‚¤ãƒˆï¼ˆé•·æœŸãƒ»æœˆæ¬¡ãƒ»æ—¥æ¬¡ï¼‰
    # =========================================================================
    st.header("ğŸ‘¤ ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ç‰¹æ€§ã‚¤ãƒ³ã‚µã‚¤ãƒˆ")
    
    # å…±é€šãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
    df_insight = df_imp.copy()
    selected_dow_indices = [dow_options.index(d) for d in selected_dows]
    df_insight = df_insight[df_insight.index.dayofweek.isin(selected_dow_indices)]
    df_insight = df_insight[(df_insight.index.hour >= time_range[0]) & (df_insight.index.hour <= time_range[1])]

    if 'é›†ä¸­åˆ¤å®š' in df_insight.columns:
        df_insight['focus_start'] = (df_insight['é›†ä¸­åˆ¤å®š'] >= 0.5) & (df_insight['é›†ä¸­åˆ¤å®š'].shift(1) < 0.5)
    if 'ç–²åŠ´åˆ¤å®š' in df_insight.columns:
        df_insight['fatigue_start'] = (df_insight['ç–²åŠ´åˆ¤å®š'] >= 0.5) & (df_insight['ç–²åŠ´åˆ¤å®š'].shift(1) < 0.5)

    def get_peak_time(metric_col):
        if metric_col not in df_insight.columns: return "ä¸æ˜", "ä¸æ˜"
        pivot_df = df_insight.pivot_table(values=metric_col, index=df_insight.index.hour, columns=df_insight.index.dayofweek, aggfunc='mean')
        daytime_pivot = pivot_df.loc[time_range[0]:time_range[1], selected_dow_indices]
        if not daytime_pivot.isna().all().all():
            best_hour, best_dow = daytime_pivot.stack().idxmax()
            return dow_options[int(best_dow)], str(int(best_hour))
        return "ä¸æ˜", "ä¸æ˜"

    f_dow, f_hour = get_peak_time('é›†ä¸­åˆ¤å®š')
    fat_dow, fat_hour = get_peak_time('ç–²åŠ´åˆ¤å®š')

    avg_focus_duration_str = "ä¸æ˜"
    daily_focus_count_str = "ä¸æ˜"
    focus_durations = pd.Series(dtype=float)
    
    if 'é›†ä¸­åˆ¤å®š' in df_ts.columns:
        df_1min = df_ts[['é›†ä¸­åˆ¤å®š']].resample('1T').mean().ffill(limit=5)
        df_1min = df_1min[df_1min.index.dayofweek.isin(selected_dow_indices)]
        df_1min = df_1min[(df_1min.index.hour >= time_range[0]) & (df_1min.index.hour <= time_range[1])]
        
        focus_mask = df_1min['é›†ä¸­åˆ¤å®š'] >= 0.5
        focus_blocks = focus_mask.groupby((focus_mask != focus_mask.shift()).cumsum())
        focus_durations = focus_blocks.sum() 
        focus_durations = focus_durations[focus_durations > 0]
        
        if not focus_durations.empty:
            avg_focus_duration_str = f"{focus_durations.mean():.0f}"
            total_focus_count = len(focus_durations)
            num_days = df_1min.index.normalize().nunique()
            daily_focus_count_str = f"{(total_focus_count / num_days if num_days > 0 else 0):.1f}"

    focus_actions = []
    if '1åˆ†é–“æ­©æ•°' in df_insight.columns and 'focus_start' in df_insight.columns:
        walk_before_focus = df_insight['1åˆ†é–“æ­©æ•°'].shift(1)[df_insight['focus_start']].dropna()
        avg_walk_overall = df_insight['1åˆ†é–“æ­©æ•°'].mean()
        if not walk_before_focus.empty and avg_walk_overall > 0:
            avg_walk_before = walk_before_focus.mean()
            if avg_walk_before > avg_walk_overall * 1.2: focus_actions.append("äº‹å‰ã«ä½“ã‚’å‹•ã‹ã™ã“ã¨ï¼ˆå°‘ã—æ­©ããªã©ï¼‰")
            elif avg_walk_before < avg_walk_overall * 0.8: focus_actions.append("äº‹å‰ã«é™ã‹ãªç’°å¢ƒã§è½ã¡ç€ã„ã¦éã”ã™ã“ã¨")

    if 'has_schedule' in df_insight.columns and 'é›†ä¸­åˆ¤å®š' in df_insight.columns:
        sched_mask = df_insight['has_schedule'] >= 0.5
        sched_blocks = (sched_mask != sched_mask.shift()).cumsum()
        sched_df = df_insight[sched_mask]
        focus_scores_rested, focus_scores_rushed = [], []
        for _, group in sched_df.groupby(sched_blocks):
            if len(group) > 1 and 'time_since_prev_event_min' in group.columns:
                rest_before = group['time_since_prev_event_min'].iloc[0]
                if not np.isnan(rest_before):
                    if rest_before >= 30: focus_scores_rested.append(group['é›†ä¸­åˆ¤å®š'].mean())
                    else: focus_scores_rushed.append(group['é›†ä¸­åˆ¤å®š'].mean())
        if focus_scores_rested and focus_scores_rushed:
            diff_focus = (np.mean(focus_scores_rested) - np.mean(focus_scores_rushed)) * 100
            if diff_focus > 0: focus_actions.append("äºˆå®šã®å‰ã«30åˆ†ä»¥ä¸Šã®ç©ºãæ™‚é–“ï¼ˆä¼‘æ†©ï¼‰ã‚’ã¨ã‚‹ã“ã¨")
            elif diff_focus < 0: focus_actions.append("äºˆå®šã¨äºˆå®šã®é–“ã‚’ç©ºã‘ãšã«é€£ç¶šã—ã¦æ´»å‹•ã™ã‚‹ã“ã¨")

    focus_actions_str = "ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ç‰¹å®šã§ãã¾ã›ã‚“" if not focus_actions else "ã€".join(focus_actions)

    fatigue_actions = []
    if 'ç–²åŠ´åˆ¤å®š' in df_insight.columns and 'has_schedule' in df_insight.columns:
        sched_mask = df_insight['has_schedule'] >= 0.5
        sched_blocks = (sched_mask != sched_mask.shift()).cumsum()
        sched_df = df_insight[sched_mask]
        fatigue_diffs = []
        for _, group in sched_df.groupby(sched_blocks):
            if len(group) > 1:
                duration_hours = len(group) * (freq_td.total_seconds() / 3600)
                if duration_hours > 0:
                    fatigue_diffs.append((group['ç–²åŠ´åˆ¤å®š'].iloc[-1] - group['ç–²åŠ´åˆ¤å®š'].iloc[0]) / duration_hours)
        if fatigue_diffs and np.mean(fatigue_diffs) > 0:
            fatigue_actions.append("1æ™‚é–“ä»¥ä¸Šã®äºˆå®šã‚’ã“ãªã™ã“ã¨")

    if 'fatigue_start' in df_insight.columns and 'focus_start' in df_insight.columns:
        recovery_consecutive, recovery_single = [], []
        fatigue_times, focus_times = df_insight[df_insight['fatigue_start']].index, df_insight[df_insight['focus_start']].index
        for fat_time in fatigue_times:
            future_focus = focus_times[focus_times > fat_time]
            if len(future_focus) > 0 and future_focus[0].date() == fat_time.date():
                if 'consecutive_schedules' in df_insight.columns:
                    if df_insight.loc[fat_time, 'consecutive_schedules'] >= 2: recovery_consecutive.append(1)
                    else: recovery_single.append(1)
        if recovery_consecutive and recovery_single and (np.mean(recovery_consecutive) - np.mean(recovery_single)) > 0:
            fatigue_actions.append("äºˆå®šã‚’é€£ç¶šã—ã¦å…¥ã‚Œã‚‹ã“ã¨")

    fatigue_actions_str = "ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ç‰¹å®šã§ãã¾ã›ã‚“" if not fatigue_actions else "ã€".join(fatigue_actions)

    # --- 3ã¤ã®ã‚¿ãƒ–ã‚’ä½œæˆ ---
    tab1, tab2, tab3 = st.tabs(["ğŸ“ ãƒã‚¤ãƒ»ã‚¹ãƒšãƒƒã‚¯", "ğŸ“… ãƒãƒ³ã‚¹ãƒªãƒ¼ã‚¤ãƒ³ã‚µã‚¤ãƒˆ", "â˜€ï¸ ãƒ‡ã‚¤ãƒªãƒ¼ã‚¤ãƒ³ã‚µã‚¤ãƒˆ"])
    
    with tab1:
        st.markdown("#### ã‚ãªãŸã®é›†ä¸­ç‰¹æ€§")
        st.markdown(f"ã€€{f_dow}æ›œæ—¥ã®{f_hour}æ™‚å°ã«æœ€ã‚‚é›†ä¸­ã—ã‚„ã™ã„å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚<br>"
                    f"ã€€å¹³å‡é›†ä¸­æŒç¶šæ™‚é–“ã¯{avg_focus_duration_str}åˆ†ã§ã™ã€‚<br>"
                    f"ã€€1æ—¥ã«{daily_focus_count_str}å›é›†ä¸­ã¨ç·©å’Œã®ãƒªã‚ºãƒ ã‚’ç¹°ã‚Šè¿”ã—ã¦ã„ã¾ã™ã€‚<br>"
                    f"ã€€é›†ä¸­ã«å…¥ã‚Šã‚„ã™ã„è¡Œå‹•ã¯{focus_actions_str}", unsafe_allow_html=True)

        st.markdown("<br>", unsafe_allow_html=True)
        st.markdown("#### ã‚ãªãŸã®ç–²åŠ´ç‰¹æ€§")
        st.markdown(f"ã€€{fat_dow}æ›œæ—¥ã®{fat_hour}æ™‚å°ã«æœ€ã‚‚ç–²åŠ´ã—ã‚„ã™ã„å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚<br>"
                    f"ã€€ç–²åŠ´ã—ã‚„ã™ã„è¡Œå‹•ã¯{fatigue_actions_str}", unsafe_allow_html=True)

        if not focus_durations.empty:
            st.markdown("<br>##### é›†ä¸­æŒç¶šæ™‚é–“ã®åˆ†å¸ƒ", unsafe_allow_html=True)
            max_duration = int(focus_durations.max())
            max_bin = math.ceil(max_duration / 10) * 10
            bins = np.arange(0, max_bin + 20, 10) 
            counts, edges = np.histogram(focus_durations, bins=bins)
            xtick_labels = [f"{int(edges[i])}-{int(edges[i+1])-1}" for i in range(len(edges)-1)]
            
            fig_dist = go.Figure(data=[go.Bar(
                x=xtick_labels, y=counts, marker_color='#4A90E2', opacity=0.8,
                hovertemplate="é›†ä¸­æ™‚é–“: %{x}åˆ†<br>å›æ•°: %{y}å›<extra></extra>"
            )])
            fig_dist.update_layout(
                xaxis_title="é›†ä¸­æŒç¶šæ™‚é–“ (åˆ†)", yaxis_title="å›æ•°", height=300,
                margin=dict(l=20, r=20, t=20, b=20), plot_bgcolor='rgba(0,0,0,0)', bargap=0.1
            )
            st.plotly_chart(fig_dist, use_container_width=True)

        st.markdown("##### æ™‚é–“å¸¯ãƒ»æ›œæ—¥åˆ¥ã®å‚¾å‘ (ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—)")
        def plot_heatmap_plotly(target_metric, colorscale_name):
            if target_metric not in df_imp.columns: return None
            pivot_df = df_imp.pivot_table(values=target_metric, index=df_imp.index.hour, columns=df_imp.index.dayofweek, aggfunc='mean')
            heatmap_data = np.full((time_range[1] - time_range[0] + 1, 7), np.nan)
            for h in pivot_df.index:
                if time_range[0] <= h <= time_range[1]:
                    for d in pivot_df.columns:
                        if d in selected_dow_indices: heatmap_data[int(h) - time_range[0], int(d)] = pivot_df.loc[h, d]
            
            fig_hm = go.Figure(data=go.Heatmap(
                z=heatmap_data, x=dow_options, y=[f"{h}:00" for h in range(time_range[0], time_range[1] + 1)],
                colorscale=colorscale_name, hoverongaps=False, hovertemplate="æ›œæ—¥: %{x}<br>æ™‚é–“å¸¯: %{y}<br>ç¢ºç‡: %{z:.2f}<extra></extra>"
            ))
            fig_hm.update_layout(yaxis_autorange='reversed', height=350, margin=dict(l=20, r=20, t=20, b=20))
            return fig_hm

        col_h1, col_h2 = st.columns(2)
        with col_h1:
            st.markdown("**ğŸ¯ é›†ä¸­ç¢ºç‡**")
            fig_focus = plot_heatmap_plotly('é›†ä¸­åˆ¤å®š', 'Blues')
            if fig_focus: st.plotly_chart(fig_focus, use_container_width=True)
        with col_h2:
            st.markdown("**ğŸ”‹ ç–²åŠ´ç¢ºç‡**")
            fig_fatigue = plot_heatmap_plotly('ç–²åŠ´åˆ¤å®š', 'Reds')
            if fig_fatigue: st.plotly_chart(fig_fatigue, use_container_width=True)

    with tab2:
        df_ts['year_month'] = df_ts.index.to_period('M').astype(str)
        available_months = sorted(df_ts['year_month'].unique().tolist(), reverse=True)
        
        if not available_months:
            st.write("åˆ†æå¯èƒ½ãªæœˆã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            selected_month = st.selectbox("åˆ†æå¯¾è±¡ã¨ã™ã‚‹å¹´æœˆã‚’é¸æŠã—ã¦ãã ã•ã„", available_months)
            df_month = df_ts[df_ts['year_month'] == selected_month]
            
            if 'é›†ä¸­åˆ¤å®š' in df_month.columns:
                # 1æ™‚é–“å˜ä½ã§ã®é›†ä¸­å›æ•°ï¼ˆåˆ†æ•°ã«ç›¸å½“ï¼‰ã‚’é›†è¨ˆ
                df_m_1t = df_month[['é›†ä¸­åˆ¤å®š']].resample('1T').mean()
                df_m_1t['é›†ä¸­åˆ¤å®š_ãƒ•ãƒ©ã‚°'] = (df_m_1t['é›†ä¸­åˆ¤å®š'] >= 0.5).astype(int)
                
                df_m_hourly = df_m_1t.resample('1H').sum()
                df_m_hourly['day'] = df_m_hourly.index.day
                df_m_hourly['hour'] = df_m_hourly.index.hour
                df_m_hourly['dow'] = df_m_hourly.index.dayofweek
                
                # ã‚°ãƒ©ãƒ•: æ›œæ—¥åˆ¥ãƒ»æ™‚é–“å¸¯åˆ¥
                col_m1, col_m2 = st.columns(2)
                
                with col_m1:
                    dow_sum = df_m_hourly.groupby('dow')['é›†ä¸­åˆ¤å®š_ãƒ•ãƒ©ã‚°'].sum().reindex(range(7), fill_value=0)
                    fig_dow = px.bar(x=dow_options, y=dow_sum.values, labels={'x': 'æ›œæ—¥', 'y': 'é›†ä¸­åˆ¤å®šå›æ•°'}, title="æ›œæ—¥åˆ¥ é›†ä¸­åˆ¤å®šå›æ•°")
                    fig_dow.update_traces(marker_color='#4A90E2')
                    st.plotly_chart(fig_dow, use_container_width=True)
                    
                with col_m2:
                    hour_sum = df_m_hourly.groupby('hour')['é›†ä¸­åˆ¤å®š_ãƒ•ãƒ©ã‚°'].sum().reindex(range(24), fill_value=0)
                    fig_hour = px.bar(x=[f"{h}:00" for h in range(24)], y=hour_sum.values, labels={'x': 'æ™‚é–“å¸¯', 'y': 'é›†ä¸­åˆ¤å®šå›æ•°'}, title="æ™‚é–“å¸¯åˆ¥ é›†ä¸­åˆ¤å®šå›æ•°")
                    fig_hour.update_traces(marker_color='#4A90E2')
                    st.plotly_chart(fig_hour, use_container_width=True)
                
                # ã‚°ãƒ©ãƒ•: æ—¥Ã—æ™‚é–“ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
                st.markdown("#### æ—¥ä»˜Ã—æ™‚é–“å¸¯ã®é›†ä¸­åˆ¤å®šå›æ•° (èµ¤æ ã¯äºˆå®šã‚ã‚Š)")
                hm_pivot = df_m_hourly.pivot_table(index='day', columns='hour', values='é›†ä¸­åˆ¤å®š_ãƒ•ãƒ©ã‚°', aggfunc='sum').fillna(0)
                # æ¬ ã‘ã¦ã„ã‚‹æ—¥ãƒ»æ™‚é–“ã‚’è£œå®Œ
                all_days = list(range(1, df_month.index.days_in_month[0] + 1))
                hm_pivot = hm_pivot.reindex(index=all_days, columns=list(range(24)), fill_value=0)
                
                fig_hm_month = go.Figure(data=go.Heatmap(
                    z=hm_pivot.values,
                    x=[f"{h}:00" for h in range(24)],
                    y=[f"{d}æ—¥" for d in all_days],
                    colorscale='Blues',
                    hovertemplate="æ—¥ä»˜: %{y}<br>æ™‚é–“å¸¯: %{x}<br>é›†ä¸­å›æ•°: %{z}<extra></extra>"
                ))
                
                # äºˆå®šãŒã‚ã‚‹æ™‚é–“å¸¯ã«èµ¤æ ï¼ˆShapesï¼‰ã‚’è¿½åŠ 
                shapes = []
                if df_sched is not None and not df_sched.empty:
                    for d in all_days:
                        for h in range(24):
                            try:
                                dt_start = pd.to_datetime(f"{selected_month}-{d:02d} {h:02d}:00:00")
                                dt_end = dt_start + pd.Timedelta('1H')
                                has_sched = ((df_sched['start_dt'] < dt_end) & (df_sched['end_dt'] > dt_start)).any()
                                if has_sched:
                                    shapes.append(dict(
                                        type="rect",
                                        x0=h - 0.5, x1=h + 0.5,
                                        y0=d - 1 - 0.5, y1=d - 1 + 0.5, # y0,y1 ã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹(0å§‹ã¾ã‚Š)ã§æŒ‡å®š
                                        line=dict(color="red", width=2),
                                        fillcolor="rgba(0,0,0,0)"
                                    ))
                            except ValueError:
                                pass # å­˜åœ¨ã—ãªã„æ—¥ä»˜ï¼ˆã†ã‚‹ã†å¹´ãªã©ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
                
                fig_hm_month.update_layout(
                    shapes=shapes,
                    yaxis_autorange='reversed',
                    height=600,
                    margin=dict(l=20, r=20, t=20, b=20)
                )
                st.plotly_chart(fig_hm_month, use_container_width=True)
                
                # ã‚³ãƒ¡ãƒ³ãƒˆã®ç”Ÿæˆ
                best_dow_m = dow_options[dow_sum.idxmax()] if dow_sum.sum() > 0 else "ä¸æ˜"
                best_hour_m = hour_sum.idxmax() if hour_sum.sum() > 0 else "ä¸æ˜"
                
                st.info(f"**ã€{selected_month} ã®ãƒãƒ³ã‚¹ãƒªãƒ¼ã‚¤ãƒ³ã‚µã‚¤ãƒˆã€‘**\n\n"
                        f"- ã“ã®æœˆã¯ **{best_dow_m}æ›œæ—¥** ã®é›†ä¸­åˆ¤å®šå›æ•°ãŒæœ€ã‚‚å¤šããªã£ã¦ã„ã¾ã™ã€‚\n"
                        f"- æ™‚é–“å¸¯ã§è¦‹ã‚‹ã¨ **{best_hour_m}æ™‚å°** ã«é›†ä¸­ã™ã‚‹å‚¾å‘ãŒå¼·ã‹ã£ãŸã§ã™ã€‚\n"
                        f"- ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä¸Šã®èµ¤æ ã¯ã€Œäºˆå®šï¼ˆä¼šè­°ãªã©ï¼‰ã€ãŒå…¥ã£ã¦ã„ã‚‹æ™‚é–“å¸¯ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚äºˆå®šã¨é›†ä¸­ã®ç›¸é–¢é–¢ä¿‚ã‚’è¦–è¦šçš„ã«ç¢ºèªã§ãã¾ã™ã€‚")
            else:
                st.write("ã€Œé›†ä¸­åˆ¤å®šã€ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")

    with tab3:
        df_ts['date_str'] = df_ts.index.date.astype(str)
        available_days = sorted(df_ts['date_str'].unique().tolist(), reverse=True)
        
        if not available_days:
            st.write("åˆ†æå¯èƒ½ãªæ—¥ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            selected_day = st.selectbox("åˆ†æå¯¾è±¡ã¨ã™ã‚‹å¹´æœˆæ—¥ã‚’é¸æŠã—ã¦ãã ã•ã„", available_days)
            df_day = df_ts[df_ts['date_str'] == selected_day].copy()
            
            if 'CVRR_SCORE_NEW' in df_day.columns:
                st.markdown("#### ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ«ã‚°ãƒ©ãƒ• (é›†ä¸­ã®æ³¢)")
                
                fig_daily = go.Figure()
                fig_daily.add_trace(go.Scatter(
                    x=df_day.index, 
                    y=df_day['CVRR_SCORE_NEW'],
                    fill='tozeroy',
                    mode='lines',
                    line=dict(color='#00B5E2', width=2),
                    name='CVRR SCORE',
                    hovertemplate="æ™‚åˆ»: %{x|%H:%M}<br>ã‚¹ã‚³ã‚¢: %{y:.1f}<extra></extra>"
                ))
                
                fig_daily.update_layout(
                    title=f"{selected_day} ã®é›†ä¸­ã¨ç·©å’Œã®æ¨ç§»",
                    xaxis_title="æ™‚åˆ»",
                    yaxis_title="CVRR SCORE (é›†ä¸­åº¦åˆã„)",
                    height=400,
                    hovermode="x unified",
                    plot_bgcolor='rgba(0,0,0,0)',
                    margin=dict(l=20, r=20, t=40, b=20)
                )
                fig_daily.update_xaxes(showgrid=True, gridcolor='lightgray', showline=True, linewidth=1, linecolor='black')
                fig_daily.update_yaxes(showgrid=True, gridcolor='lightgray', showline=True, linewidth=1, linecolor='black')
                st.plotly_chart(fig_daily, use_container_width=True)
                
                # ã‚³ãƒ¡ãƒ³ãƒˆã®ç”Ÿæˆ
                if not df_day['CVRR_SCORE_NEW'].isna().all():
                    max_idx = df_day['CVRR_SCORE_NEW'].idxmax()
                    max_val = df_day['CVRR_SCORE_NEW'].max()
                    avg_val = df_day['CVRR_SCORE_NEW'].mean()
                    
                    st.info(f"**ã€{selected_day} ã®ãƒ‡ã‚¤ãƒªãƒ¼ã‚¤ãƒ³ã‚µã‚¤ãƒˆã€‘**\n\n"
                            f"- ã“ã®æ—¥ã®é›†ä¸­ï¼ˆCVRR SCOREï¼‰ã®ãƒ”ãƒ¼ã‚¯ã¯ **{max_idx.strftime('%H:%M')}é ƒ** ï¼ˆã‚¹ã‚³ã‚¢: {max_val:.1f}ï¼‰ã§ã—ãŸã€‚\n"
                            f"- 1æ—¥ã®å¹³å‡ã‚¹ã‚³ã‚¢ã¯ **{avg_val:.1f}** ã¨ãªã£ã¦ã„ã¾ã™ã€‚\n"
                            f"- ã‚°ãƒ©ãƒ•ã®å±±ãŒé«˜ã„æ™‚é–“å¸¯ã»ã©é›†ä¸­åº¦ãŒé«˜ãã€è°·ãŒæ·±ã„æ™‚é–“å¸¯ã»ã©ç·©å’Œï¼ˆãƒªãƒ©ãƒƒã‚¯ã‚¹ï¼‰çŠ¶æ…‹ã«ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚")
                else:
                    st.write("ã“ã®æ—¥ã®æœ‰åŠ¹ãªã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            else:
                st.write("ãƒ‡ãƒ¼ã‚¿ã«ã€ŒCVRR_SCORE_NEWã€ãŒå«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ«ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")


    # =========================================================================
    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ (Real-time Focus)
    # =========================================================================
    st.header("âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ (Real-time Focus)")
    
    auc_eval = "ç®—å‡ºä¸å¯"
    if not np.isnan(auc_test):
        if auc_test >= 0.8: auc_eval = "ğŸŸ¢ éå¸¸ã«è‰¯ã„"
        elif auc_test >= 0.7: auc_eval = "ğŸ”µ è‰¯ã„ (å®Ÿç”¨ãƒ¬ãƒ™ãƒ«)"
        elif auc_test >= 0.6: auc_eval = "ğŸŸ¡ æ™®é€š"
        else: auc_eval = "ğŸ”´ æ”¹å–„ãŒå¿…è¦"

    loss_eval = "ç®—å‡ºä¸å¯"
    if not np.isnan(logloss_test):
        if logloss_test <= 0.4: loss_eval = "ğŸŸ¢ éå¸¸ã«è‰¯ã„"
        elif logloss_test <= 0.6: loss_eval = "ğŸŸ¡ æ™®é€š"
        else: loss_eval = "ğŸ”´ æ”¹å–„ãŒå¿…è¦"

    col_m1, col_m2 = st.columns(2)
    col_m1.info(f"**ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ (AUC-ROC)**: {auc_test:.3f} ğŸ‘‰ **{auc_eval}**\n\n*1.0ã«è¿‘ã„ã»ã©çŠ¶æ…‹ã®åˆ¤åˆ¥ãŒæ­£ç¢ºã«ã§ãã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ï¼ˆ0.7ä»¥ä¸ŠãŒå®Ÿç”¨ã®ç›®å®‰ï¼‰ã€‚*")
    col_m2.info(f"**äºˆæ¸¬ã®ç¢ºä¿¡åº¦ (Log Loss)**: {logloss_test:.3f} ğŸ‘‰ **{loss_eval}**\n\n*0.0ã«è¿‘ã„ã»ã©AIãŒã€Œè¿·ã„ãªãã€æ­£è§£ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ï¼ˆ0.6ä»¥ä¸‹ãŒç›®å®‰ï¼‰ã€‚*")
    
    with st.expander("ğŸ“Š ãƒ†ã‚¹ãƒˆæœŸé–“ã®äºˆæ¸¬ç¢ºç‡æ¨ç§»ã‚’è¡¨ç¤º"):
        fig_ts_plot = go.Figure()
        fig_ts_plot.add_trace(go.Scatter(
            x=test_df.index, y=y_test_class, mode='markers', name='å®Ÿéš›ã®çŠ¶æ…‹ (1=Yes, 0=No)',
            marker=dict(color='blue', opacity=0.6, size=6), hovertemplate="æ—¥æ™‚: %{x}<br>çŠ¶æ…‹: %{y}<extra></extra>"
        ))
        fig_ts_plot.add_trace(go.Scatter(
            x=test_df.index, y=preds_proba, mode='lines', name='LightGBM äºˆæ¸¬ç¢ºç‡',
            line=dict(color='red', width=2), opacity=0.8, hovertemplate="æ—¥æ™‚: %{x}<br>äºˆæ¸¬ç¢ºç‡: %{y:.2f}<extra></extra>"
        ))
        fig_ts_plot.update_layout(title=f"ãƒ†ã‚¹ãƒˆæœŸé–“ã® {selected_target_name} äºˆæ¸¬ç¢ºç‡ã®æ¨ç§»", hovermode="x unified", legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))
        st.plotly_chart(fig_ts_plot, use_container_width=True)

    st.subheader("ğŸ”® ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã¨è¦å› åˆ†æ")
    available_data_all = df_imp.drop(columns=drop_cols, errors='ignore')
    if TARGET_DATETIME is not None:
        try:
            target_dt = pd.to_datetime(TARGET_DATETIME)
            available_data = available_data_all[available_data_all.index <= target_dt]
            if len(available_data) == 0:
                st.warning("æŒ‡å®šã•ã‚ŒãŸåŸºæº–æ—¥æ™‚ä»¥å‰ã®ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                available_data = available_data_all
        except Exception as e:
            st.warning(f"æ—¥æ™‚ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆ{e}ï¼‰ã€‚æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    else:
        available_data = available_data_all

    target_data = available_data.iloc[-1:]
    current_time = target_data.index[0]
    current_val = float(target_data[target_col].values[0])
    current_state_bool = current_val >= target_threshold
    current_proba = model.predict_proba(target_data)[0, 1]
    predicted_state_bool = current_proba >= 0.5
    
    col_p1, col_p2, col_p3, col_p4 = st.columns(4)
    col_p1.metric("åŸºæº–æ—¥æ™‚", current_time.strftime('%Y-%m-%d %H:%M'))
    col_p2.metric(f"ç¾åœ¨ã® {selected_target_name} çŠ¶æ…‹", "Yes" if current_state_bool else "No")
    col_p3.metric(f"{PREDICT_AHEAD}å¾Œã®äºˆæ¸¬åˆ¤å®š", "Yes" if predicted_state_bool else "No")
    col_p4.metric(f"ç™ºç”Ÿç¢ºç‡", f"{current_proba * 100:.1f} %")
    st.caption(f"â€» **äºˆæ¸¬åˆ¤å®šã¨ç™ºç”Ÿç¢ºç‡ã«ã¤ã„ã¦**: {PREDICT_AHEAD}å¾Œã«ã‚ãªãŸãŒã€Œ{selected_target_name}ã€ã®çŠ¶æ…‹ã«ãªã£ã¦ã„ã‚‹ç¢ºç‡ã‚’AIãŒç®—å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚50%ä»¥ä¸Šã‚’ã€ŒYesã€ã¨åˆ¤å®šã—ã¦ã„ã¾ã™ã€‚")

    with st.spinner("SHAPã§è¦å› ã‚’åˆ†æã—ã¦ã„ã¾ã™..."):
        explainer = shap.TreeExplainer(model)
        shap_values_latest = explainer(target_data)
        if len(shap_values_latest.shape) == 3:
            shap_vals = shap_values_latest[0, :, 1].values
            shap_base_obj = shap_values_latest[0, :, 1]
        else:
            shap_vals = shap_values_latest[0].values
            shap_base_obj = shap_values_latest[0]
        
        def is_actionable(col: str) -> bool: return not (target_col in col or col in ["hour", "dayofweek"])
        exp_df = pd.DataFrame({'Feature': target_data.columns, 'Value': target_data.values[0], 'SHAP': shap_vals})
        exp_df['AbsSHAP'] = exp_df['SHAP'].abs()
        exp_df_action = exp_df[exp_df['Feature'].apply(is_actionable)].sort_values('AbsSHAP', ascending=False)
        
        fig2, ax2 = plt.subplots(figsize=(8, 4))
        shap.plots.waterfall(shap_base_obj, show=False)
        st.pyplot(fig2)

        st.markdown("**ã€è¦å› åˆ†æã®è§£èª¬ã€‘**")
        st.caption("â€» ä¸Šè¨˜ã®SHAPã‚°ãƒ©ãƒ•ã¯å°‚ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãŸã‚é™æ­¢ç”»åƒã§å‡ºåŠ›ã—ã¦ã„ã¾ã™ã€‚ä¸€ç•ªé•·ã„ãƒãƒ¼ï¼ˆèµ¤ã¾ãŸã¯é’ï¼‰ãŒç¢ºç‡ã«æœ€ã‚‚å½±éŸ¿ã‚’ä¸ãˆãŸè¦å› ã§ã™ã€‚")
        
        pos_factors = exp_df_action[exp_df_action['SHAP'] > 0]
        neg_factors = exp_df_action[exp_df_action['SHAP'] < 0]
        
        if target_col in ['NEMUKE_SCORE_NEW', 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š']:
            pos_effect_text, neg_effect_text = "ç¢ºç‡ä¸Šæ˜‡ï¼ˆæ‚ªåŒ–æ–¹å‘ï¼‰", "ç¢ºç‡ä½ä¸‹ï¼ˆå¥½è»¢æ–¹å‘ï¼‰"
            bar_desc = f"â€»ã‚°ãƒ©ãƒ•ã®èµ¤ã„ãƒãƒ¼ãŒ{selected_target_name}ã®ç™ºç”Ÿç¢ºç‡ã‚’æŠ¼ã—ä¸Šã’ã‚‹ï¼ˆæ‚ªåŒ–ï¼‰è¦å› ã€é’ã„ãƒãƒ¼ãŒæŠ¼ã—ä¸‹ã’ã‚‹ï¼ˆå¥½è»¢ï¼‰è¦å› ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚"
        else:
            pos_effect_text, neg_effect_text = "ç¢ºç‡ä¸Šæ˜‡ï¼ˆå¥½è»¢æ–¹å‘ï¼‰", "ç¢ºç‡ä½ä¸‹ï¼ˆæ‚ªåŒ–æ–¹å‘ï¼‰"
            bar_desc = f"â€»ã‚°ãƒ©ãƒ•ã®èµ¤ã„ãƒãƒ¼ãŒ{selected_target_name}ã®ç™ºç”Ÿç¢ºç‡ã‚’æŠ¼ã—ä¸Šã’ã‚‹ï¼ˆå¥½è»¢ï¼‰è¦å› ã€é’ã„ãƒãƒ¼ãŒæŠ¼ã—ä¸‹ã’ã‚‹ï¼ˆæ‚ªåŒ–ï¼‰è¦å› ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚"

        base_pos = None
        if not pos_factors.empty:
            top_pos = pos_factors.iloc[0]
            desc_pos = get_factor_direction_text(top_pos['Feature'], top_pos['Value'], available_data_all)
            base_pos = get_base_feature_name(top_pos['Feature'])
            st.write(f"- ğŸ“ˆ **ç¢ºç‡ã‚’ä¸Šã’ã‚‹è¦å› **: **{desc_pos}** ãŒ{pos_effect_text}ã«åƒã„ã¦ã„ã¾ã™ (å½±éŸ¿åº¦: {top_pos['SHAP']:+.2f})ã€‚")
            
        if not neg_factors.empty:
            top_neg = neg_factors.iloc[0]
            if base_pos is not None and get_base_feature_name(top_neg['Feature']) == base_pos and len(neg_factors) > 1:
                top_neg = neg_factors.iloc[1]
            desc_neg = get_factor_direction_text(top_neg['Feature'], top_neg['Value'], available_data_all)
            st.write(f"- ğŸ“‰ **ç¢ºç‡ã‚’ä¸‹ã’ã‚‹è¦å› **: **{desc_neg}** ãŒ{neg_effect_text}ã«åƒã„ã¦ã„ã¾ã™ (å½±éŸ¿åº¦: {top_neg['SHAP']:+.2f})ã€‚")
            
        st.caption(bar_desc)

    schedule_density = float(target_data["schedule_density_2h"].values[0]) if "schedule_density_2h" in target_data else 0
    time_to_next = float(target_data["time_to_next_event_min"].values[0]) if "time_to_next_event_min" in target_data else np.nan
    is_meeting = float(target_data["is_meeting"].values[0]) if "is_meeting" in target_data else 0
    
    state_trend_prob = 1.0 - current_proba if target_col in ['NEMUKE_SCORE_NEW', 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š'] else current_proba

    reasons = []
    if is_meeting > 0: reasons.append("ç¾åœ¨ä¼šè­°ä¸­")
    if schedule_density >= 0.6: reasons.append("äºˆå®šå¯†åº¦ãŒé«˜ã„")
    if state_trend_prob >= 0.6: reasons.append(f"{selected_target_name}ã®å¥½ã¾ã—ã„ç¢ºç‡ãŒé«˜ã„")
    elif state_trend_prob <= 0.4: reasons.append(f"{selected_target_name}ã®å¥½ã¾ã—ããªã„ç¢ºç‡ãŒé«˜ã„")
    
    if is_meeting > 0:
        work_mode, advice = "E: æ®µå–ã‚Šï¼ˆä¼šè­°ãƒ¢ãƒ¼ãƒ‰ï¼‰", "è«–ç‚¹ã‚’1æšã«æ•´ç†ã—ã€æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ToDoåŒ–ã—ã¾ã—ã‚‡ã†ã€‚"
    elif state_trend_prob >= 0.6 and (np.isnan(time_to_next) or time_to_next >= 50) and schedule_density < 0.6:
        work_mode, advice = "C: ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆï¼ˆæ·±ï¼‰- ä¼ç”»ãƒ»æˆ¦ç•¥", "çŠ¶æ…‹ãŒå¥½è»¢ã™ã‚‹ç¢ºç‡ãŒé«˜ãã€ã¾ã¨ã¾ã£ãŸæ™‚é–“ã‚‚ã‚ã‚Šã¾ã™ã€‚è¨­è¨ˆãƒ»ä¼ç”»ã®éª¨æ ¼ã¥ãã‚Šãªã©ã€é‡ã„æ€è€ƒã‚¿ã‚¹ã‚¯ã‚’é€²ã‚ã‚‹ã®ãŒæœ€é©ã§ã™ã€‚"
    elif state_trend_prob <= 0.4 or schedule_density >= 0.6:
        work_mode, advice = "D: ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆï¼ˆè»½ï¼‰- æ•´ç†ãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼", "äºˆå®šãŒç´°åˆ‡ã‚Œã‹ã€çŠ¶æ…‹ãŒæ‚ªåŒ–ã™ã‚‹ç¢ºç‡ãŒé«˜ã„ã§ã™ã€‚10ã€œ20åˆ†ã§çµ‚ã‚ã‚‹ToDoæ¶ˆåŒ–ã‚„ã€è³‡æ–™ã®æ•´å½¢ãƒ»ãƒã‚§ãƒƒã‚¯ä½œæ¥­ã«æ™‚é–“ã‚’å½“ã¦ã¾ã—ã‚‡ã†ã€‚"
    else:
        if (np.isnan(time_to_next) or time_to_next >= 30) and schedule_density < 0.6:
            work_mode, advice = "A: ã‚¤ãƒ³ãƒ—ãƒƒãƒˆï¼ˆé‡ï¼‰ ã¾ãŸã¯ B: ã‚¤ãƒ³ãƒ—ãƒƒãƒˆï¼ˆè»½ï¼‰", "é›£ã—ã‚è³‡æ–™ã®èª­ã¿è¾¼ã¿ã‚„æƒ…å ±æ•´ç†ãªã©ã€æ¬¡ã®æ·±ã„æ€è€ƒã«å‘ã‘ãŸã‚¤ãƒ³ãƒ—ãƒƒãƒˆä½œæ¥­ã«é©ã—ã¦ã„ã¾ã™ã€‚"
        else:
            work_mode, advice = "E: æ®µå–ã‚Š", "æ¬¡ã®æ·±ã„ä½œæ¥­ã¸ã‚¹ãƒ ãƒ¼ã‚ºã«å…¥ã‚Œã‚‹ã‚ˆã†ã€è«–ç‚¹ã®åˆ—æŒ™ã‚„å„ªå…ˆé †ä½ä»˜ã‘ã€ç´ æã®æ´—ã„å‡ºã—ã‚’è¡Œã„ã¾ã—ã‚‡ã†ã€‚"

    st.subheader("ğŸ“ åˆ†æãƒ¬ãƒãƒ¼ãƒˆ (AIã«ã‚ˆã‚‹ææ¡ˆ)")
    main_factor_desc = get_factor_direction_text(exp_df_action.iloc[0]['Feature'], exp_df_action.iloc[0]['Value'], available_data_all) if not exp_df_action.empty else "ä¸æ˜"
    prompt_context = f"ç¾åœ¨æ™‚åˆ»: {current_time.strftime('%Y-%m-%d %H:%M')}\nç¾åœ¨ã®{selected_target_name}ã®çŠ¶æ…‹: {'Yes' if current_state_bool else 'No'}\n{PREDICT_AHEAD}å¾Œã®äºˆæ¸¬åˆ¤å®š: {'Yes' if predicted_state_bool else 'No'} (ç™ºç”Ÿç¢ºç‡: {current_proba * 100:.1f}%)\nç›´è¿‘ã®ä¸»è¦å› : {main_factor_desc} (SHAP: {exp_df_action.iloc[0]['SHAP']:+.2f})\nåˆ¤å®šã•ã‚ŒãŸåƒãæ–¹: {work_mode}\nç†ç”±: {', '.join(reasons) if reasons else 'ç‰¹ã«ãªã—'}"
    
    if use_gemini and api_key:
        with st.spinner("GeminiãŒãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­..."):
            try:
                genai.configure(api_key=api_key)
                model_llm = genai.GenerativeModel('gemini-2.5-flash')
                resp = model_llm.generate_content(f"ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€å®¢è¦³çš„ãªåƒãæ–¹ã‚¢ãƒ‰ãƒã‚¤ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n\n{prompt_context}\n\næ§‹æˆ:\n1. äºˆæ¸¬çµæœã¨ä¸»ãªè¦å› \n2. å¥¨åŠ±ã™ã‚‹åƒãæ–¹ã®å…·ä½“ä¾‹")
                st.write(resp.text)
            except Exception as e:
                st.error(f"Gemini APIã‚¨ãƒ©ãƒ¼: {e}")
    else:
        st.info("ğŸ’¡ Gemini APIã‚­ãƒ¼ãŒæœªå…¥åŠ›ã®ãŸã‚ã€ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
        st.markdown(f"#### 1. è¿‘ã„å°†æ¥ï¼ˆ{PREDICT_AHEAD}å¾Œï¼‰ã®äºˆæ¸¬çµæœ")
        st.write(f"åŸºæº–æ—¥æ™‚ï¼ˆ{current_time.strftime('%Y-%m-%d %H:%M')}ï¼‰ã® {selected_target_name} ã¯ **{'Yes' if current_state_bool else 'No'}** ã®çŠ¶æ…‹ã§ã™ã€‚")
        st.write(f"{PREDICT_AHEAD}å¾Œã¯ **{'Yes' if predicted_state_bool else 'No'}** ï¼ˆç™ºç”Ÿç¢ºç‡ **{current_proba * 100:.1f} %**ï¼‰ã¨äºˆæ¸¬ã•ã‚Œã¾ã™ã€‚\nã“ã®äºˆæ¸¬ã®ä¸»ãªè¦å› ã¨ã—ã¦ã€**{main_factor_desc}** ãŒå½±éŸ¿ã—ã¦ã„ã¾ã™ã€‚")
        st.markdown(f"#### 2. å¥¨åŠ±ã™ã‚‹åƒãæ–¹")
        st.write(f"ç¾åœ¨ã®äºˆæ¸¬ç¢ºç‡ã¨äºˆå®šçŠ¶æ³ï¼ˆ{', '.join(reasons) if reasons else 'é˜»å®³è¦å› ãªã—'}ï¼‰ã‹ã‚‰ã€**ã€Œ{work_mode}ã€**ã«å–ã‚Šçµ„ã‚€ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n**ğŸ’¡ é€²ã‚æ–¹ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹**: {advice}")

# --- UI ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ ---
st.write("### ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
col_file1, col_file2 = st.columns(2)
with col_file1:
    file_ts = st.file_uploader("1. ç”Ÿä½“ãƒ‡ãƒ¼ã‚¿ (CSVå½¢å¼)", type=['csv'])
with col_file2:
    file_sched = st.file_uploader("2. äºˆå®šè¡¨ãƒ‡ãƒ¼ã‚¿ (äºˆå®šè¡¨.CSV) â€»ä»»æ„", type=['csv'])

if st.button("ğŸš€ åˆ†æã‚’å®Ÿè¡Œã™ã‚‹", type="primary"):
    if file_ts is not None:
        try:
            df_ts = pd.read_csv(file_ts, skiprows=2)
            df_sched = pd.read_csv(file_sched) if file_sched is not None else None
            run_analysis(df_ts, df_sched, use_gemini=True if api_key else False)
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    else:
        st.warning("âš ï¸ ç”Ÿä½“ãƒ‡ãƒ¼ã‚¿ (CSVå½¢å¼) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")