# -*- coding: utf-8 -*-
"""
Deep Work æœ€å¤§åŒ–ãƒ»é›†ä¸­æ³¢è§£æã‚¢ãƒ—ãƒª (Wave Dynamics + Fatigue/Arousal + Task Optimization)
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import lightgbm as lgb
from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, brier_score_loss
import scipy.signal as signal
import shap
import warnings
import plotly.express as px
import plotly.graph_objects as go
from pathlib import Path
import matplotlib as mpl
import matplotlib.font_manager as fm
import datetime
import math
import io

# --- Streamlit ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="Deep Work Wave Dynamics", layout="wide", initial_sidebar_state="expanded")

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
font_path = Path(__file__).parent / "assets" / "fonts" / "NotoSansCJKjp-Regular.otf"
if font_path.exists():
    fm.fontManager.addfont(str(font_path))
    prop = fm.FontProperties(fname=str(font_path))
    mpl.rcParams["font.family"] = prop.get_name()

mpl.rcParams["axes.unicode_minus"] = False
warnings.filterwarnings('ignore')

# --- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š (ã‚¿ã‚¹ã‚¯æœ€é©é…ç½®ã‚¨ãƒ³ã‚¸ãƒ³) ---
MIN_DEEP_DURATION = 60
GAP_TOLERANCE = 5
FOCUS_STREAK_MIN = 5
WEEKLY_MIN_DEEP = 2

# --- ã‚«ã‚¹ã‚¿ãƒ CSS ---
st.markdown("""
<style>
    .kpi-card { background-color: #ffffff; border-radius: 12px; padding: 24px; text-align: center; box-shadow: 0 4px 12px rgba(0,0,0,0.05); margin-bottom: 20px; border: 1px solid #f0f2f6; }
    .kpi-title { font-size: 1.1rem; color: #6c757d; margin-bottom: 8px; font-weight: 600; text-transform: uppercase; letter-spacing: 0.05em; }
    .kpi-value-main { font-size: 3.5rem; color: #1e293b; font-weight: 800; line-height: 1.1; margin-bottom: 5px; }
    .kpi-value-wave { font-size: 2.5rem; color: #2563eb; font-weight: 800; line-height: 1.2; margin-bottom: 5px; }
    .kpi-unit { font-size: 1.2rem; color: #64748b; font-weight: 500; }
    .kpi-sub { font-size: 1.1rem; color: #10b981; font-weight: bold; margin-top: 10px; }
    .kpi-sub.warning { color: #f59e0b; }
    .kpi-sub.alert { color: #ef4444; }
    .chance-box { background-color: #f0fdf4; border-left: 6px solid #10b981; padding: 20px; border-radius: 8px; margin-bottom: 20px; }
    .chance-time { font-size: 1.8rem; color: #047857; font-weight: 800; }
    .sim-box { background-color: #f8fafc; padding: 16px; border-radius: 8px; border: 1px dashed #cbd5e1; height: 100%; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# ğŸ›‘ A. ç–²åŠ´ãƒ»å›å¾©ãƒ¬ã‚¤ãƒ¤ãƒ¼ (1åˆ†ç²’åº¦)
# ==========================================
def compute_fatigue_features(df_1min, steps_col=None):
    df = df_1min.copy()
    has_rmssd = 'RMSSD_SCORE_NEW' in df.columns
    has_tp = 'TP_SCORE_NEW' in df.columns
    
    if has_rmssd and has_tp:
        df['fatigue_score'] = 0.6 * df['RMSSD_SCORE_NEW'] + 0.4 * df['TP_SCORE_NEW']
    elif has_rmssd:
        df['fatigue_score'] = df['RMSSD_SCORE_NEW']
    elif has_tp:
        df['fatigue_score'] = df['TP_SCORE_NEW']
    else:
        df['fatigue_score'] = 50.0 
        
    df['fatigue_smooth'] = df['fatigue_score'].ewm(span=10, min_periods=1).mean()
    
    w60 = np.arange(60) - 29.5
    var_x = np.sum(w60**2)
    w60 = w60 / var_x if var_x > 0 else w60
    df['fatigue_drift_60m'] = df['fatigue_smooth'].rolling(60, min_periods=60).apply(lambda y: np.dot(w60, y), raw=True).fillna(0)
    
    q33 = df['fatigue_smooth'].quantile(0.33) if not df['fatigue_smooth'].isna().all() else 33.0
    q66 = df['fatigue_smooth'].quantile(0.66) if not df['fatigue_smooth'].isna().all() else 66.0
    df['fatigue_level_band'] = np.where(df['fatigue_smooth'] >= q66, 'é«˜', np.where(df['fatigue_smooth'] <= q33, 'ä½', 'ä¸­'))
    
    if steps_col and steps_col in df.columns:
        df['rest_flag'] = np.where(df[steps_col] <= 5, 1, np.where(df[steps_col] >= 20, 0, np.nan))
        df['rest_flag'] = df['rest_flag'].ffill().fillna(0)
        rest_blocks = (df['rest_flag'] != df['rest_flag'].shift()).cumsum()
        df['rest_block_id'] = rest_blocks.where(df['rest_flag'] == 1, np.nan)
    else:
        df['rest_flag'] = np.nan
        df['rest_block_id'] = np.nan
        
    return df

def compute_morning_residual(df_1min, date_col='date', tp_col="TP_SCORE_NEW", rest_flag_col="rest_flag"):
    results = []
    df = df_1min.copy()
    if 'date' not in df.columns: df['date'] = df.index.date
    
    for d, group in df.groupby('date'):
        morning = group[(group.index.hour >= 0) & (group.index.hour < 12)]
        if morning.empty or rest_flag_col not in morning.columns: continue
            
        rest_blocks = morning[rest_flag_col] == 1
        blocks = rest_blocks.groupby((rest_blocks != rest_blocks.shift()).cumsum())
        
        target_block = None
        for _, b in blocks:
            if b.iloc[0] == True:
                if len(b) >= 60:
                    target_block = b
                    break
                elif len(b) >= 30 and target_block is None:
                    target_block = b 
                    
        if target_block is not None and tp_col in morning.columns:
            tp_median = morning.loc[target_block.index, tp_col].median()
            results.append({'date': d, 'morning_tp_median': tp_median, 'rest_duration': len(target_block)})
            
    res_df = pd.DataFrame(results)
    if not res_df.empty and 'morning_tp_median' in res_df.columns:
        median_all = res_df['morning_tp_median'].median()
        mad = (res_df['morning_tp_median'] - median_all).abs().median()
        if mad == 0: mad = 1.0
        res_df['morning_residual_z'] = 0.6745 * (res_df['morning_tp_median'] - median_all) / mad
        res_df['morning_residual_band'] = np.where(res_df['morning_residual_z'] >= 1.0, 'é«˜',
                                          np.where(res_df['morning_residual_z'] <= -1.0, 'ä½', 'ä¸­'))
    return res_df

# ==========================================
# ğŸ›‘ B. ä½è¦šé†’ãƒ¬ã‚¤ãƒ¤ãƒ¼ (1åˆ†ç²’åº¦)
# ==========================================
def compute_low_arousal(df_1min, pr_col="PR_SCORE_NEW", steps_col=None):
    df = df_1min.copy()
    if pr_col not in df.columns:
        df['low_arousal'] = 0.0
        df['low_arousal_band'] = 'ä½'
        df['low_arousal_rise_15m'] = 0.0
        return df
        
    w5 = np.array([-2, -1, 0, 1, 2]) / 10.0
    slope = df[pr_col].rolling(5, min_periods=5).apply(lambda y: np.dot(w5, y), raw=True).fillna(0)
    eps = 0.02
    delta = np.maximum(0, -(slope + eps))
    alpha = 0.95
    k = 1.0
    
    low_arousal = np.zeros(len(df))
    dates = df.index.date
    steps = df[steps_col].values if steps_col and steps_col in df.columns else np.zeros(len(df))
    
    for i in range(1, len(df)):
        if dates[i] != dates[i-1]:
            low_arousal[i] = 0
        else:
            current_alpha = 0.80 if steps[i] >= 20 else alpha
            low_arousal[i] = current_alpha * low_arousal[i-1] + k * delta.iloc[i]
            
    df['low_arousal'] = low_arousal
    q33 = df['low_arousal'].quantile(0.33) if df['low_arousal'].max() > 0 else 0
    q66 = df['low_arousal'].quantile(0.66) if df['low_arousal'].max() > 0 else 0
    df['low_arousal_band'] = np.where(df['low_arousal'] >= q66, 'é«˜', np.where(df['low_arousal'] <= q33, 'ä½', 'ä¸­'))
    df['low_arousal_rise_15m'] = df['low_arousal'] - df['low_arousal'].shift(15).fillna(0)
    
    return df

def summarize_daily_condition(df_1min):
    m_res = compute_morning_residual(df_1min)
    daily = []
    df = df_1min.copy()
    if 'date' not in df.columns: df['date'] = df.index.date
    
    for d, group in df.groupby('date'):
        daytime = group[(group.index.hour >= 9) & (group.index.hour <= 19)]
        fatigue_load = daytime['fatigue_score'].sum() if 'fatigue_score' in daytime.columns else 0
        recovery = 0.0
        if 'rest_block_id' in group.columns and 'fatigue_smooth' in group.columns:
            for _, b in group.groupby('rest_block_id'):
                diff = b['fatigue_smooth'].iloc[0] - b['fatigue_smooth'].iloc[-1]
                if diff > 0: recovery += diff
                
        la_peak_time = "ãªã—"
        if 'low_arousal' in group.columns and group['low_arousal'].max() > 0:
            peak_idx = group['low_arousal'].idxmax()
            la_peak_time = peak_idx.strftime('%H:%M')
            
        daily.append({
            'æ—¥ä»˜': d, 'æ—¥ä¸­ç–²åŠ´è² è·': int(fatigue_load), 'å®‰é™å›å¾©é‡': round(recovery, 1), 'ä½è¦šé†’ãƒ”ãƒ¼ã‚¯': la_peak_time
        })
        
    df_daily = pd.DataFrame(daily)
    if not m_res.empty:
        df_daily = pd.merge(df_daily, m_res[['date', 'morning_residual_band']], left_on='æ—¥ä»˜', right_on='date', how='left')
        df_daily = df_daily.rename(columns={'morning_residual_band': 'æœã®æ®‹ç–²åŠ´ (å¯èƒ½æ€§)'}).drop(columns=['date'])
    else:
        df_daily['æœã®æ®‹ç–²åŠ´ (å¯èƒ½æ€§)'] = 'ä¸æ˜'
        
    cols = ['æ—¥ä»˜', 'æœã®æ®‹ç–²åŠ´ (å¯èƒ½æ€§)', 'æ—¥ä¸­ç–²åŠ´è² è·', 'å®‰é™å›å¾©é‡', 'ä½è¦šé†’ãƒ”ãƒ¼ã‚¯']
    return df_daily[[c for c in cols if c in df_daily.columns]]

# ==========================================
# ğŸŒŠ 1. æ³¢è§£æãƒ»ç‰¹å¾´é‡æŠ½å‡ºãƒ»Deep Workç”Ÿæˆ
# ==========================================
def make_wave_features(df_resampled, df_sched, freq_td):
    df_feat = df_resampled.copy()
    focus_components = []
    if 'CVRR_SCORE_NEW' in df_feat.columns: focus_components.append(df_feat['CVRR_SCORE_NEW'])
    if 'RMSSD_SCORE_NEW' in df_feat.columns: focus_components.append(100 - df_feat['RMSSD_SCORE_NEW']) 
    if 'LFHF_SCORE_NEW' in df_feat.columns: focus_components.append(df_feat['LFHF_SCORE_NEW'])
        
    if focus_components: df_feat['focus_intensity'] = pd.concat(focus_components, axis=1).mean(axis=1)
    elif 'é›†ä¸­åˆ¤å®š' in df_feat.columns: df_feat['focus_intensity'] = df_feat['é›†ä¸­åˆ¤å®š'] * 100 
    else: df_feat['focus_intensity'] = 50.0
        
    win_size_5m = max(1, int(pd.Timedelta('5T') / freq_td))
    df_feat['focus_smooth'] = df_feat['focus_intensity'].rolling(window=win_size_5m, min_periods=1).mean()
    df_feat['focus_diff'] = df_feat['focus_smooth'].diff()
    df_feat['phase_num'] = np.where(df_feat['focus_diff'] > 0, 1, np.where(df_feat['focus_diff'] < 0, -1, 0))
    df_feat['phase_str'] = np.where(df_feat['phase_num'] > 0, 'ä¸Šæ˜‡å±€é¢ â†—', np.where(df_feat['phase_num'] < 0, 'ä¸‹é™å±€é¢ â†˜', 'åœæ»'))
    
    dist_steps = max(1, int(pd.Timedelta('15T') / freq_td))
    prominence = df_feat['focus_smooth'].std() * 0.2
    if pd.isna(prominence) or prominence == 0: prominence = 0.1
    
    fs_arr = df_feat['focus_smooth'].fillna(0).values
    peaks, _ = signal.find_peaks(fs_arr, distance=dist_steps, prominence=prominence)
    valleys, _ = signal.find_peaks(-fs_arr, distance=dist_steps, prominence=prominence)
    
    df_feat['is_peak'] = 0
    if len(peaks) > 0: df_feat.iloc[peaks, df_feat.columns.get_loc('is_peak')] = 1
    df_feat['is_valley'] = 0
    if len(valleys) > 0: df_feat.iloc[valleys, df_feat.columns.get_loc('is_valley')] = 1
    
    df_feat['last_peak_val'] = df_feat['focus_smooth'].where(df_feat['is_peak'] == 1).ffill()
    df_feat['last_valley_val'] = df_feat['focus_smooth'].where(df_feat['is_valley'] == 1).ffill()
    
    idx_series = pd.Series(df_feat.index, index=df_feat.index)
    df_feat['last_peak_time'] = idx_series.where(df_feat['is_peak'] == 1).ffill()
    df_feat['wave_amplitude'] = (df_feat['last_peak_val'] - df_feat['last_valley_val']).fillna(0)
    
    df_feat['prev_peak_time'] = df_feat['last_peak_time'].where(df_feat['is_peak']==1).shift(1).ffill()
    df_feat['wave_period_min'] = (df_feat['last_peak_time'] - df_feat['prev_peak_time']).dt.total_seconds() / 60
    df_feat['wave_period_min'] = df_feat['wave_period_min'].fillna(0)
    
    q70 = df_feat['focus_smooth'].quantile(0.70)
    if pd.isna(q70): q70 = 50.0
    df_feat['is_high_focus_wave'] = (df_feat['focus_smooth'] >= q70).astype(int)
    
    df_feat['has_schedule'] = 0
    df_feat['is_meeting'] = 0
    if df_sched is not None and not df_sched.empty:
        meeting_keywords = ['ä¼šè­°', 'æ‰“åˆã›', 'MTG', 'é¢è«‡', 'å•†è«‡']
        for _, row in df_sched.iterrows():
            mask = (df_feat.index < row['end_dt']) & ((df_feat.index + freq_td) > row['start_dt'])
            df_feat.loc[mask, 'has_schedule'] = 1
            if any(kw in str(row.get('ä»¶å', '')) for kw in meeting_keywords):
                df_feat.loc[mask, 'is_meeting'] = 1
                
    win_steps_2h = max(1, int(pd.Timedelta('2H') / freq_td))
    df_feat['schedule_density_2h'] = df_feat['has_schedule'].rolling(win_steps_2h, min_periods=1).mean().shift(1).fillna(0)
    df_feat['deep_work'] = ((df_feat['has_schedule'] == 0) & (df_feat['is_high_focus_wave'] == 1)).astype(int)
    
    dw_series = df_feat['deep_work']
    df_feat['dw_block_id'] = (dw_series != dw_series.shift()).cumsum()
    df_feat['dw_block_id'] = df_feat['dw_block_id'].where(dw_series == 1, np.nan)
    
    df_feat['hour'] = df_feat.index.hour
    df_feat['dayofweek'] = df_feat.index.dayofweek
    return df_feat, q70

def compute_personal_metrics(df_feat, freq_td, current_time):
    metrics = {}
    mins_per_step = freq_td.total_seconds() / 60
    df_feat['date'] = df_feat.index.date
    
    block_lengths = df_feat.groupby('dw_block_id').size() * mins_per_step
    metrics['avg_dw_duration'] = block_lengths.mean() if not block_lengths.empty else 0
    metrics['dw_loss_minutes_total'] = block_lengths[block_lengths < 30].sum() if not block_lengths.empty else 0
    
    valid_periods = df_feat['wave_period_min'][df_feat['wave_period_min'] > 0]
    metrics['avg_wave_period'] = valid_periods.median() if not valid_periods.empty else 18.0
    metrics['avg_wave_amplitude'] = df_feat['wave_amplitude'][df_feat['wave_amplitude'] > 0].mean()
    if pd.isna(metrics['avg_wave_amplitude']): metrics['avg_wave_amplitude'] = 10.0
    
    total_blank_steps = (df_feat['has_schedule'] == 0).sum()
    total_dw_steps = df_feat['deep_work'].sum()
    metrics['dw_rate'] = (total_dw_steps / total_blank_steps * 100) if total_blank_steps > 0 else 0
    
    past_28_days = current_time.date() - pd.Timedelta(days=28)
    df_past = df_feat[(df_feat['date'] >= past_28_days) & (df_feat['date'] < current_time.date())]
    df_past_weekday = df_past[df_past['dayofweek'] < 5]
    
    if not df_past_weekday.empty:
        past_daily_dw = df_past_weekday.groupby('date')['deep_work'].sum() * mins_per_step
        target_raw = past_daily_dw.mean() * 1.10
        metrics['target_dw_mins'] = int(round(target_raw / 5.0) * 5)
    else:
        metrics['target_dw_mins'] = 120
    if metrics['target_dw_mins'] == 0: metrics['target_dw_mins'] = 60
    
    today_data = df_feat[df_feat['date'] == current_time.date()]
    today_blank_steps = (today_data['has_schedule'] == 0).sum()
    today_dw_steps = today_data['deep_work'].sum()
    metrics['today_dw_mins'] = today_dw_steps * mins_per_step
    metrics['today_dw_rate'] = (today_dw_steps / today_blank_steps * 100) if today_blank_steps > 0 else 0
    
    today_blocks = today_data.groupby('dw_block_id').size() * mins_per_step
    metrics['today_dw_loss'] = today_blocks[today_blocks < 30].sum() if not today_blocks.empty else 0
    return metrics

def train_predict_classifier(df_feat, ahead_steps):
    df_feat['target_class'] = df_feat['is_high_focus_wave'].shift(-ahead_steps)
    feature_cols = ['hour', 'dayofweek', 'wave_amplitude', 'wave_period_min', 'phase_num', 'schedule_density_2h']
    for col in ['1åˆ†é–“æ­©æ•°', 'SkinTemp']:
        if col in df_feat.columns: feature_cols.append(col)
    
    df_model = df_feat.dropna(subset=['target_class'] + feature_cols).copy()
    if len(df_model) < 50: return None, None, {}, df_feat
        
    split_idx = int(len(df_model) * 0.8)
    train_df = df_model.iloc[:split_idx]
    test_df = df_model.iloc[split_idx:]
    
    X_train, y_train = train_df[feature_cols], train_df['target_class']
    X_test, y_test = test_df[feature_cols], test_df['target_class']
    if y_train.nunique() <= 1: return None, None, {}, df_feat
        
    model = lgb.LGBMClassifier(objective='binary', n_estimators=100, learning_rate=0.05, random_state=42, verbose=-1)
    model.fit(X_train, y_train)
    
    eval_metrics = {}
    if y_test.nunique() > 1:
        preds_proba = model.predict_proba(X_test)[:, 1]
        preds_bin = (preds_proba >= 0.5).astype(int)
        eval_metrics['ROC-AUC'] = roc_auc_score(y_test, preds_proba)
        eval_metrics['PR-AUC'] = average_precision_score(y_test, preds_proba)
        eval_metrics['F1 Score'] = f1_score(y_test, preds_bin)
        eval_metrics['Brier Score'] = brier_score_loss(y_test, preds_proba)
    return model, feature_cols, eval_metrics, df_model

# ==========================================
# ğŸ¯ C. ã‚¿ã‚¹ã‚¯æœ€é©é…ç½®ã‚¨ãƒ³ã‚¸ãƒ³ (This Week Design)
# ==========================================
def add_1min_focus_wave(df_1min):
    df = df_1min.copy()
    focus_components = []
    if 'CVRR_SCORE_NEW' in df.columns: focus_components.append(df['CVRR_SCORE_NEW'])
    if 'RMSSD_SCORE_NEW' in df.columns: focus_components.append(100 - df['RMSSD_SCORE_NEW'])
    if 'LFHF_SCORE_NEW' in df.columns: focus_components.append(df['LFHF_SCORE_NEW'])
    
    if focus_components: df['focus_intensity'] = pd.concat(focus_components, axis=1).mean(axis=1)
    elif 'é›†ä¸­åˆ¤å®š' in df.columns: df['focus_intensity'] = df['é›†ä¸­åˆ¤å®š'] * 100
    else: df['focus_intensity'] = 50.0
        
    df['focus_smooth'] = df['focus_intensity'].rolling(window=5, min_periods=1).mean()
    q70 = df['focus_smooth'].quantile(0.70) if not df['focus_smooth'].isna().all() else 50.0
    df['is_high_focus_wave'] = (df['focus_smooth'] >= q70).astype(int)
    
    peaks, _ = signal.find_peaks(df['focus_smooth'].fillna(0).values, distance=15, prominence=df['focus_smooth'].std()*0.2 if df['focus_smooth'].std()>0 else 0.1)
    valleys, _ = signal.find_peaks(-df['focus_smooth'].fillna(0).values, distance=15, prominence=df['focus_smooth'].std()*0.2 if df['focus_smooth'].std()>0 else 0.1)
    
    df['is_peak'] = 0
    if len(peaks) > 0: df.iloc[peaks, df.columns.get_loc('is_peak')] = 1
    df['is_valley'] = 0
    if len(valleys) > 0: df.iloc[valleys, df.columns.get_loc('is_valley')] = 1
    
    df['last_peak_val'] = df['focus_smooth'].where(df['is_peak'] == 1).ffill()
    df['last_valley_val'] = df['focus_smooth'].where(df['is_valley'] == 1).ffill()
    df['wave_amplitude'] = (df['last_peak_val'] - df['last_valley_val']).fillna(0)
    return df

def extract_free_blocks(df_sched_raw, start_dt, end_dt):
    idx = pd.date_range(start=start_dt.ceil('1T'), end=end_dt.floor('1T'), freq='1T')
    df_dummy = pd.DataFrame(index=idx)
    df_dummy = df_dummy[(df_dummy.index.hour >= 9) & (df_dummy.index.hour < 19)]
    df_dummy = df_dummy[df_dummy.index.dayofweek < 5]
    
    df_dummy['has_schedule'] = 0
    if df_sched_raw is not None and not df_sched_raw.empty:
        for _, row in df_sched_raw.iterrows():
            mask = (df_dummy.index >= row['start_dt']) & (df_dummy.index < row['end_dt'])
            df_dummy.loc[mask, 'has_schedule'] = 1
            
    # GAP_TOLERANCE ä»¥ä¸‹ã®äºˆå®šã¯ç„¡è¦–
    sched_blocks = (df_dummy['has_schedule'] != df_dummy['has_schedule'].shift()).cumsum()
    for b_id, b_df in df_dummy[df_dummy['has_schedule'] == 1].groupby(sched_blocks):
        if len(b_df) <= GAP_TOLERANCE:
            df_dummy.loc[b_df.index, 'has_schedule'] = 0
            
    free_blocks_id = (df_dummy['has_schedule'] != df_dummy['has_schedule'].shift()).cumsum()
    blocks = []
    for b_id, b_df in df_dummy[df_dummy['has_schedule'] == 0].groupby(free_blocks_id):
        for d, d_df in b_df.groupby(b_df.index.date):
            duration = len(d_df)
            if duration >= MIN_DEEP_DURATION:
                blocks.append({
                    'date': d, 'start_dt': d_df.index[0], 'end_dt': d_df.index[-1] + pd.Timedelta(minutes=1),
                    'duration': duration, 'hour': d_df.index[0].hour
                })
    return blocks

def evaluate_deep_success(df_1min, block, fatigue_drift_th):
    b_df = df_1min[(df_1min.index >= block['start_dt']) & (df_1min.index < block['end_dt'])]
    if len(b_df) < MIN_DEEP_DURATION: return 0
        
    # A: é«˜é›†ä¸­ãŒé€£ç¶š5åˆ†ä»¥ä¸Š
    focus = b_df.get('is_high_focus_wave', pd.Series(0, index=b_df.index))
    if focus.sum() == 0: cond_A = False
    else:
        focus_streaks = focus.groupby((focus != focus.shift()).cumsum()).sum()
        cond_A = focus_streaks.max() >= FOCUS_STREAK_MIN
        
    cond_B = True # ãƒ–ãƒ­ãƒƒã‚¯æŠ½å‡ºæ™‚ã«å¯¾å‡¦æ¸ˆ
    
    # C: fatigue_smoothã®å‚¾ããŒä¸Šä½20%ä»¥ä¸‹
    fatigue = b_df.get('fatigue_smooth', pd.Series(0, index=b_df.index)).dropna()
    if len(fatigue) > 10:
        x = np.arange(len(fatigue))
        slope = np.polyfit(x, fatigue.values, 1)[0]
    else: slope = 0
    cond_C = slope <= fatigue_drift_th
    
    return 1 if (cond_A and cond_B and cond_C) else 0

def compute_hourly_profile(df_1min, df_sched_raw, current_time):
    past_start = current_time - pd.Timedelta(weeks=4)
    past_blocks = extract_free_blocks(df_sched_raw, past_start, current_time)
    df_1m = add_1min_focus_wave(df_1min)
    
    slopes = []
    for b in past_blocks:
        b_df = df_1m[(df_1m.index >= b['start_dt']) & (df_1m.index < b['end_dt'])]
        fatigue = b_df.get('fatigue_smooth', pd.Series(dtype=float)).dropna()
        if len(fatigue) > 10:
            slopes.append(np.polyfit(np.arange(len(fatigue)), fatigue.values, 1)[0])
    fatigue_drift_th = np.percentile(slopes, 80) if slopes else 0.5
    
    for b in past_blocks:
        b['deep_success'] = evaluate_deep_success(df_1m, b, fatigue_drift_th)
        
    df_blocks = pd.DataFrame(past_blocks)
    if not df_blocks.empty and 'hour' in df_blocks.columns:
        success_rate = df_blocks.groupby('hour')['deep_success'].mean()
    else:
        success_rate = pd.Series(0, index=np.arange(9, 19))
        
    df_1m['hour'] = df_1m.index.hour
    daytime_df = df_1m[(df_1m['hour'] >= 9) & (df_1m['hour'] <= 18)]
    
    mean_amp = daytime_df.groupby('hour')['wave_amplitude'].mean() if 'wave_amplitude' in daytime_df else pd.Series(0)
    mean_fat = daytime_df.groupby('hour')['fatigue_smooth'].mean() if 'fatigue_smooth' in daytime_df else pd.Series(0)
    mean_aro = daytime_df.groupby('hour')['low_arousal'].mean() if 'low_arousal' in daytime_df else pd.Series(0)
    
    if 'is_high_focus_wave' in daytime_df:
        focus_series = daytime_df['is_high_focus_wave']
        focus_blocks = focus_series.groupby((focus_series != focus_series.shift()).cumsum())
        df_dur = pd.DataFrame({'duration': focus_blocks.sum(), 'hour': daytime_df['hour'].groupby((focus_series != focus_series.shift()).cumsum()).first()})
        mean_dur = df_dur[df_dur['duration'] > 0].groupby('hour')['duration'].mean()
    else:
        mean_dur = pd.Series(0)
        
    profile = pd.DataFrame({
        'success_rate': success_rate, 'mean_amp': mean_amp, 'mean_dur': mean_dur, 'mean_fat': mean_fat, 'mean_aro': mean_aro
    }).reindex(np.arange(9, 19)).fillna(0)
    
    def z_score(s):
        if s.std() == 0: return s - s.mean()
        return (s - s.mean()) / s.std()
        
    profile['suitability'] = z_score(profile['success_rate']) + z_score(profile['mean_dur']) - z_score(profile['mean_fat']) - z_score(profile['mean_aro'])
    
    def get_level(s):
        if s.nunique() <= 1: return pd.Series('ä¸­', index=s.index)
        try: return pd.qcut(s, 3, labels=['ä½', 'ä¸­', 'é«˜'], duplicates='drop').astype(str)
        except: return pd.Series('ä¸­', index=s.index)
            
    profile['focus_level'] = get_level(profile['mean_dur'] + profile['mean_amp'])
    profile['fatigue_level'] = get_level(profile['mean_fat'])
    return profile

def optimize_next_week(df_sched_raw, hourly_profile, current_time):
    future_start = current_time
    future_end = current_time + pd.Timedelta(days=7)
    future_blocks = extract_free_blocks(df_sched_raw, future_start, future_end)
    
    for b in future_blocks:
        h = b['hour']
        base_score = hourly_profile.loc[h, 'suitability'] if h in hourly_profile.index else 0
        penalty = 0
        if df_sched_raw is not None and not df_sched_raw.empty:
            prev_s = df_sched_raw[(df_sched_raw['end_dt'] > b['start_dt'] - pd.Timedelta(minutes=30)) & (df_sched_raw['end_dt'] <= b['start_dt'])]
            next_s = df_sched_raw[(df_sched_raw['start_dt'] >= b['end_dt']) & (df_sched_raw['start_dt'] < b['end_dt'] + pd.Timedelta(minutes=30))]
            if not prev_s.empty: penalty -= 0.5
            if not next_s.empty: penalty -= 0.5
        b['score'] = base_score + penalty
        
    future_blocks.sort(key=lambda x: x['score'], reverse=True)
    deep_blocks = []
    used_dates = set()
    
    for b in future_blocks:
        if len(deep_blocks) >= WEEKLY_MIN_DEEP: break
        if b['date'] not in used_dates:
            deep_blocks.append(b)
            used_dates.add(b['date'])
            
    other_blocks = [b for b in future_blocks if b not in deep_blocks]
    for b in other_blocks:
        h = b['hour']
        focus_l = hourly_profile.loc[h, 'focus_level'] if h in hourly_profile.index else 'ä¸­'
        fatigue_l = hourly_profile.loc[h, 'fatigue_level'] if h in hourly_profile.index else 'ä¸­'
        
        if focus_l == 'é«˜' and fatigue_l == 'ä½': b['type'] = 'ä»•ä¸Šã’ (Execution)'
        elif focus_l == 'ä¸­' and fatigue_l == 'ä¸­': b['type'] = 'æ·±æ€è€ƒäºˆå‚™'
        else: b['type'] = 'è»½æ€è€ƒ (Light Thinking)'
            
    return deep_blocks, other_blocks

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼UI ---
with st.sidebar:
    st.header("âš™ï¸ ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")
    file_ts = st.file_uploader("1. ç”Ÿä½“ãƒ‡ãƒ¼ã‚¿ (CSV)", type=['csv'])
    file_sched = st.file_uploader("2. äºˆå®šè¡¨ãƒ‡ãƒ¼ã‚¿ (CSV) â€»å¿…é ˆ", type=['csv'])
    
    with st.expander("ğŸ›  æ³¢è§£æãƒ»è©³ç´°è¨­å®š (ç®¡ç†è€…ç”¨)"):
        RESAMPLE_FREQ = st.selectbox("åˆ†æå˜ä½ (æ³¢è§£åƒåº¦)", ['1T', '5T', '10T', '30T'], index=1)
        PREDICT_AHEAD_MINS = st.selectbox("äºˆæ¸¬å…ˆ (åˆ†)", [30, 60], index=0)
        TARGET_DATETIME_STR = st.text_input("äºˆæ¸¬åŸºæº–æ—¥æ™‚ (ç©ºæ¬„ã§æœ€æ–°)")
        
        st.markdown("**ğŸ“… åˆ†æå¯¾è±¡ãƒ•ã‚£ãƒ«ã‚¿**")
        dow_options = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]
        selected_dows = st.multiselect("åˆ†æå¯¾è±¡ã¨ã™ã‚‹æ›œæ—¥", dow_options, default=dow_options[0:5])
        time_range = st.slider("ã‚°ãƒ©ãƒ•è¡¨ç¤ºæ™‚é–“å¸¯", 0, 23, (9, 19))
        
        selected_dow_indices = [dow_options.index(d) for d in selected_dows]
        
    st.markdown("---")
    run_btn = st.button("ğŸš€ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³ã‚’è§£æ", type="primary", use_container_width=True)

freq_td = pd.Timedelta(RESAMPLE_FREQ)
ahead_steps = max(1, int(pd.Timedelta(minutes=PREDICT_AHEAD_MINS) / freq_td))
TARGET_DATETIME = TARGET_DATETIME_STR if TARGET_DATETIME_STR.strip() != "" else None

# === ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ===
if run_btn or file_ts is not None:
    if file_ts is None:
        st.info("ğŸ‘ˆ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã€Œç”Ÿä½“ãƒ‡ãƒ¼ã‚¿ã€ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        st.stop()
        
    with st.spinner("AIãŒé›†ä¸­ãƒ»ç–²åŠ´ãƒ»è¦šé†’ã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’è§£æä¸­..."):
        df_ts_raw = pd.read_csv(io.BytesIO(file_ts.getvalue()), skiprows=2)
        df_ts_raw['timestamp_clean'] = df_ts_raw['timestamp'].astype(str).str.split(' GMT').str[0]
        df_ts_raw['datetime'] = pd.to_datetime(df_ts_raw['timestamp_clean'], errors='coerce')
        df_ts_raw = df_ts_raw.dropna(subset=['datetime']).set_index('datetime').sort_index()

        df_sched_raw = None
        if file_sched:
            df_sched_raw = pd.read_csv(io.BytesIO(file_sched.getvalue()))
            df_sched_raw = df_sched_raw[df_sched_raw['çµ‚æ—¥ã‚¤ãƒ™ãƒ³ãƒˆ'].astype(str).str.upper() != 'TRUE']
            df_sched_raw['start_dt'] = pd.to_datetime(df_sched_raw['é–‹å§‹æ—¥'].astype(str) + ' ' + df_sched_raw['é–‹å§‹æ™‚åˆ»'].astype(str), errors='coerce')
            df_sched_raw['end_dt']   = pd.to_datetime(df_sched_raw['çµ‚äº†æ—¥'].astype(str) + ' ' + df_sched_raw['çµ‚äº†æ™‚åˆ»'].astype(str), errors='coerce')
            df_sched_raw = df_sched_raw.dropna(subset=['start_dt', 'end_dt']).sort_values('start_dt')
            
        num_cols = df_ts_raw.select_dtypes(include=[np.number]).columns
        df_1min = df_ts_raw[num_cols].resample('1T').mean()
        if '1åˆ†é–“æ­©æ•°' in df_ts_raw.columns:
            df_1min['1åˆ†é–“æ­©æ•°'] = df_ts_raw['1åˆ†é–“æ­©æ•°'].resample('1T').sum()
        df_1min = df_1min.ffill(limit=5)
        
        steps_col_name = '1åˆ†é–“æ­©æ•°' if '1åˆ†é–“æ­©æ•°' in df_1min.columns else None
        df_1min = compute_fatigue_features(df_1min, steps_col=steps_col_name)
        df_1min = compute_low_arousal(df_1min, pr_col='PR_SCORE_NEW' if 'PR_SCORE_NEW' in df_1min.columns else None, steps_col=steps_col_name)

        df_resampled = df_ts_raw[num_cols].resample(RESAMPLE_FREQ).mean()
        if '1åˆ†é–“æ­©æ•°' in df_ts_raw.columns:
            df_resampled['1åˆ†é–“æ­©æ•°'] = df_ts_raw['1åˆ†é–“æ­©æ•°'].resample(RESAMPLE_FREQ).sum()
            
        df_feat, q70_thresh = make_wave_features(df_resampled, df_sched_raw, freq_td)
        
        if TARGET_DATETIME:
            try:
                current_time = pd.to_datetime(TARGET_DATETIME)
                target_data_all = df_feat[df_feat.index <= current_time]
                target_data = target_data_all.iloc[-1:] if not target_data_all.empty else df_feat.iloc[-1:]
            except:
                target_data = df_feat.iloc[-1:]
        else:
            target_data = df_feat.iloc[-1:]
        current_time = target_data.index[0]
        
        metrics = compute_personal_metrics(df_feat, freq_td, current_time)
        model, feature_cols, eval_metrics, df_model = train_predict_classifier(df_feat, ahead_steps)
        
        focus_prob = model.predict_proba(target_data[feature_cols])[0, 1] if model is not None else 0.0

        current_1min = df_1min[df_1min.index <= current_time]
        cur_1m = current_1min.iloc[-1] if not current_1min.empty else df_1min.iloc[-1]
        
        fatigue_band = cur_1m.get('fatigue_level_band', 'ä¸æ˜')
        fatigue_drift = cur_1m.get('fatigue_drift_60m', 0.0)
        drift_str = "è“„ç©ä¸­ â†—" if fatigue_drift > 0.05 else "å›å¾©å‚¾å‘ â†˜" if fatigue_drift < -0.05 else "æ¨ªã°ã„ â†’"
        la_band = cur_1m.get('low_arousal_band', 'ä¸æ˜')
        la_rise = cur_1m.get('low_arousal_rise_15m', 0.0)
        la_str = "ä¸Šæ˜‡ä¸­ âš ï¸" if la_rise > 0.5 else "å®‰å®š"

        current_phase = target_data['phase_str'].values[0]
        avg_period = metrics['avg_wave_period']
        last_peak_time_val = target_data['last_peak_time'].values[0]
        if pd.notna(last_peak_time_val):
            mins_since_peak = (current_time - pd.to_datetime(last_peak_time_val)).total_seconds() / 60
            next_peak_in = max(0, int(avg_period - mins_since_peak))
        else:
            next_peak_in = int(avg_period)

        next_chance_text = "æœ¬æ—¥ã¯çµ‚äº†ã€ã¾ãŸã¯ç©ºãæ™‚é–“ãŒã‚ã‚Šã¾ã›ã‚“"
        if current_time.hour < 19:
            end_of_day = current_time.replace(hour=19, minute=0, second=0)
            future_mask = (df_feat.index > current_time) & (df_feat.index <= end_of_day) & (df_feat['has_schedule'] == 0)
            future_blank_times = df_feat[future_mask].index
            if not future_blank_times.empty:
                blank_blocks = (future_mask != future_mask.shift()).cumsum()[future_mask]
                longest_block_id = blank_blocks.value_counts().idxmax()
                best_block_times = future_blank_times[blank_blocks == longest_block_id]
                if len(best_block_times) > 0:
                    next_chance_text = f"{best_block_times[0].strftime('%H:%M')} â€“ {(best_block_times[-1] + freq_td).strftime('%H:%M')}"

        is_focus_low = focus_prob < 0.4
        action_text = "ç¾åœ¨ã®ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³ã¯å®‰å®šã—ã¦ã„ã¾ã™ã€‚ã“ã®ã¾ã¾æ³¢ã«ä¹—ã£ã¦Deep Workã‚’é€²ã‚ã¾ã—ã‚‡ã†ã€‚"
        if la_band == 'é«˜' and is_focus_low: action_text = "é›†ä¸­åŠ›ãŒä½ä¸‹ã—ã€çœ æ°—ï¼ˆä½è¦šé†’ï¼‰ãŒé«˜ã¾ã£ã¦ã„ã¾ã™ã€‚çŸ­ã„æ­©è¡Œã‚„è»½ã„ã‚¹ãƒˆãƒ¬ãƒƒãƒã§è„³ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã—ã¾ã—ã‚‡ã†ã€‚"
        elif la_band == 'é«˜' and fatigue_band == 'é«˜': action_text = "ç–²åŠ´ã¨çœ æ°—ãŒãƒ”ãƒ¼ã‚¯ã«é”ã—ã¦ã„ã¾ã™ã€‚ç„¡ç†ãªä½œæ¥­ã¯æ§ãˆã€å®Œå…¨ãªä¼‘æ¯ã‚’å–ã‚‹ã“ã¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚"
        elif la_band == 'é«˜' and fatigue_band == 'ä½': action_text = "ç–²åŠ´ã¯å°‘ãªã„ã§ã™ãŒã€å˜èª¿ã•ã‹ã‚‰çœ æ°—ãŒç”Ÿã˜ã¦ã„ã¾ã™ã€‚å°‘ã—ç«‹ã¡ä¸ŠãŒã£ã¦æ­©ããªã©ã€å§¿å‹¢ã‚’å¤‰ãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚"

    # ==========================================
    # UI æç”»
    # ==========================================
    st.markdown(f"<p style='text-align: right; color: gray;'>æœ€çµ‚æ›´æ–°: {current_time.strftime('%Y/%m/%d %H:%M')}</p>", unsafe_allow_html=True)
    
    tab_today, tab_weekly, tab_design, tab_spec = st.tabs(["ğŸŒŠ Today", "ğŸ“Š Weekly Report", "ğŸ¯ This Week Design", "ğŸ‘¤ My Spec"])

    # --- TAB 1: Today ---
    with tab_today:
        col_m1, col_m2, col_m3 = st.columns([1, 1, 1])
        with col_m1:
            remain_dw = max(0, metrics['target_dw_mins'] - metrics['today_dw_mins'])
            achieved_color = "ğŸŸ¢ ç›®æ¨™ã‚¯ãƒªã‚¢ï¼" if remain_dw == 0 else f"ç›®æ¨™ã¾ã§ã‚ã¨ {remain_dw} åˆ†"
            st.markdown(f"""
            <div class="kpi-card" style="border-top: 5px solid #0f172a; height: 100%;">
                <div class="kpi-title">ä»Šæ—¥ã®Deep Worké”æˆçŠ¶æ³</div>
                <div class="kpi-value-main">{int(metrics['today_dw_mins'])} <span class="kpi-unit">/ {metrics['target_dw_mins']} åˆ†</span></div>
                <div class="kpi-sub {'alert' if remain_dw > 60 else ''}">{achieved_color}</div>
            </div>
            """, unsafe_allow_html=True)
            
        with col_m2:
            phase_color = "#ef4444" if "ä¸‹é™" in current_phase else "#10b981" if "ä¸Šæ˜‡" in current_phase else "#64748b"
            st.markdown(f"""
            <div class="kpi-card" style="border-top: 5px solid #3b82f6; height: 100%;">
                <div class="kpi-title">ç¾åœ¨ã®é›†ä¸­æ³¢ãƒ•ã‚§ãƒ¼ã‚º</div>
                <div class="kpi-value-wave" style="color: {phase_color};">{current_phase}</div>
                <div class="kpi-sub" style="color:#64748b; font-weight:normal;">æ¬¡ã®é›†ä¸­ãƒ”ãƒ¼ã‚¯äºˆæƒ³: ç´„ <strong>{next_peak_in} åˆ†å¾Œ</strong></div>
            </div>
            """, unsafe_allow_html=True)
            
        with col_m3:
            prob_color = "#10b981" if focus_prob > 0.6 else "#f59e0b" if focus_prob > 0.4 else "#ef4444"
            st.markdown(f"""
            <div class="kpi-card" style="border-top: 5px solid #8b5cf6; height: 100%;">
                <div class="kpi-title">{PREDICT_AHEAD_MINS}åˆ†å¾Œã® é«˜é›†ä¸­æ³¢ å†çªå…¥ç¢ºç‡</div>
                <div class="kpi-value-main" style="color: {prob_color};">{focus_prob * 100:.1f} <span class="kpi-unit">%</span></div>
                <div class="kpi-sub" style="color:#64748b; font-weight:normal;">ä¸Šä½30%ã®ã‚¾ãƒ¼ãƒ³ã«åˆ°é”ã™ã‚‹ç¢ºç‡</div>
            </div>
            """, unsafe_allow_html=True)
            
        st.markdown("### ğŸ”‹ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³ (ç–²åŠ´ãƒ»è¦šé†’)")
        col_c1, col_c2, col_c3 = st.columns([1, 1, 1.5])
        with col_c1:
            f_color = "#ef4444" if fatigue_band == 'é«˜' else "#10b981" if fatigue_band == 'ä½' else "#f59e0b"
            st.markdown(f"""
            <div class="kpi-card" style="border-top: 5px solid {f_color}; padding: 15px;">
                <div class="kpi-title" style="font-size:0.9rem;">ç¾åœ¨ã®ç–²åŠ´ãƒ¬ãƒ™ãƒ«</div>
                <div style="font-size:2rem; font-weight:bold; color:{f_color};">{fatigue_band}</div>
                <div style="font-size:0.9rem; color:#64748b;">ãƒˆãƒ¬ãƒ³ãƒ‰: {drift_str}</div>
            </div>
            """, unsafe_allow_html=True)
            
        with col_c2:
            la_color = "#ef4444" if la_band == 'é«˜' else "#10b981" if la_band == 'ä½' else "#f59e0b"
            st.markdown(f"""
            <div class="kpi-card" style="border-top: 5px solid {la_color}; padding: 15px;">
                <div class="kpi-title" style="font-size:0.9rem;">ç¾åœ¨ã®ä½è¦šé†’ (çœ æ°—)</div>
                <div style="font-size:2rem; font-weight:bold; color:{la_color};">{la_band}</div>
                <div style="font-size:0.9rem; color:#64748b;">çŠ¶æ…‹: {la_str}</div>
            </div>
            """, unsafe_allow_html=True)
            
        with col_c3:
            st.markdown(f"""
            <div class="chance-box" style="margin-bottom: 0; background-color: #f8fafc; border-left: 6px solid #3b82f6;">
                <div class="kpi-title" style="color: #1e293b; font-size:0.9rem;">ğŸ¤– AIã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆ</div>
                <div style="font-size: 1.1rem; color: #334155; margin-top: 10px; font-weight: 500;">{action_text}</div>
            </div>
            """, unsafe_allow_html=True)

        st.markdown("<br>", unsafe_allow_html=True)
        col_s1, col_s2 = st.columns([1, 1.5])
        with col_s1:
            st.markdown(f"""
            <div style="display: flex; gap: 10px;">
                <div class="kpi-card" style="flex: 1; padding: 15px;">
                    <div class="kpi-title" style="font-size:0.85rem;">ç©ºç™½æ™‚é–“ã®é›†ä¸­ç‡</div>
                    <div style="font-size:1.8rem; font-weight:bold; color:#334155;">{metrics['today_dw_rate']:.1f} <span style="font-size:1rem;">%</span></div>
                </div>
                <div class="kpi-card" style="flex: 1; padding: 15px;">
                    <div class="kpi-title" style="font-size:0.85rem;">åˆ†æ–­ãƒ­ã‚¹(æ³¢ã®é “æŒ«)</div>
                    <div style="font-size:1.8rem; font-weight:bold; color:#334155;">{int(metrics['today_dw_loss'])} <span style="font-size:1rem;">åˆ†</span></div>
                </div>
            </div>
            """, unsafe_allow_html=True)
        with col_s2:
            st.markdown(f"""
            <div class="chance-box" style="margin-bottom: 0;">
                <div class="kpi-title" style="color: #047857;">ğŸ¯ æ¬¡ã®Deep Workãƒãƒ£ãƒ³ã‚¹æ </div>
                <div class="chance-time">{next_chance_text}</div>
                <div style="font-size: 0.95rem; color: #065f46; margin-top: 8px;">ã“ã®æ™‚é–“ã‚’æ­»å®ˆã—ã€æ³¢ã«ä¹—ã£ã¦é‡è¦ã‚¿ã‚¹ã‚¯ã‚’æ¶ˆåŒ–ã—ã¦ãã ã•ã„ã€‚</div>
            </div>
            """, unsafe_allow_html=True)

    # --- TAB 2: Weekly Report ---
    with tab_weekly:
        st.markdown("## ä»Šé€±ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³æŒ¯ã‚Šè¿”ã‚Š")
        # æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨åŒç­‰ã®ãŸã‚çœç•¥(è¡¨ç¤ºã®ã¿)
        df_this_week = df_feat[(df_feat['date'] > (current_time.date() - pd.Timedelta(days=7))) & (df_feat['date'] <= current_time.date())]
        tw_dw = df_this_week['deep_work'].sum() * (freq_td.total_seconds() / 60)
        st.metric("ä»Šé€±ã®Deep Workåˆè¨ˆæ™‚é–“", f"{int(tw_dw)} åˆ†")
        
        st.markdown("#### ğŸ“… æ—¥åˆ¥ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³ãƒ»ã‚µãƒãƒªãƒ¼ (ç–²åŠ´ã¨å›å¾©)")
        df_daily_cond = summarize_daily_condition(df_1min)
        if not df_daily_cond.empty: st.dataframe(df_daily_cond, use_container_width=True)

    # --- TAB 3: This Week Design (ã‚¿ã‚¹ã‚¯æœ€é©é…ç½®) ---
    with tab_design:
        st.markdown("## ğŸ¯ ã‚¿ã‚¹ã‚¯æœ€é©é…ç½® (This Week Design)")
        st.write("äºˆå®šè¡¨ã®ç©ºãæ™‚é–“ã¨éå»ã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ç‰¹æ€§ã‹ã‚‰ã€ä»Šå¾Œ7æ—¥é–“ã®æœ€é©ãªã‚¿ã‚¹ã‚¯é…ç½®ã‚’ææ¡ˆã—ã¾ã™ã€‚")
        
        if file_sched is None:
            st.warning("ã“ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã€Œäºˆå®šè¡¨ãƒ‡ãƒ¼ã‚¿ (CSV)ã€ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.spinner("ã‚¿ã‚¹ã‚¯é…ç½®ã‚’æœ€é©åŒ–ä¸­..."):
                try:
                    hourly_profile = compute_hourly_profile(df_1min, df_sched_raw, current_time)
                    deep_blocks, other_blocks = optimize_next_week(df_sched_raw, hourly_profile, current_time)
                    
                    col_d1, col_d2 = st.columns([1, 2])
                    with col_d1:
                        st.markdown("### ã€æ·±æ€è€ƒ ç¢ºä¿çŠ¶æ³ã€‘")
                        secured = len(deep_blocks)
                        shortage = max(0, WEEKLY_MIN_DEEP - secured)
                        st.metric("ç›®æ¨™", f"{WEEKLY_MIN_DEEP} æ ")
                        st.metric("ç¢ºä¿", f"{secured} æ ", f"{'-' + str(shortage) + ' ä¸è¶³' if shortage > 0 else 'é”æˆ'}")
                        
                    with col_d2:
                        st.markdown("### ğŸ‘‘ æ¨å¥¨ æ·±æ€è€ƒæ  (Deep Thinking)")
                        st.write("60åˆ†ä»¥ä¸Šé›†ä¸­ã§ãã‚‹ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¿ã‚¤ãƒ ã§ã™ã€‚æœ€é‡è¦ã‚¿ã‚¹ã‚¯ã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚")
                        if deep_blocks:
                            for b in deep_blocks:
                                dow_str = ['æœˆ','ç«','æ°´','æœ¨','é‡‘','åœŸ','æ—¥'][b['start_dt'].weekday()]
                                time_str = f"{b['start_dt'].strftime('%m/%d')} ({dow_str}) {b['start_dt'].strftime('%H:%M')} - {b['end_dt'].strftime('%H:%M')}"
                                succ_rate = hourly_profile.loc[b['hour'], 'success_rate'] * 100 if b['hour'] in hourly_profile.index else 0
                                st.success(f"**{time_str}** (æ™‚é–“å¸¯æˆåŠŸç‡: {succ_rate:.1f}%)")
                        else:
                            st.warning("ä»Šå¾Œ7æ—¥é–“ã«60åˆ†ä»¥ä¸Šã®ã¾ã¨ã¾ã£ãŸç©ºãæ™‚é–“ãŒã‚ã‚Šã¾ã›ã‚“ã€‚äºˆå®šã‚’èª¿æ•´ã—ã¦ãƒ–ãƒ­ãƒƒã‚¯ã‚’ç¢ºä¿ã—ã¦ãã ã•ã„ã€‚")
                    
                    st.markdown("---")
                    st.markdown("### ğŸ“ ãã®ä»–ã®æ¨å¥¨é…ç½®")
                    col_o1, col_o2 = st.columns(2)
                    
                    execution_blocks = [b for b in other_blocks if b['type'] == 'ä»•ä¸Šã’ (Execution)']
                    light_blocks = [b for b in other_blocks if b['type'] == 'è»½æ€è€ƒ (Light Thinking)']
                    
                    with col_o1:
                        st.markdown("#### âš¡ ä»•ä¸Šã’ (Execution)")
                        st.write("é›†ä¸­åŠ›ãŒé«˜ãç–²åŠ´ãŒå°‘ãªã„æ™‚é–“å¸¯ã§ã™ã€‚è³‡æ–™ã®å®Œæˆã‚„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã«å‘ã„ã¦ã„ã¾ã™ã€‚")
                        for b in execution_blocks[:3]:
                            st.info(f"{b['start_dt'].strftime('%m/%d %H:%M')} - {b['end_dt'].strftime('%H:%M')} ({b['duration']}åˆ†)")
                            
                    with col_o2:
                        st.markdown("#### ğŸ’¬ è»½æ€è€ƒ (Light Thinking)")
                        st.write("ç–²åŠ´ãŒæºœã¾ã‚Šã‚„ã™ã„æ™‚é–“å¸¯ã§ã™ã€‚ãƒ¡ãƒ¼ãƒ«è¿”ä¿¡ã‚„ãƒ«ãƒ¼ãƒãƒ³ä½œæ¥­ã€ãƒ–ãƒ¬ã‚¹ãƒˆã«å‘ã„ã¦ã„ã¾ã™ã€‚")
                        for b in light_blocks[:3]:
                            st.info(f"{b['start_dt'].strftime('%m/%d %H:%M')} - {b['end_dt'].strftime('%H:%M')} ({b['duration']}åˆ†)")

                    st.markdown("---")
                    st.markdown("### ğŸ“ æ™‚é–“å¸¯åˆ¥ Deep Suitability (æ·±æ€è€ƒ é©æ€§ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—)")
                    suitability = hourly_profile[['suitability']].copy()
                    fig_suit = px.bar(x=[f"{h}:00" for h in suitability.index], y=suitability['suitability'],
                                      labels={'x':'æ™‚é–“å¸¯', 'y':'é©æ€§ã‚¹ã‚³ã‚¢ (Z-score)'},
                                      title="æ™‚é–“å¸¯åˆ¥ã®æ·±æ€è€ƒé©æ€§ã‚¹ã‚³ã‚¢")
                    fig_suit.update_traces(marker_color='#8b5cf6')
                    st.plotly_chart(fig_suit, use_container_width=True)

                except Exception as e:
                    st.error(f"æœ€é©é…ç½®ã‚¨ãƒ³ã‚¸ãƒ³ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    # --- TAB 4: My Spec ---
    with tab_spec:
        st.markdown("## ğŸ‘¤ ã‚ãªãŸã®ã€Œé›†ä¸­ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã€æ”»ç•¥æ³•")
        st.write("éå»ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’æ³¢å½¢è§£æã—ã€ã‚ãªãŸå›ºæœ‰ã®é›†ä¸­ãƒªã‚ºãƒ ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
        best_hour = df_feat.groupby('hour')['deep_work'].sum().idxmax()
        
        c_spec1, c_spec2, c_spec3 = st.columns(3)
        c_spec1.metric("â± å¹³å‡é›†ä¸­æ³¢ å‘¨æœŸ", f"{int(metrics['avg_wave_period'])} åˆ†")
        c_spec2.metric("ğŸ¯ æœ€é©é›†ä¸­æ™‚é–“å¸¯", f"{best_hour}:00 å°")
        c_spec3.metric("ğŸ“ˆ æ³¢ã®å¹³å‡æŒ¯å¹…", f"{metrics['avg_wave_amplitude']:.1f} pt")