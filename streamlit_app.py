# -*- coding: utf-8 -*-
"""
ã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ« + Outlookã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« é›†ä¸­ãƒ»ç–²åŠ´äºˆæ¸¬ã‚¢ãƒ—ãƒª (Streamlitç‰ˆ)
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import lightgbm as lgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score, log_loss
from sklearn.model_selection import TimeSeriesSplit
from sklearn.tree import DecisionTreeRegressor, _tree
import google.generativeai as genai
import shap
import warnings
import math
import plotly.graph_objects as go
import plotly.express as px
from pathlib import Path
import matplotlib as mpl
import matplotlib.font_manager as fm

# --- Streamlit ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="é›†ä¸­ãƒ»ç–²åŠ´äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ", layout="wide")

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š (ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸOTFãƒ•ã‚©ãƒ³ãƒˆã‚’é©ç”¨)
font_path = Path(__file__).parent / "assets" / "fonts" / "NotoSansCJKjp-Regular.otf"
if font_path.exists():
    fm.fontManager.addfont(str(font_path))
    prop = fm.FontProperties(fname=str(font_path))
    mpl.rcParams["font.family"] = prop.get_name()
else:
    st.warning("âš ï¸ NotoSansCJKjp-Regular.otf ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚GitHubã§ `assets/fonts/` ãƒ•ã‚©ãƒ«ãƒ€å†…ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

mpl.rcParams["axes.unicode_minus"] = False

warnings.filterwarnings('ignore')

st.title("ğŸ§  ã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ«Ã—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« é›†ä¸­äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ")
st.markdown("""
ã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ«ãƒ‡ãƒã‚¤ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã¨äºˆå®šè¡¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ„ã¿åˆã‚ã›ã¦ã€æ•°æ™‚é–“å¾Œã®é›†ä¸­ã‚¹ã‚³ã‚¢ã‚’äºˆæ¸¬ã—ã€æ¨å¥¨ã™ã‚‹åƒãæ–¹ã‚’ææ¡ˆã—ã¾ã™ã€‚
""")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ (è¨­å®š) ---
st.sidebar.header("âš™ï¸ è¨­å®š")
api_key = st.sidebar.text_input("Gemini APIã‚­ãƒ¼ (çœç•¥æ™‚ã¯å›ºå®šãƒ«ãƒ¼ãƒ«ã§å‡ºåŠ›)", type="password")

st.sidebar.subheader("åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
RESAMPLE_FREQ = st.sidebar.selectbox("åˆ†æå˜ä½", ['10T', '30T', '1H'], index=1)
PREDICT_AHEAD = st.sidebar.selectbox("äºˆæ¸¬å…ˆ", ['10T', '30T', '1H'], index=1)
LOOKBACK_PERIOD = st.sidebar.selectbox("éå»å‚ç…§", ['1H', '2H', '3H'], index=1)
INTERPOLATE_LIMIT = st.sidebar.selectbox("è£œå®Œé™ç•Œ", ['10T', '30T', '1H'], index=1)

st.sidebar.subheader("äºˆæ¸¬åŸºæº–æ—¥æ™‚ (ä»»æ„)")
TARGET_DATETIME_STR = st.sidebar.text_input("ä¾‹ï¼‰2026-01-01 16:00 (ç©ºæ¬„ã§æœ€æ–°ãƒ‡ãƒ¼ã‚¿)")
TARGET_DATETIME = TARGET_DATETIME_STR if TARGET_DATETIME_STR.strip() != "" else None

freq_td = pd.Timedelta(RESAMPLE_FREQ)
ahead_td = pd.Timedelta(PREDICT_AHEAD)
lookback_td = pd.Timedelta(LOOKBACK_PERIOD)
interp_td = pd.Timedelta(INTERPOLATE_LIMIT)

ahead_steps = max(1, int(ahead_td / freq_td))
lookback_steps = max(2, int(lookback_td / freq_td))
interp_steps = max(1, int(interp_td / freq_td))

# --- äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®é¸æŠæ©Ÿèƒ½ã‚’è¿½åŠ  ---
TARGET_OPTIONS = {
    'é›†ä¸­åˆ¤å®š': 'é›†ä¸­åˆ¤å®š(1=é›†ä¸­)',
    'ç–²åŠ´åˆ¤å®š': 'ç–²åŠ´åˆ¤å®š(1=ç–²åŠ´)',
    'çœ æ°—åˆ¤å®š': 'çœ æ°—åˆ¤å®š(1=çœ æ°—)',
    'å¼·ã„çœ æ°—åˆ¤å®š': 'å¼·ã„çœ æ°—åˆ¤å®š(1=å¼·ã„çœ æ°—)'
}
st.sidebar.subheader("ğŸ¯ äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ")
selected_target_name = st.sidebar.selectbox("äºˆæ¸¬ã™ã‚‹æŒ‡æ¨™ã‚’é¸æŠ", list(TARGET_OPTIONS.values()), index=0)
target_col = [k for k, v in TARGET_OPTIONS.items() if v == selected_target_name][0]

# --- æ–°è¦: é•·æœŸåˆ†æç”¨ãƒ•ã‚£ãƒ«ã‚¿ ---
st.sidebar.subheader("ğŸ“… é•·æœŸåˆ†æãƒ•ã‚£ãƒ«ã‚¿ (ç‰¹æ€§ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”¨)")
dow_options = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]
selected_dows = st.sidebar.multiselect("å¯¾è±¡æ›œæ—¥", dow_options, default=dow_options)
time_range = st.sidebar.slider("å¯¾è±¡æ™‚é–“å¸¯", 0, 23, (9, 19))

# --- ç‰¹å¾´é‡åæ—¥æœ¬èªåŒ–ãƒ˜ãƒ«ãƒ‘ãƒ¼ ---
def jp_feat_name(col_name: str) -> str:
    mapping = {
        'CVRR_SCORE_NEW': 'é›†ä¸­ã‚¹ã‚³ã‚¢', 'SkinTemp': 'çš®è†šæ¸©åº¦', 'LP_SCORE_NEW': 'ãƒªãƒ©ãƒƒã‚¯ã‚¹ã‚¹ã‚³ã‚¢',
        'LFHF_SCORE_NEW': 'LF/HF(è‡ªå¾‹ç¥çµŒãƒãƒ©ãƒ³ã‚¹)', 'TP': 'TP(è‡ªå¾‹ç¥çµŒãƒˆãƒ¼ã‚¿ãƒ«ãƒ‘ãƒ¯ãƒ¼)', 'NEMUKE_SCORE_NEW': 'ä½è¦šé†’ã‚¹ã‚³ã‚¢',
        'PR_SCORE_NEW': 'è„ˆæ‹', 'RMSSD_SCORE_NEW': 'ç–²åŠ´ãƒ»å›å¾©ã‚¹ã‚³ã‚¢', '1åˆ†é–“æ­©æ•°': 'æ­©æ•°', 'accDeviation': 'æ´»å‹•é‡(åŠ é€Ÿåº¦)',
        'has_schedule': 'äºˆå®šã®æœ‰ç„¡', 'is_meeting': 'ä¼šè­°ä¸­ã‹ã©ã†ã‹', 'schedule_density_2h': 'æœ€è¿‘ã®äºˆå®šã®è©°ã¾ã‚Šå…·åˆ',
        'time_to_next_event_min': 'æ¬¡ã®äºˆå®šã¾ã§ã®æ™‚é–“', 'time_since_prev_event_min': 'å‰ã®äºˆå®šã‹ã‚‰ã®çµŒéæ™‚é–“',
        'daily_schedule_hours': '1æ—¥ã®ç·äºˆå®šæ™‚é–“', 'consecutive_schedules': 'é€£ç¶šäºˆå®šãƒ–ãƒ­ãƒƒã‚¯æ•°',
        'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°', 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“_åˆ†': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“(åˆ†)',
        'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“(åˆ†)', 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“(åˆ†)',
        'é›†ä¸­åˆ¤å®š': 'é›†ä¸­åˆ¤å®š', 'ç–²åŠ´åˆ¤å®š': 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š': 'å¼·ã„ç–²åŠ´åˆ¤å®š',
        'çœ æ°—åˆ¤å®š': 'çœ æ°—åˆ¤å®š', 'å¼·ã„çœ æ°—åˆ¤å®š': 'å¼·ã„çœ æ°—åˆ¤å®š',
        'é›†ä¸­çŠ¶æ…‹': 'é›†ä¸­çŠ¶æ…‹', 'çœ æ°—çŠ¶æ…‹': 'çœ æ°—çŠ¶æ…‹', 'ç–²åŠ´çŠ¶æ…‹': 'ç–²åŠ´çŠ¶æ…‹',
        'ä¼‘æ†©åˆ¤å®š': 'ä¼‘æ†©åˆ¤å®š', 'çŸ­æ™‚é–“æ­©è¡Œ': 'çŸ­æ™‚é–“æ­©è¡Œ',
        'é›†ä¸­ç¶™ç¶šæ™‚é–“': 'é›†ä¸­ç¶™ç¶šæ™‚é–“', 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“': 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“',
        'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'ç–²åŠ´ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“': 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“'
    }
    
    base_jp = col_name
    remainder = ""
    for k, v in mapping.items():
        if col_name.startswith(k):
            base_jp = v
            remainder = col_name[len(k):]
            break
            
    if remainder == "": return base_jp
    elif remainder == "_roll_mean": return f"æœ€è¿‘ã®ã€Œ{base_jp}ã€ã®å¹³å‡çš„ãªé«˜ã•"
    elif remainder == "_roll_slope": return f"æœ€è¿‘ã®ã€Œ{base_jp}ã€ã®æ€¥ãªå¤‰åŒ–(ãƒˆãƒ¬ãƒ³ãƒ‰)"
    elif remainder == "_diff1": return f"å‰å›ã‹ã‚‰ã®ã€Œ{base_jp}ã€ã®å¤‰å‹•å¹…"
    elif remainder.startswith("_lag"): return f"å°‘ã—å‰ã®ã€Œ{base_jp}ã€ã®çŠ¶æ…‹"
    elif remainder == "_is_missing": return f"ã€Œ{base_jp}ã€ãŒæœªè¨ˆæ¸¬ã§ã‚ã‚‹ã“ã¨"
    else: return f"{base_jp}{remainder}"

def get_base_feature_name(feat: str) -> str:
    mapping = {
        'CVRR_SCORE_NEW': 'é›†ä¸­ã‚¹ã‚³ã‚¢', 'SkinTemp': 'çš®è†šæ¸©åº¦', 'LP_SCORE_NEW': 'ãƒªãƒ©ãƒƒã‚¯ã‚¹ã‚¹ã‚³ã‚¢',
        'LFHF_SCORE_NEW': 'LF/HF(è‡ªå¾‹ç¥çµŒãƒãƒ©ãƒ³ã‚¹)', 'TP': 'TP(è‡ªå¾‹ç¥çµŒãƒˆãƒ¼ã‚¿ãƒ«ãƒ‘ãƒ¯ãƒ¼)', 'NEMUKE_SCORE_NEW': 'ä½è¦šé†’ã‚¹ã‚³ã‚¢',
        'PR_SCORE_NEW': 'è„ˆæ‹', 'RMSSD_SCORE_NEW': 'ç–²åŠ´ãƒ»å›å¾©ã‚¹ã‚³ã‚¢', '1åˆ†é–“æ­©æ•°': 'æ­©æ•°', 'accDeviation': 'æ´»å‹•é‡(åŠ é€Ÿåº¦)',
        'has_schedule': 'äºˆå®š', 'is_meeting': 'ä¼šè­°', 'schedule_density_2h': 'äºˆå®šã®å¯†åº¦',
        'time_to_next_event_min': 'æ¬¡ã®äºˆå®šã¾ã§ã®æ™‚é–“', 'time_since_prev_event_min': 'å‰ã®äºˆå®šã‹ã‚‰ã®çµŒéæ™‚é–“',
        'daily_schedule_hours': '1æ—¥ã®ç·äºˆå®šæ™‚é–“', 'consecutive_schedules': 'é€£ç¶šäºˆå®šãƒ–ãƒ­ãƒƒã‚¯æ•°',
        'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°', 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“_åˆ†': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“',
        'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“', 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“',
        'é›†ä¸­åˆ¤å®š': 'é›†ä¸­åˆ¤å®š', 'ç–²åŠ´åˆ¤å®š': 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š': 'å¼·ã„ç–²åŠ´åˆ¤å®š',
        'çœ æ°—åˆ¤å®š': 'çœ æ°—åˆ¤å®š', 'å¼·ã„çœ æ°—åˆ¤å®š': 'å¼·ã„çœ æ°—åˆ¤å®š',
        'é›†ä¸­çŠ¶æ…‹': 'é›†ä¸­çŠ¶æ…‹', 'çœ æ°—çŠ¶æ…‹': 'çœ æ°—çŠ¶æ…‹', 'ç–²åŠ´çŠ¶æ…‹': 'ç–²åŠ´çŠ¶æ…‹',
        'ä¼‘æ†©åˆ¤å®š': 'ä¼‘æ†©åˆ¤å®š', 'çŸ­æ™‚é–“æ­©è¡Œ': 'çŸ­æ™‚é–“æ­©è¡Œ',
        'é›†ä¸­ç¶™ç¶šæ™‚é–“': 'é›†ä¸­ç¶™ç¶šæ™‚é–“', 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“': 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“',
        'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'ç–²åŠ´ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“': 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“'
    }
    for k, v in mapping.items():
        if feat.startswith(k): return v
    return feat

def get_factor_direction_text(feat: str, val: float, df_all: pd.DataFrame) -> str:
    mapping = {
        'CVRR_SCORE_NEW': 'é›†ä¸­ã‚¹ã‚³ã‚¢', 'SkinTemp': 'çš®è†šæ¸©åº¦', 'LP_SCORE_NEW': 'ãƒªãƒ©ãƒƒã‚¯ã‚¹ã‚¹ã‚³ã‚¢',
        'LFHF_SCORE_NEW': 'LF/HF(è‡ªå¾‹ç¥çµŒãƒãƒ©ãƒ³ã‚¹)', 'TP': 'TP(è‡ªå¾‹ç¥çµŒãƒˆãƒ¼ã‚¿ãƒ«ãƒ‘ãƒ¯ãƒ¼)', 'NEMUKE_SCORE_NEW': 'ä½è¦šé†’ã‚¹ã‚³ã‚¢',
        'PR_SCORE_NEW': 'è„ˆæ‹', 'RMSSD_SCORE_NEW': 'ç–²åŠ´ãƒ»å›å¾©ã‚¹ã‚³ã‚¢', '1åˆ†é–“æ­©æ•°': 'æ­©æ•°', 'accDeviation': 'æ´»å‹•é‡(åŠ é€Ÿåº¦)',
        'has_schedule': 'äºˆå®š', 'is_meeting': 'ä¼šè­°', 'schedule_density_2h': 'äºˆå®šã®å¯†åº¦',
        'time_to_next_event_min': 'æ¬¡ã®äºˆå®šã¾ã§ã®æ™‚é–“', 'time_since_prev_event_min': 'å‰ã®äºˆå®šã‹ã‚‰ã®çµŒéæ™‚é–“',
        'daily_schedule_hours': '1æ—¥ã®ç·äºˆå®šæ™‚é–“', 'consecutive_schedules': 'é€£ç¶šäºˆå®šãƒ–ãƒ­ãƒƒã‚¯æ•°',
        'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°', 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“_åˆ†': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“',
        'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“', 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“',
        'é›†ä¸­åˆ¤å®š': 'é›†ä¸­åˆ¤å®š', 'ç–²åŠ´åˆ¤å®š': 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š': 'å¼·ã„ç–²åŠ´åˆ¤å®š',
        'çœ æ°—åˆ¤å®š': 'çœ æ°—åˆ¤å®š', 'å¼·ã„çœ æ°—åˆ¤å®š': 'å¼·ã„çœ æ°—åˆ¤å®š',
        'é›†ä¸­çŠ¶æ…‹': 'é›†ä¸­çŠ¶æ…‹', 'çœ æ°—çŠ¶æ…‹': 'çœ æ°—çŠ¶æ…‹', 'ç–²åŠ´çŠ¶æ…‹': 'ç–²åŠ´çŠ¶æ…‹',
        'ä¼‘æ†©åˆ¤å®š': 'ä¼‘æ†©åˆ¤å®š', 'çŸ­æ™‚é–“æ­©è¡Œ': 'çŸ­æ™‚é–“æ­©è¡Œ',
        'é›†ä¸­ç¶™ç¶šæ™‚é–“': 'é›†ä¸­ç¶™ç¶šæ™‚é–“', 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“': 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“',
        'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'ç–²åŠ´ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“': 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“'
    }
    
    base_jp = feat
    remainder = ""
    for k, v in mapping.items():
        if feat.startswith(k):
            base_jp = v
            remainder = feat[len(k):]
            break
            
    if "_is_missing" in feat: return f"ã€Œ{base_jp}ã€ãŒæœªè¨ˆæ¸¬ã§ã‚ã‚‹ã“ã¨"
    elif feat in ["has_schedule", "is_meeting"]: return f"ã€Œ{base_jp}ã€ãŒå…¥ã£ã¦ã„ã‚‹ã“ã¨" if val > 0 else f"ã€Œ{base_jp}ã€ãŒå…¥ã£ã¦ã„ãªã„ã“ã¨"
    elif feat in ["ä¼‘æ†©åˆ¤å®š", "çŸ­æ™‚é–“æ­©è¡Œ"]: return f"ã€Œ{base_jp}ã€ã‚’ã—ã¦ã„ã‚‹ã“ã¨" if val > 0 else f"ã€Œ{base_jp}ã€ã‚’ã—ã¦ã„ãªã„ã“ã¨"
    elif feat in ["é›†ä¸­çŠ¶æ…‹", "çœ æ°—çŠ¶æ…‹", "ç–²åŠ´çŠ¶æ…‹"]: return f"ã€Œ{base_jp}ã€ãŒã€Œ{val}ã€ã§ã‚ã‚‹ã“ã¨"
            
    if "_roll_slope" in feat or "_diff1" in feat:
        direction = "ã®å¢—åŠ " if val > 0 else "ã®ä½ä¸‹" if val < 0 else "ã®å¤‰åŒ–ãªã—"
        return f"æœ€è¿‘ã®ã€Œ{base_jp}ã€ã®ãƒˆãƒ¬ãƒ³ãƒ‰{direction}" if "_roll_slope" in feat else f"å‰å›ã‹ã‚‰ã®ã€Œ{base_jp}ã€{direction}"
    else:
        if "_roll_mean" in feat: return f"æœ€è¿‘ã®ã€Œ{base_jp}ã€"
        elif "_lag" in feat: return f"å°‘ã—å‰ã®ã€Œ{base_jp}ã€"
        else: return f"ã€Œ{base_jp}ã€"

# --- åˆ†æãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def run_analysis(df_ts, df_sched, use_gemini=False):
    with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ãƒ»å‰å‡¦ç†ã—ã¦ã„ã¾ã™..."):
        if 'timestamp' in df_ts.columns:
            df_ts['timestamp_clean'] = df_ts['timestamp'].astype(str).str.split(' GMT').str[0]
            df_ts['datetime'] = pd.to_datetime(df_ts['timestamp_clean'], errors='coerce')
            df_ts = df_ts.dropna(subset=['datetime'])
            df_ts.set_index('datetime', inplace=True)
            df_ts.drop(columns=['timestamp', 'timestamp_clean'], inplace=True, errors='ignore')
            df_ts = df_ts.sort_index()

        base_agg_dict = {
            'SkinTemp': 'mean', 'CVRR_SCORE_NEW': 'mean', 'LP_SCORE_NEW': 'mean',
            'LFHF_SCORE_NEW': 'mean', 'TP': 'mean', 'NEMUKE_SCORE_NEW': 'mean',
            'PR_SCORE_NEW': 'mean', 'RMSSD_SCORE_NEW': 'mean', '1åˆ†é–“æ­©æ•°': 'sum', 'accDeviation': 'mean',
            'é›†ä¸­åˆ¤å®š': 'mean', 'ç–²åŠ´åˆ¤å®š': 'mean', 'å¼·ã„ç–²åŠ´åˆ¤å®š': 'mean', 'çœ æ°—åˆ¤å®š': 'mean', 'å¼·ã„çœ æ°—åˆ¤å®š': 'mean',
            'ä¼‘æ†©åˆ¤å®š': 'mean', 'çŸ­æ™‚é–“æ­©è¡Œ': 'mean',
            'é›†ä¸­ç¶™ç¶šæ™‚é–“': 'mean', 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“': 'mean', 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“': 'mean', 'ç–²åŠ´ç¶™ç¶šæ™‚é–“': 'mean', 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“': 'mean'
        }
        
        cat_agg_dict = {'é›†ä¸­çŠ¶æ…‹': 'last', 'çœ æ°—çŠ¶æ…‹': 'last', 'ç–²åŠ´çŠ¶æ…‹': 'last'}
        
        agg_dict = {col: func for col, func in base_agg_dict.items() if col in df_ts.columns}
        for col, func in cat_agg_dict.items():
            if col in df_ts.columns: agg_dict[col] = func
        
        if target_col not in agg_dict:
            st.error(f"ã‚¨ãƒ©ãƒ¼: äºˆæ¸¬ã«å¿…è¦ãªç›®çš„å¤‰æ•°ã€Œ{selected_target_name}ã€ãŒãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return

        for col in agg_dict.keys():
            if col in base_agg_dict.keys():
                df_ts[col] = pd.to_numeric(df_ts[col], errors='coerce')

        df_resampled = df_ts.resample(RESAMPLE_FREQ).agg(agg_dict)

        if df_sched is not None:
            df_sched = df_sched[df_sched['çµ‚æ—¥ã‚¤ãƒ™ãƒ³ãƒˆ'].astype(str).str.upper() != 'TRUE']
            df_sched['start_dt'] = pd.to_datetime(df_sched['é–‹å§‹æ—¥'].astype(str) + ' ' + df_sched['é–‹å§‹æ™‚åˆ»'].astype(str), errors='coerce')
            df_sched['end_dt']   = pd.to_datetime(df_sched['çµ‚äº†æ—¥'].astype(str) + ' ' + df_sched['çµ‚äº†æ™‚åˆ»'].astype(str), errors='coerce')
            df_sched = df_sched.dropna(subset=['start_dt', 'end_dt']).sort_values('start_dt')

            df_resampled['has_schedule'] = 0
            df_resampled['is_meeting'] = 0
            meeting_keywords = ['ä¼šè­°', 'æ‰“åˆã›', 'æ‰“ã¡åˆã‚ã›', 'MTG', 'é¢è«‡', 'å•†è«‡', 'æ¥å®¢', 'è¨ªå•']

            for _, row in df_sched.iterrows():
                mask = (df_resampled.index < row['end_dt']) & ((df_resampled.index + freq_td) > row['start_dt'])
                df_resampled.loc[mask, 'has_schedule'] = 1
                subject = str(row.get('ä»¶å', ''))
                if any(kw in subject for kw in meeting_keywords):
                    df_resampled.loc[mask, 'is_meeting'] = 1

            s = df_resampled['has_schedule']
            df_resampled['consecutive_schedules'] = s.groupby((s != s.shift()).cumsum()).cumsum()

            df_resampled['date'] = df_resampled.index.date
            df_resampled = df_resampled.join(df_resampled.groupby('date')['has_schedule'].sum().rename('daily_schedule_hours'), on='date').fillna({'daily_schedule_hours': 0})
            df_resampled.drop(columns=['date'], inplace=True)

            event_starts = df_sched['start_dt'].to_numpy(dtype='datetime64[ns]')
            event_ends   = df_sched['end_dt'].to_numpy(dtype='datetime64[ns]')
            t = df_resampled.index.to_numpy(dtype='datetime64[ns]')

            next_start_idx = np.searchsorted(event_starts, t, side='left')
            has_next = next_start_idx < len(event_starts)
            next_idx_safe = np.clip(next_start_idx, 0, max(len(event_starts) - 1, 0))
            next_start = np.full(t.shape, np.datetime64('NaT'), dtype='datetime64[ns]')
            if len(event_starts) > 0: next_start[has_next] = event_starts[next_idx_safe[has_next]]

            prev_end_idx = np.searchsorted(event_ends, t, side='right') - 1
            has_prev = prev_end_idx >= 0
            prev_idx_safe = np.clip(prev_end_idx, 0, max(len(event_ends) - 1, 0))
            prev_end = np.full(t.shape, np.datetime64('NaT'), dtype='datetime64[ns]')
            if len(event_ends) > 0: prev_end[has_prev] = event_ends[prev_idx_safe[has_prev]]

            df_resampled['time_to_next_event_min'] = (next_start - t) / np.timedelta64(1, 'm')
            df_resampled['time_since_prev_event_min'] = (t - prev_end) / np.timedelta64(1, 'm')

            win_steps = max(1, int(pd.Timedelta('2H') / freq_td))
            df_resampled['schedule_density_2h'] = df_resampled['has_schedule'].rolling(win_steps, min_periods=1).mean()

    with st.spinner("ç‰¹å¾´é‡ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™..."):
        df_features = df_resampled.copy()
        df_features['hour'] = df_features.index.hour.astype('category')
        df_features['dayofweek'] = df_features.index.dayofweek.astype('category')
        
        for c in ['é›†ä¸­çŠ¶æ…‹', 'çœ æ°—çŠ¶æ…‹', 'ç–²åŠ´çŠ¶æ…‹']:
            if c in df_features.columns: df_features[c] = df_features[c].astype('category')
        
        numeric_cols = df_resampled.select_dtypes(include=[np.number]).columns.tolist()
        win = lookback_steps
        x = np.arange(win, dtype=float)
        x_mean = x.mean()
        x_var = ((x - x_mean) ** 2).sum()
        def rolling_slope(arr):
            y = arr.astype(float)
            if x_var == 0: return 0.0
            return ((x - x_mean) * (y - y.mean())).sum() / x_var

        for col in numeric_cols:
            df_features[f'{col}_is_missing'] = df_resampled[col].isna().astype(int)
            r = df_features[col].rolling(win, min_periods=win)
            df_features[f'{col}_roll_mean'] = r.mean()
            df_features[f'{col}_roll_slope'] = r.apply(rolling_slope, raw=True)
            df_features[f'{col}_diff1'] = df_features[col] - df_features[col].shift(1)

        df_features['date'] = df_features.index.date
        if '1åˆ†é–“æ­©æ•°' in df_features.columns:
            df_features['ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°'] = df_features.groupby('date')['1åˆ†é–“æ­©æ•°'].cumsum()
        if 'is_meeting' in df_features.columns:
            df_features['ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“_åˆ†'] = df_features.groupby('date')['is_meeting'].cumsum() * (freq_td.total_seconds() / 60)
        
        def calc_duration_mins(series):
            group_id = (series != series.shift()).cumsum()
            return (series.groupby(group_id).cumcount() + 1) * (freq_td.total_seconds() / 60)
            
        if 'é›†ä¸­åˆ¤å®š' in df_features.columns:
            focus_mask = (df_features['é›†ä¸­åˆ¤å®š'] >= 0.5).astype(int)
            df_features['ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“_åˆ†'] = calc_duration_mins(focus_mask) * focus_mask
        if 'ç–²åŠ´åˆ¤å®š' in df_features.columns:
            fatigue_mask = (df_features['ç–²åŠ´åˆ¤å®š'] >= 0.5).astype(int)
            df_features['ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“_åˆ†'] = calc_duration_mins(fatigue_mask) * fatigue_mask
            
        df_features.drop(columns=['date'], inplace=True)

        target_threshold = 0.5 if target_col in ['é›†ä¸­åˆ¤å®š', 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š', 'çœ æ°—åˆ¤å®š', 'å¼·ã„çœ æ°—åˆ¤å®š'] else df_features[target_col].median()
        df_features['target_ahead_class'] = (df_features[target_col].shift(-ahead_steps) >= target_threshold).astype(int)

    with st.spinner("LightGBMåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¦ã„ã¾ã™..."):
        drop_cols = ['target_ahead_class']
        df_all = df_features.copy()
        split_idx = int(len(df_all) * 0.8)
        
        df_imp = df_all.copy()
        for col in df_imp.columns:
            if col not in drop_cols:
                df_imp[col] = df_imp[col].ffill(limit=interp_steps).bfill(limit=interp_steps)
        
        train_df = df_imp.iloc[:split_idx].dropna(subset=drop_cols + [target_col])
        test_df  = df_imp.iloc[split_idx:].dropna(subset=drop_cols + [target_col])

        X_train = train_df.drop(columns=drop_cols)
        y_train_class = train_df['target_ahead_class']
        X_test  = test_df.drop(columns=drop_cols)
        y_test_class = test_df['target_ahead_class']
        
        cat_cols = [c for c in X_train.columns if str(X_train[c].dtype) == 'category']
        model = lgb.LGBMClassifier(objective='binary', n_estimators=500, learning_rate=0.03, random_state=42)
        model.fit(X_train, y_train_class, categorical_feature=cat_cols if cat_cols else 'auto')
        preds_proba = model.predict_proba(X_test)[:, 1]
        try:
            auc_test = roc_auc_score(y_test_class, preds_proba)
            logloss_test = log_loss(y_test_class, preds_proba)
        except ValueError:
            auc_test = np.nan
            logloss_test = np.nan


    # =========================================================================
    # ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ç‰¹æ€§ã‚¤ãƒ³ã‚µã‚¤ãƒˆï¼ˆé•·æœŸãƒ»æœˆæ¬¡ãƒ»æ—¥æ¬¡ï¼‰
    # =========================================================================
    st.header("ğŸ‘¤ ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ç‰¹æ€§ã‚¤ãƒ³ã‚µã‚¤ãƒˆ")
    
    # å…±é€šãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
    df_insight = df_imp.copy()
    selected_dow_indices = [dow_options.index(d) for d in selected_dows]
    df_insight = df_insight[df_insight.index.dayofweek.isin(selected_dow_indices)]
    df_insight = df_insight[(df_insight.index.hour >= time_range[0]) & (df_insight.index.hour <= time_range[1])]

    if 'é›†ä¸­åˆ¤å®š' in df_insight.columns:
        df_insight['focus_start'] = (df_insight['é›†ä¸­åˆ¤å®š'] >= 0.5) & (df_insight['é›†ä¸­åˆ¤å®š'].shift(1) < 0.5)
    if 'ç–²åŠ´åˆ¤å®š' in df_insight.columns:
        df_insight['fatigue_start'] = (df_insight['ç–²åŠ´åˆ¤å®š'] >= 0.5) & (df_insight['ç–²åŠ´åˆ¤å®š'].shift(1) < 0.5)

    def get_peak_time(metric_col):
        if metric_col not in df_insight.columns: return "ä¸æ˜", "ä¸æ˜"
        pivot_df = df_insight.pivot_table(values=metric_col, index=df_insight.index.hour, columns=df_insight.index.dayofweek, aggfunc='mean')
        daytime_pivot = pivot_df.loc[time_range[0]:time_range[1], selected_dow_indices]
        if not daytime_pivot.isna().all().all():
            best_hour, best_dow = daytime_pivot.stack().idxmax()
            return dow_options[int(best_dow)], str(int(best_hour))
        return "ä¸æ˜", "ä¸æ˜"

    f_dow, f_hour = get_peak_time('é›†ä¸­åˆ¤å®š')
    fat_dow, fat_hour = get_peak_time('ç–²åŠ´åˆ¤å®š')

    avg_focus_duration_str = "ä¸æ˜"
    daily_focus_count_str = "ä¸æ˜"
    daily_total_focus_time_str = "ä¸æ˜"
    focus_durations = pd.Series(dtype=float)
    
    if 'é›†ä¸­åˆ¤å®š' in df_ts.columns:
        df_1min = df_ts[['é›†ä¸­åˆ¤å®š']].resample('1T').mean().ffill(limit=5)
        df_1min = df_1min[df_1min.index.dayofweek.isin(selected_dow_indices)]
        df_1min = df_1min[(df_1min.index.hour >= time_range[0]) & (df_1min.index.hour <= time_range[1])]
        
        focus_mask = df_1min['é›†ä¸­åˆ¤å®š'] >= 0.5
        focus_blocks = focus_mask.groupby((focus_mask != focus_mask.shift()).cumsum())
        focus_durations = focus_blocks.sum() 
        focus_durations = focus_durations[focus_durations > 0]
        
        if not focus_durations.empty:
            avg_focus_duration_str = f"{focus_durations.mean():.0f}"
            total_focus_count = len(focus_durations)
            num_days = df_1min.index.normalize().nunique()
            daily_focus_count_str = f"{(total_focus_count / num_days if num_days > 0 else 0):.1f}"
            total_focus_minutes = focus_mask.sum()
            daily_total_focus_time_str = f"{(total_focus_minutes / num_days if num_days > 0 else 0):.0f}"

    focus_actions = []
    if '1åˆ†é–“æ­©æ•°' in df_insight.columns and 'focus_start' in df_insight.columns:
        walk_before_focus = df_insight['1åˆ†é–“æ­©æ•°'].shift(1)[df_insight['focus_start']].dropna()
        avg_walk_overall = df_insight['1åˆ†é–“æ­©æ•°'].mean()
        if not walk_before_focus.empty and avg_walk_overall > 0:
            avg_walk_before = walk_before_focus.mean()
            if avg_walk_before > avg_walk_overall * 1.2: focus_actions.append("äº‹å‰ã«ä½“ã‚’å‹•ã‹ã™ã“ã¨ï¼ˆå°‘ã—æ­©ããªã©ï¼‰")
            elif avg_walk_before < avg_walk_overall * 0.8: focus_actions.append("äº‹å‰ã«é™ã‹ãªç’°å¢ƒã§è½ã¡ç€ã„ã¦éã”ã™ã“ã¨")

    if 'çŸ­æ™‚é–“æ­©è¡Œ' in df_insight.columns and 'focus_start' in df_insight.columns:
        walk_before = df_insight['çŸ­æ™‚é–“æ­©è¡Œ'].shift(1)[df_insight['focus_start']].dropna()
        avg_overall = df_insight['çŸ­æ™‚é–“æ­©è¡Œ'].mean()
        if not walk_before.empty and avg_overall > 0:
            if walk_before.mean() > avg_overall * 1.2: focus_actions.append("äº‹å‰ã«çŸ­æ™‚é–“æ­©è¡Œï¼ˆãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ï¼‰ã‚’å–ã‚Šå…¥ã‚Œã‚‹ã“ã¨")

    if 'ä¼‘æ†©åˆ¤å®š' in df_insight.columns and 'focus_start' in df_insight.columns:
        rest_before = df_insight['ä¼‘æ†©åˆ¤å®š'].shift(1)[df_insight['focus_start']].dropna()
        avg_overall = df_insight['ä¼‘æ†©åˆ¤å®š'].mean()
        if not rest_before.empty and avg_overall > 0:
            if rest_before.mean() > avg_overall * 1.2: focus_actions.append("äº‹å‰ã«ã—ã£ã‹ã‚Šä¼‘æ†©ã‚’ã¨ã‚‹ã“ã¨")

    if 'has_schedule' in df_insight.columns and 'é›†ä¸­åˆ¤å®š' in df_insight.columns:
        sched_mask = df_insight['has_schedule'] >= 0.5
        sched_blocks = (sched_mask != sched_mask.shift()).cumsum()
        sched_df = df_insight[sched_mask]
        focus_scores_rested, focus_scores_rushed = [], []
        for _, group in sched_df.groupby(sched_blocks):
            if len(group) > 1 and 'time_since_prev_event_min' in group.columns:
                rest_before = group['time_since_prev_event_min'].iloc[0]
                if not np.isnan(rest_before):
                    if rest_before >= 30: focus_scores_rested.append(group['é›†ä¸­åˆ¤å®š'].mean())
                    else: focus_scores_rushed.append(group['é›†ä¸­åˆ¤å®š'].mean())
        if focus_scores_rested and focus_scores_rushed:
            diff_focus = (np.mean(focus_scores_rested) - np.mean(focus_scores_rushed)) * 100
            if diff_focus > 0: focus_actions.append("äºˆå®šã®å‰ã«30åˆ†ä»¥ä¸Šã®ç©ºãæ™‚é–“ï¼ˆä¼‘æ†©ï¼‰ã‚’ã¨ã‚‹ã“ã¨")
            elif diff_focus < 0: focus_actions.append("äºˆå®šã¨äºˆå®šã®é–“ã‚’ç©ºã‘ãšã«é€£ç¶šã—ã¦æ´»å‹•ã™ã‚‹ã“ã¨")

    focus_actions_str = "ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ç‰¹å®šã§ãã¾ã›ã‚“" if not focus_actions else "ã€".join(focus_actions)

    fatigue_actions = []
    if 'ç–²åŠ´åˆ¤å®š' in df_insight.columns and 'has_schedule' in df_insight.columns:
        sched_mask = df_insight['has_schedule'] >= 0.5
        sched_blocks = (sched_mask != sched_mask.shift()).cumsum()
        sched_df = df_insight[sched_mask]
        fatigue_diffs = []
        for _, group in sched_df.groupby(sched_blocks):
            if len(group) > 1:
                duration_hours = len(group) * (freq_td.total_seconds() / 3600)
                if duration_hours > 0:
                    fatigue_diffs.append((group['ç–²åŠ´åˆ¤å®š'].iloc[-1] - group['ç–²åŠ´åˆ¤å®š'].iloc[0]) / duration_hours)
        if fatigue_diffs and np.mean(fatigue_diffs) > 0:
            fatigue_actions.append("1æ™‚é–“ä»¥ä¸Šã®äºˆå®šã‚’ã“ãªã™ã“ã¨")

    if 'fatigue_start' in df_insight.columns and 'focus_start' in df_insight.columns:
        recovery_consecutive, recovery_single = [], []
        fatigue_times, focus_times = df_insight[df_insight['fatigue_start']].index, df_insight[df_insight['focus_start']].index
        for fat_time in fatigue_times:
            future_focus = focus_times[focus_times > fat_time]
            if len(future_focus) > 0 and future_focus[0].date() == fat_time.date():
                if 'consecutive_schedules' in df_insight.columns:
                    if df_insight.loc[fat_time, 'consecutive_schedules'] >= 2: recovery_consecutive.append(1)
                    else: recovery_single.append(1)
        if recovery_consecutive and recovery_single and (np.mean(recovery_consecutive) - np.mean(recovery_single)) > 0:
            fatigue_actions.append("äºˆå®šã‚’é€£ç¶šã—ã¦å…¥ã‚Œã‚‹ã“ã¨")

    fatigue_actions_str = "ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ç‰¹å®šã§ãã¾ã›ã‚“" if not fatigue_actions else "ã€".join(fatigue_actions)

    recovery_actions = []
    if 'fatigue_start' in df_insight.columns and 'focus_start' in df_insight.columns:
        fatigue_times, focus_times = df_insight[df_insight['fatigue_start']].index, df_insight[df_insight['focus_start']].index
        
        if 'çŸ­æ™‚é–“æ­©è¡Œ' in df_insight.columns:
            rec_with_walk, rec_no_walk = [], []
            for fat_time in fatigue_times:
                future_focus = focus_times[focus_times > fat_time]
                if len(future_focus) > 0 and future_focus[0].date() == fat_time.date():
                    first_focus = future_focus[0]
                    rec_time = (first_focus - fat_time).total_seconds() / 60
                    period_val = df_insight.loc[fat_time:first_focus, 'çŸ­æ™‚é–“æ­©è¡Œ'].mean()
                    if pd.notna(period_val):
                        if period_val > df_insight['çŸ­æ™‚é–“æ­©è¡Œ'].mean(): rec_with_walk.append(rec_time)
                        else: rec_no_walk.append(rec_time)
            if rec_with_walk and rec_no_walk:
                diff = np.mean(rec_no_walk) - np.mean(rec_with_walk)
                if diff > 10: recovery_actions.append(f"çŸ­æ™‚é–“æ­©è¡Œï¼ˆå‹•çš„ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ï¼‰ã‚’è¡Œã†ã“ã¨ï¼ˆå¹³å‡{abs(diff):.0f}åˆ†æ—©ãå›å¾©ï¼‰")
                elif diff < -10: recovery_actions.append(f"æ­©ãå›ã‚‰ãšé™ã‹ã«ä¼‘ã‚€ã“ã¨ï¼ˆå¹³å‡{abs(diff):.0f}åˆ†æ—©ãå›å¾©ï¼‰")
                
        if 'ä¼‘æ†©åˆ¤å®š' in df_insight.columns:
            rec_with_rest, rec_no_rest = [], []
            for fat_time in fatigue_times:
                future_focus = focus_times[focus_times > fat_time]
                if len(future_focus) > 0 and future_focus[0].date() == fat_time.date():
                    first_focus = future_focus[0]
                    rec_time = (first_focus - fat_time).total_seconds() / 60
                    period_val = df_insight.loc[fat_time:first_focus, 'ä¼‘æ†©åˆ¤å®š'].mean()
                    if pd.notna(period_val):
                        if period_val > df_insight['ä¼‘æ†©åˆ¤å®š'].mean(): rec_with_rest.append(rec_time)
                        else: rec_no_rest.append(rec_time)
            if rec_with_rest and rec_no_rest:
                diff = np.mean(rec_no_rest) - np.mean(rec_with_rest)
                if diff > 10: recovery_actions.append(f"æ„è­˜çš„ã«ä¼‘æ†©æ™‚é–“ã‚’ã¨ã‚‹ã“ã¨ï¼ˆå¹³å‡{abs(diff):.0f}åˆ†æ—©ãå›å¾©ï¼‰")

    recovery_actions_str = "ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ç‰¹å®šã§ãã¾ã›ã‚“" if not recovery_actions else "ã€".join(recovery_actions)

    # --- 4ã¤ã®ã‚¿ãƒ–ã‚’ä½œæˆ ---
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ ãƒã‚¤ãƒ»ã‚¹ãƒšãƒƒã‚¯", "ğŸ“… ãƒãƒ³ã‚¹ãƒªãƒ¼ã‚¤ãƒ³ã‚µã‚¤ãƒˆ", "â˜€ï¸ ãƒ‡ã‚¤ãƒªãƒ¼ã‚¤ãƒ³ã‚µã‚¤ãƒˆ", "ğŸ“Š è¡Œå‹•ãƒªã‚¿ãƒ¼ãƒ³åˆ†æ"])
    
    with tab1:
        st.markdown("#### ã‚ãªãŸã®é›†ä¸­ç‰¹æ€§")
        st.markdown(f"ã€€{f_dow}æ›œæ—¥ã®{f_hour}æ™‚å°ã«æœ€ã‚‚é›†ä¸­ã—ã‚„ã™ã„å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚<br>"
                    f"ã€€å¹³å‡é›†ä¸­æŒç¶šæ™‚é–“ã¯{avg_focus_duration_str}åˆ†ã§ã™ã€‚<br>"
                    f"ã€€1æ—¥ã®å¹³å‡é›†ä¸­æ™‚é–“ã¯{daily_total_focus_time_str}åˆ†ã§ã™ã€‚<br>"
                    f"ã€€1æ—¥ã«{daily_focus_count_str}å›é›†ä¸­ã¨ç·©å’Œã®ãƒªã‚ºãƒ ã‚’ç¹°ã‚Šè¿”ã—ã¦ã„ã¾ã™ã€‚<br>"
                    f"ã€€é›†ä¸­ã«å…¥ã‚Šã‚„ã™ã„è¡Œå‹•ã¯{focus_actions_str}", unsafe_allow_html=True)

        st.markdown("<br>", unsafe_allow_html=True)
        st.markdown("#### ã‚ãªãŸã®ç–²åŠ´ç‰¹æ€§")
        st.markdown(f"ã€€{fat_dow}æ›œæ—¥ã®{fat_hour}æ™‚å°ã«æœ€ã‚‚ç–²åŠ´ã—ã‚„ã™ã„å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚<br>"
                    f"ã€€ç–²åŠ´ã—ã‚„ã™ã„è¡Œå‹•ã¯{fatigue_actions_str}<br>"
                    f"ã€€ç–²åŠ´ã‹ã‚‰æ—©ãå›å¾©ã™ã‚‹è¡Œå‹•ã¯{recovery_actions_str}", unsafe_allow_html=True)

        if not focus_durations.empty:
            st.markdown("<br>##### é›†ä¸­æŒç¶šæ™‚é–“ã®åˆ†å¸ƒ", unsafe_allow_html=True)
            max_duration = int(focus_durations.max())
            max_bin = math.ceil(max_duration / 10) * 10
            bins = np.arange(0, max_bin + 20, 10) 
            counts, edges = np.histogram(focus_durations, bins=bins)
            xtick_labels = [f"{int(edges[i])}-{int(edges[i+1])-1}" for i in range(len(edges)-1)]
            
            fig_dist = go.Figure(data=[go.Bar(
                x=xtick_labels, y=counts, marker_color='#4A90E2', opacity=0.8,
                hovertemplate="é›†ä¸­æ™‚é–“: %{x}åˆ†<br>å›æ•°: %{y}å›<extra></extra>"
            )])
            fig_dist.update_layout(
                xaxis_title="é›†ä¸­æŒç¶šæ™‚é–“ (åˆ†)", yaxis_title="å›æ•°", height=300,
                margin=dict(l=20, r=20, t=20, b=20), plot_bgcolor='rgba(0,0,0,0)', bargap=0.1
            )
            st.plotly_chart(fig_dist, use_container_width=True)

        st.markdown("##### æ™‚é–“å¸¯ãƒ»æ›œæ—¥åˆ¥ã®å‚¾å‘ (ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—)")
        def plot_heatmap_plotly(target_metric, colorscale_name):
            if target_metric not in df_imp.columns: return None
            pivot_df = df_imp.pivot_table(values=target_metric, index=df_imp.index.hour, columns=df_imp.index.dayofweek, aggfunc='mean')
            heatmap_data = np.full((time_range[1] - time_range[0] + 1, 7), np.nan)
            for h in pivot_df.index:
                if time_range[0] <= h <= time_range[1]:
                    for d in pivot_df.columns:
                        if d in selected_dow_indices: heatmap_data[int(h) - time_range[0], int(d)] = pivot_df.loc[h, d]
            
            fig_hm = go.Figure(data=go.Heatmap(
                z=heatmap_data, x=dow_options, y=[f"{h}:00" for h in range(time_range[0], time_range[1] + 1)],
                colorscale=colorscale_name, hoverongaps=False, hovertemplate="æ›œæ—¥: %{x}<br>æ™‚é–“å¸¯: %{y}<br>ç¢ºç‡: %{z:.2f}<extra></extra>"
            ))
            fig_hm.update_layout(yaxis_autorange='reversed', height=350, margin=dict(l=20, r=20, t=20, b=20))
            return fig_hm

        col_h1, col_h2 = st.columns(2)
        with col_h1:
            st.markdown("**ğŸ¯ é›†ä¸­ç¢ºç‡**")
            fig_focus = plot_heatmap_plotly('é›†ä¸­åˆ¤å®š', 'Blues')
            if fig_focus: st.plotly_chart(fig_focus, use_container_width=True)
        with col_h2:
            st.markdown("**ğŸ”‹ ç–²åŠ´ç¢ºç‡**")
            fig_fatigue = plot_heatmap_plotly('ç–²åŠ´åˆ¤å®š', 'Reds')
            if fig_fatigue: st.plotly_chart(fig_fatigue, use_container_width=True)

    with tab2:
        df_ts['year_month'] = df_ts.index.to_period('M').astype(str)
        available_months = sorted(df_ts['year_month'].unique().tolist(), reverse=True)
        
        if not available_months:
            st.write("åˆ†æå¯èƒ½ãªæœˆã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            selected_month = st.selectbox("åˆ†æå¯¾è±¡ã¨ã™ã‚‹å¹´æœˆã‚’é¸æŠã—ã¦ãã ã•ã„", available_months)
            df_month = df_ts[df_ts['year_month'] == selected_month]
            
            if 'é›†ä¸­åˆ¤å®š' in df_month.columns:
                # 1æ™‚é–“å˜ä½ã§ã®é›†ä¸­å›æ•°ï¼ˆåˆ†æ•°ã«ç›¸å½“ï¼‰ã‚’é›†è¨ˆ
                df_m_1t = df_month[['é›†ä¸­åˆ¤å®š']].resample('1T').mean()
                df_m_1t['é›†ä¸­åˆ¤å®š_ãƒ•ãƒ©ã‚°'] = (df_m_1t['é›†ä¸­åˆ¤å®š'] >= 0.5).astype(int)
                
                df_m_hourly = df_m_1t.resample('1H').sum()
                df_m_hourly['day'] = df_m_hourly.index.day
                df_m_hourly['hour'] = df_m_hourly.index.hour
                df_m_hourly['dow'] = df_m_hourly.index.dayofweek
                
                # è¨­å®šã•ã‚ŒãŸæ™‚é–“å¸¯ï¼ˆtime_rangeï¼‰ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
                target_hours = list(range(time_range[0], time_range[1] + 1))
                
                # ã‚°ãƒ©ãƒ•: æ›œæ—¥åˆ¥ãƒ»æ™‚é–“å¸¯åˆ¥
                col_m1, col_m2 = st.columns(2)
                
                with col_m1:
                    dow_sum = df_m_hourly.groupby('dow')['é›†ä¸­åˆ¤å®š_ãƒ•ãƒ©ã‚°'].sum().reindex(range(7), fill_value=0)
                    fig_dow = px.bar(x=dow_options, y=dow_sum.values, labels={'x': 'æ›œæ—¥', 'y': 'é›†ä¸­åˆ¤å®šå›æ•°'}, title="æ›œæ—¥åˆ¥ é›†ä¸­åˆ¤å®šå›æ•°")
                    fig_dow.update_traces(marker_color='#4A90E2')
                    st.plotly_chart(fig_dow, use_container_width=True)
                    
                with col_m2:
                    # æ™‚é–“å¸¯ã‚’å¯¾è±¡æ™‚é–“å¸¯ã®ã¿ã«çµã‚Šè¾¼ã‚€
                    hour_sum = df_m_hourly.groupby('hour')['é›†ä¸­åˆ¤å®š_ãƒ•ãƒ©ã‚°'].sum().reindex(target_hours, fill_value=0)
                    fig_hour = px.bar(x=[f"{h}:00" for h in target_hours], y=hour_sum.values, labels={'x': 'æ™‚é–“å¸¯', 'y': 'é›†ä¸­åˆ¤å®šå›æ•°'}, title="æ™‚é–“å¸¯åˆ¥ é›†ä¸­åˆ¤å®šå›æ•°")
                    fig_hour.update_traces(marker_color='#4A90E2')
                    st.plotly_chart(fig_hour, use_container_width=True)
                
                # ã‚°ãƒ©ãƒ•: æ—¥Ã—æ™‚é–“ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
                st.markdown("#### æ—¥ä»˜Ã—æ™‚é–“å¸¯ã®é›†ä¸­åˆ¤å®šå›æ•° (èµ¤æ ã¯äºˆå®šã‚ã‚Š)")
                hm_pivot = df_m_hourly.pivot_table(index='day', columns='hour', values='é›†ä¸­åˆ¤å®š_ãƒ•ãƒ©ã‚°', aggfunc='sum').fillna(0)
                
                # æ¬ ã‘ã¦ã„ã‚‹æ—¥ãƒ»æ™‚é–“ã‚’è£œå®Œã—ã€å¯¾è±¡æ™‚é–“å¸¯ã®ã¿ã«çµã‚Šè¾¼ã‚€
                if len(df_month.index) > 0:
                    days_in_month = df_month.index[0].days_in_month
                else:
                    days_in_month = 31 # fallback
                all_days = list(range(1, days_in_month + 1))
                hm_pivot = hm_pivot.reindex(index=all_days, columns=target_hours, fill_value=0)
                
                fig_hm_month = go.Figure(data=go.Heatmap(
                    z=hm_pivot.values,
                    x=[f"{h}:00" for h in target_hours],
                    y=[f"{d}æ—¥" for d in all_days],
                    colorscale='Blues',
                    hovertemplate="æ—¥ä»˜: %{y}<br>æ™‚é–“å¸¯: %{x}<br>é›†ä¸­å›æ•°: %{z}<extra></extra>"
                ))
                
                # äºˆå®šãŒã‚ã‚‹æ™‚é–“å¸¯ã«èµ¤æ ï¼ˆShapesï¼‰ã‚’è¿½åŠ 
                shapes = []
                if df_sched is not None and not df_sched.empty:
                    for d in all_days:
                        for h in target_hours:
                            try:
                                dt_start = pd.to_datetime(f"{selected_month}-{d:02d} {h:02d}:00:00")
                                dt_end = dt_start + pd.Timedelta('1H')
                                has_sched = ((df_sched['start_dt'] < dt_end) & (df_sched['end_dt'] > dt_start)).any()
                                if has_sched:
                                    # æ¨ªè»¸ãŒçµã‚Šè¾¼ã¾ã‚ŒãŸãŸã‚ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—ã—ç›´ã™
                                    x_idx = h - time_range[0]
                                    shapes.append(dict(
                                        type="rect",
                                        x0=x_idx - 0.5, x1=x_idx + 0.5,
                                        y0=d - 1 - 0.5, y1=d - 1 + 0.5, # y0,y1 ã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹(0å§‹ã¾ã‚Š)ã§æŒ‡å®š
                                        line=dict(color="red", width=2),
                                        fillcolor="rgba(0,0,0,0)"
                                    ))
                            except ValueError:
                                pass # å­˜åœ¨ã—ãªã„æ—¥ä»˜ï¼ˆã†ã‚‹ã†å¹´ãªã©ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
                
                fig_hm_month.update_layout(
                    shapes=shapes,
                    yaxis_autorange='reversed',
                    height=600,
                    margin=dict(l=20, r=20, t=20, b=20)
                )
                st.plotly_chart(fig_hm_month, use_container_width=True)
                
                # ã‚³ãƒ¡ãƒ³ãƒˆã®ç”Ÿæˆ
                best_dow_m = dow_options[dow_sum.idxmax()] if dow_sum.sum() > 0 else "ä¸æ˜"
                best_hour_m = hour_sum.idxmax() if hour_sum.sum() > 0 else "ä¸æ˜"
                
                st.info(f"**ã€{selected_month} ã®ãƒãƒ³ã‚¹ãƒªãƒ¼ã‚¤ãƒ³ã‚µã‚¤ãƒˆã€‘**\n\n"
                        f"- ã“ã®æœˆã¯ **{best_dow_m}æ›œæ—¥** ã®é›†ä¸­åˆ¤å®šå›æ•°ãŒæœ€ã‚‚å¤šããªã£ã¦ã„ã¾ã™ã€‚\n"
                        f"- æ™‚é–“å¸¯ã§è¦‹ã‚‹ã¨ **{best_hour_m}æ™‚å°** ã«é›†ä¸­ã™ã‚‹å‚¾å‘ãŒå¼·ã‹ã£ãŸã§ã™ã€‚\n"
                        f"- ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä¸Šã®èµ¤æ ã¯ã€Œäºˆå®šï¼ˆä¼šè­°ãªã©ï¼‰ã€ãŒå…¥ã£ã¦ã„ã‚‹æ™‚é–“å¸¯ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚äºˆå®šã¨é›†ä¸­ã®ç›¸é–¢é–¢ä¿‚ã‚’è¦–è¦šçš„ã«ç¢ºèªã§ãã¾ã™ã€‚")
            else:
                st.write("ã€Œé›†ä¸­åˆ¤å®šã€ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")

    with tab3:
        df_ts['date_str'] = df_ts.index.date.astype(str)
        available_days = sorted(df_ts['date_str'].unique().tolist(), reverse=True)
        
        if not available_days:
            st.write("åˆ†æå¯èƒ½ãªæ—¥ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            selected_day = st.selectbox("åˆ†æå¯¾è±¡ã¨ã™ã‚‹å¹´æœˆæ—¥ã‚’é¸æŠã—ã¦ãã ã•ã„", available_days)
            df_day = df_ts[df_ts['date_str'] == selected_day].copy()
            
            # è¨­å®šã•ã‚ŒãŸæ™‚é–“å¸¯ï¼ˆtime_rangeï¼‰ã§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            df_day = df_day[(df_day.index.hour >= time_range[0]) & (df_day.index.hour <= time_range[1])]
            
            score_col = 'CVRR_SCORE_NEW'
            graph_title_base = "é›†ä¸­ã¨ç·©å’Œ"
            score_label = "CVRR SCORE (é›†ä¸­åº¦åˆã„)"
            state_high = "é›†ä¸­"
            state_low = "ç·©å’Œï¼ˆãƒªãƒ©ãƒƒã‚¯ã‚¹ï¼‰"
            
            if target_col in ['ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š']:
                score_col = 'RMSSD_SCORE_NEW'
                graph_title_base = "ç–²åŠ´ã¨å›å¾©"
                score_label = "RMSSD SCORE (ç–²åŠ´ãƒ»å›å¾©åº¦åˆã„)"
                state_high = "å›å¾©ï¼ˆãƒªãƒ©ãƒƒã‚¯ã‚¹ï¼‰"
                state_low = "ç–²åŠ´ï¼ˆã‚¹ãƒˆãƒ¬ã‚¹ï¼‰"
            elif target_col in ['çœ æ°—åˆ¤å®š', 'å¼·ã„çœ æ°—åˆ¤å®š']:
                score_col = 'NEMUKE_SCORE_NEW'
                graph_title_base = "çœ æ°—ã¨è¦šé†’"
                score_label = "NEMUKE SCORE (çœ æ°—åº¦åˆã„)"
                state_high = "ä½è¦šé†’ï¼ˆçœ æ°—ï¼‰"
                state_low = "è¦šé†’"
            
            if score_col in df_day.columns and not df_day.empty:
                st.markdown(f"#### ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ«ã‚°ãƒ©ãƒ• ({graph_title_base}ã®æ³¢)")
                
                base_val = 50.0 # åŸºæº–ã¨ãªã‚‹å¹³å‡å€¤
                
                fig_daily = go.Figure()
                
                # åŸºæº–ç·š(50)ã‚’æç”»ï¼ˆãƒ›ãƒãƒ¼ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                fig_daily.add_trace(go.Scatter(
                    x=df_day.index, y=[base_val]*len(df_day),
                    mode='lines', line=dict(color='gray', width=1, dash='dash'),
                    name='åŸºæº–(50)', hoverinfo='skip'
                ))
                
                # ä¸Šå´ï¼ˆé›†ä¸­ï¼‰ã®é’ã„é¢
                y_upper = np.where(df_day[score_col] >= base_val, df_day[score_col], base_val)
                fig_daily.add_trace(go.Scatter(
                    x=df_day.index, y=y_upper,
                    fill='tonexty', fillcolor='rgba(54, 162, 235, 0.5)', # é’ç³»
                    mode='lines', line=dict(color='rgba(0,0,0,0)', width=0),
                    showlegend=False, hoverinfo='skip'
                ))
                
                # ä¸‹å´ã®é¢ã‚’æããŸã‚ã«ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ã‚‚ã†ä¸€åº¦å¼•ã
                fig_daily.add_trace(go.Scatter(
                    x=df_day.index, y=[base_val]*len(df_day),
                    mode='lines', line=dict(color='rgba(0,0,0,0)', width=0),
                    showlegend=False, hoverinfo='skip'
                ))
                
                # ä¸‹å´ï¼ˆç·©å’Œï¼‰ã®ã‚ªãƒ¬ãƒ³ã‚¸ç³»ã®é¢
                y_lower = np.where(df_day[score_col] <= base_val, df_day[score_col], base_val)
                fig_daily.add_trace(go.Scatter(
                    x=df_day.index, y=y_lower,
                    fill='tonexty', fillcolor='rgba(255, 159, 64, 0.5)', # ã‚ªãƒ¬ãƒ³ã‚¸ç³»
                    mode='lines', line=dict(color='rgba(0,0,0,0)', width=0),
                    showlegend=False, hoverinfo='skip'
                ))
                
                # ãƒ›ãƒãƒ¼ãƒ»è¡¨ç¤ºç”¨ã®å®Ÿéš›ã®æ¨ç§»ç·šï¼ˆé»’è‰²ï¼‰
                fig_daily.add_trace(go.Scatter(
                    x=df_day.index, 
                    y=df_day[score_col],
                    mode='lines',
                    line=dict(color='#333333', width=2),
                    name=score_col,
                    hovertemplate="æ™‚åˆ»: %{x|%H:%M}<br>ã‚¹ã‚³ã‚¢: %{y:.1f}<extra></extra>"
                ))
                
                fig_daily.update_layout(
                    title=f"{selected_day} ã®{graph_title_base}ã®æ¨ç§» ({time_range[0]}æ™‚ã€œ{time_range[1]}æ™‚)",
                    xaxis_title="æ™‚åˆ»",
                    yaxis_title=score_label,
                    height=400,
                    hovermode="x unified",
                    plot_bgcolor='rgba(0,0,0,0)',
                    margin=dict(l=20, r=20, t=40, b=20)
                )
                fig_daily.update_xaxes(showgrid=True, gridcolor='lightgray', showline=True, linewidth=1, linecolor='black')
                fig_daily.update_yaxes(showgrid=True, gridcolor='lightgray', showline=True, linewidth=1, linecolor='black')
                st.plotly_chart(fig_daily, use_container_width=True)
                
                # ã‚³ãƒ¡ãƒ³ãƒˆã®ç”Ÿæˆ
                if not df_day[score_col].isna().all():
                    max_idx = df_day[score_col].idxmax()
                    max_val = df_day[score_col].max()
                    avg_val = df_day[score_col].mean()
                    
                    st.info(f"**ã€{selected_day} ã®ãƒ‡ã‚¤ãƒªãƒ¼ã‚¤ãƒ³ã‚µã‚¤ãƒˆã€‘**\n\n"
                            f"- ã“ã®æ—¥ã®è¨­å®šæ™‚é–“å¸¯ï¼ˆ{time_range[0]}æ™‚ã€œ{time_range[1]}æ™‚ï¼‰ã«ãŠã‘ã‚‹ã‚¹ã‚³ã‚¢ã®ãƒ”ãƒ¼ã‚¯ã¯ **{max_idx.strftime('%H:%M')}é ƒ** ï¼ˆã‚¹ã‚³ã‚¢: {max_val:.1f}ï¼‰ã§ã—ãŸã€‚\n"
                            f"- å¹³å‡ã‚¹ã‚³ã‚¢ã¯ **{avg_val:.1f}** ã¨ãªã£ã¦ã„ã¾ã™ã€‚\n"
                            f"- ã‚°ãƒ©ãƒ•ã«ãŠã„ã¦åŸºæº–å€¤(50)ã‚ˆã‚Šä¸Šå´ã®**é’ã„é¢**ãŒã€Œ{state_high}ã€ã—ã¦ã„ã‚‹çŠ¶æ…‹ã€ä¸‹å´ã®**ã‚ªãƒ¬ãƒ³ã‚¸ã®é¢**ãŒã€Œ{state_low}ã€ã—ã¦ã„ã‚‹çŠ¶æ…‹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚")
                else:
                    st.write("ã“ã®æ—¥ã®æœ‰åŠ¹ãªã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            else:
                st.write(f"å¯¾è±¡æ™‚é–“å¸¯ã®ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã€ã¾ãŸã¯ã€Œ{score_col}ã€ãŒå«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ«ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")

    with tab4:
        st.markdown("#### è¡Œå‹•ãƒªã‚¿ãƒ¼ãƒ³åˆ†æï¼ˆé‡å›å¸°åˆ†æï¼‰")
        st.markdown(f"éå»ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ã€Œç¾åœ¨ã€ãŠã‚ˆã³ã€Œç›´å‰ï¼ˆ{RESAMPLE_FREQ}å‰ï¼‰ã€ã®ä¼‘æ†©ã‚„çŸ­æ™‚é–“æ­©è¡Œã¨ã„ã£ãŸè¡Œå‹•ãŒã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«ã©ã‚Œã ã‘ã®ãƒ—ãƒ©ã‚¹/ãƒã‚¤ãƒŠã‚¹åŠ¹æœã‚’ä¸ãˆã¦ã„ã‚‹ã‹ã‚’çµ±è¨ˆçš„ã«ç®—å‡ºã—ã¾ã™ã€‚")
        
        reg_df = df_imp.copy()
        lag_steps = 1 # ç›´å‰ï¼ˆ1ã‚¹ãƒ†ãƒƒãƒ—å‰ï¼‰ã®è¡Œå‹•ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã‚·ãƒ•ãƒˆ
        
        action_cols = []
        if 'ä¼‘æ†©åˆ¤å®š' in reg_df.columns: 
            action_cols.append('ä¼‘æ†©åˆ¤å®š')
            reg_df['ä¼‘æ†©åˆ¤å®š_å‰'] = reg_df['ä¼‘æ†©åˆ¤å®š'].shift(lag_steps)
            action_cols.append('ä¼‘æ†©åˆ¤å®š_å‰')
        if 'çŸ­æ™‚é–“æ­©è¡Œ' in reg_df.columns: 
            action_cols.append('çŸ­æ™‚é–“æ­©è¡Œ')
            reg_df['çŸ­æ™‚é–“æ­©è¡Œ_å‰'] = reg_df['çŸ­æ™‚é–“æ­©è¡Œ'].shift(lag_steps)
            action_cols.append('çŸ­æ™‚é–“æ­©è¡Œ_å‰')
        
        control_cols = []
        if 'is_meeting' in reg_df.columns: control_cols.append('is_meeting')
        if 'schedule_density_2h' in reg_df.columns: control_cols.append('schedule_density_2h')
        
        if not action_cols:
            st.write("åˆ†æã«å¿…è¦ãªè¡Œå‹•ãƒ‡ãƒ¼ã‚¿ï¼ˆã€Œä¼‘æ†©åˆ¤å®šã€ã‚„ã€ŒçŸ­æ™‚é–“æ­©è¡Œã€ï¼‰ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        else:
            X_cols = action_cols + control_cols
            reg_df = reg_df.dropna(subset=X_cols + [target_col])
            
            if len(reg_df) > 10:
                X = reg_df[X_cols].astype(float)
                y = reg_df[target_col].astype(float)
                
                try:
                    import statsmodels.api as sm
                    # å®šæ•°é …ï¼ˆåˆ‡ç‰‡ï¼‰ã‚’è¿½åŠ ã—ã¦OLSãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
                    X_sm = sm.add_constant(X)
                    model_sm = sm.OLS(y, X_sm)
                    results = model_sm.fit()
                    
                    # çµ±è¨ˆå€¤ã®å–å¾—
                    nobs = int(results.nobs)
                    r2 = results.rsquared
                    r2_adj = results.rsquared_adj
                    
                    coef_dict = {}
                    pvalue_dict = {}
                    for col in action_cols:
                        if col in results.params:
                            coef_dict[col] = results.params[col]
                            pvalue_dict[col] = results.pvalues[col]
                    
                    # --- çµ±è¨ˆã‚µãƒãƒªã®è¡¨ç¤º ---
                    st.markdown("##### ğŸ“ˆ çµ±è¨ˆã‚µãƒãƒª")
                    col_s1, col_s2, col_s3 = st.columns(3)
                    col_s1.metric("ã‚µãƒ³ãƒ—ãƒ«æ•° (n)", f"{nobs} ä»¶")
                    col_s2.metric("æ±ºå®šä¿‚æ•° (RÂ²)", f"{r2:.3f}")
                    col_s3.metric("è‡ªç”±åº¦èª¿æ•´æ¸ˆ RÂ²", f"{r2_adj:.3f}")
                    
                    st.markdown("##### ğŸ“ å›å¸°ä¿‚æ•°ã¨På€¤ã®è©³ç´°")
                    detail_data = []
                    for col in results.params.index:
                        if col == "const":
                            col_name = "å®šæ•°é … (Intercept)"
                        elif '_å‰' in col:
                            base_name = jp_feat_name(col.replace('_å‰', ''))
                            col_name = f"ç›´å‰ã®ã€Œ{base_name}ã€"
                        else:
                            base_name = jp_feat_name(col)
                            if col in ['ä¼‘æ†©åˆ¤å®š', 'çŸ­æ™‚é–“æ­©è¡Œ']:
                                col_name = f"ç¾åœ¨ã®ã€Œ{base_name}ã€"
                            else:
                                col_name = f"ã€Œ{base_name}ã€"
                            
                        pval = results.pvalues[col]
                        sig = "â­ æœ‰æ„" if pval < 0.05 else "ãƒ¼"
                        
                        detail_data.append({
                            "å¤‰æ•°å": col_name,
                            "ä¿‚æ•° (åŠ¹æœé‡)": results.params[col],
                            "æ¨™æº–èª¤å·®": results.bse[col],
                            "tå€¤": results.tvalues[col],
                            "På€¤": pval,
                            "æœ‰æ„åˆ¤å®š": sig
                        })
                    
                    df_detail = pd.DataFrame(detail_data)
                    st.dataframe(df_detail.style.format({
                        "ä¿‚æ•° (åŠ¹æœé‡)": "{:.4f}",
                        "æ¨™æº–èª¤å·®": "{:.4f}",
                        "tå€¤": "{:.3f}",
                        "På€¤": "{:.4f}"
                    }), use_container_width=True)
                    st.caption("â€» På€¤ãŒ0.05æœªæº€ï¼ˆ5%æ°´æº–ï¼‰ã®å ´åˆã€ã€Œçµ±è¨ˆçš„ã«æœ‰æ„ï¼ˆå¶ç„¶ã§ã¯ãªãå®Ÿéš›ã«åŠ¹æœãŒã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ï¼‰ã€ã¨åˆ¤å®šã•ã‚Œã¾ã™ã€‚")

                except ImportError:
                    st.warning("è©³ç´°ãªçµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã«ã¯ `statsmodels` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™ã€‚`requirements.txt` ã« `statsmodels` ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚ä»Šå›ã¯ `scikit-learn` ã«ã‚ˆã‚‹ç°¡æ˜“åˆ†æã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
                    from sklearn.linear_model import LinearRegression
                    model_reg = LinearRegression()
                    model_reg.fit(X, y)
                    coef_dict = {col: coef for col, coef in zip(X_cols, model_reg.coef_) if col in action_cols}
                    pvalue_dict = {col: np.nan for col in action_cols}

                # ã‚°ãƒ©ãƒ•æç”»
                action_names = []
                for col in coef_dict.keys():
                    if '_å‰' in col:
                        action_names.append(f"ç›´å‰ã®ã€Œ{jp_feat_name(col.replace('_å‰', ''))}ã€")
                    else:
                        action_names.append(f"ç¾åœ¨ã®ã€Œ{jp_feat_name(col)}ã€")

                coef_values = list(coef_dict.values())
                colors = ['#E24A4A' if c < 0 else '#4AE290' for c in coef_values]
                
                fig_roi = go.Figure(data=[go.Bar(
                    x=action_names, 
                    y=coef_values, 
                    marker_color=colors,
                    text=[f"{c*100:+.1f} pt" for c in coef_values],
                    textposition='auto',
                    hovertemplate="è¡Œå‹•: %{x}<br>åŠ¹æœé‡: %{y:+.3f}<extra></extra>"
                )])
                
                target_label = jp_feat_name(target_col)
                fig_roi.update_layout(
                    title=f"å„è¡Œå‹•ãŒã€Œ{target_label}ã€ã«ä¸ãˆã‚‹ç´”ç²‹ãªåŠ¹æœé‡",
                    xaxis_title="è¡Œå‹•",
                    yaxis_title="åŠ¹æœé‡ (ä¿‚æ•°)",
                    height=400,
                    margin=dict(l=20, r=20, t=40, b=20),
                    plot_bgcolor='rgba(0,0,0,0)'
                )
                fig_roi.update_yaxes(showgrid=True, gridcolor='lightgray', zeroline=True, zerolinecolor='black', zerolinewidth=1)
                st.plotly_chart(fig_roi, use_container_width=True)
                
                # ã‚¤ãƒ³ã‚µã‚¤ãƒˆã®ç”Ÿæˆ
                st.markdown("##### ğŸ’¡ åˆ†æçµæœï¼ˆè¡Œå‹•ã®æŠ•è³‡å¯¾åŠ¹æœï¼‰")
                for col, coef in coef_dict.items():
                    if '_å‰' in col:
                        action_desc = f"äº‹å‰ã«ã€Œ{jp_feat_name(col.replace('_å‰', ''))}ã€ã‚’è¡Œã†ã“ã¨"
                    else:
                        action_desc = f"ç¾åœ¨ã€Œ{jp_feat_name(col)}ã€ã‚’è¡Œã†ã“ã¨"

                    effect_pt = coef * 100
                    pval = pvalue_dict.get(col, np.nan)
                    
                    sig_note = ""
                    if not np.isnan(pval) and pval >= 0.05:
                        sig_note = " *(â€»På€¤ãŒ0.05ä»¥ä¸Šã®ãŸã‚ã€ã“ã®åŠ¹æœã¯å¶ç„¶ã®èª¤å·®ã®ç¯„å›²ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™)*"

                    if target_col in ['NEMUKE_SCORE_NEW', 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š', 'çœ æ°—åˆ¤å®š', 'å¼·ã„çœ æ°—åˆ¤å®š']:
                        # æ‚ªåŒ–ç³»ã®æŒ‡æ¨™ã®å ´åˆï¼ˆãƒã‚¤ãƒŠã‚¹ãŒè‰¯ã„åŠ¹æœï¼‰
                        if coef < -0.01:
                            st.write(f"- ğŸŸ¢ **{action_desc}**: ã€Œ{target_label}ã€ã®ç™ºç”Ÿã‚’ **å¹³å‡ {abs(effect_pt):.1f} ãƒã‚¤ãƒ³ãƒˆæŠ‘ãˆã‚‹** åŠ¹æœï¼ˆãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥åŠ¹æœï¼‰ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚{sig_note}")
                        elif coef > 0.01:
                            st.write(f"- ğŸ”´ **{action_desc}**: é€†ã«ã€Œ{target_label}ã€ã®ç™ºç”Ÿã‚’ **å¹³å‡ {abs(effect_pt):.1f} ãƒã‚¤ãƒ³ãƒˆæ‚ªåŒ–** ã•ã›ã¦ã—ã¾ã†å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã®è¦‹ç›´ã—ãŒå¿…è¦ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚{sig_note}")
                        else:
                            st.write(f"- âšª **{action_desc}**: ã€Œ{target_label}ã€ã«å¯¾ã™ã‚‹ç›´æ¥çš„ãªå¢—æ¸›åŠ¹æœã¯ã»ã¨ã‚“ã©è¦‹ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                    else:
                        # å¥½è»¢ç³»ã®æŒ‡æ¨™ã®å ´åˆï¼ˆãƒ—ãƒ©ã‚¹ãŒè‰¯ã„åŠ¹æœï¼‰
                        if coef > 0.01:
                            st.write(f"- ğŸŸ¢ **{action_desc}**: ã€Œ{target_label}ã€ã®ç™ºç”Ÿã‚’ **å¹³å‡ {abs(effect_pt):.1f} ãƒã‚¤ãƒ³ãƒˆé«˜ã‚ã‚‹** åŠ¹æœï¼ˆãƒ–ãƒ¼ã‚¹ãƒˆåŠ¹æœï¼‰ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚ç©æ¥µçš„ã«å–ã‚Šå…¥ã‚Œã¾ã—ã‚‡ã†ã€‚{sig_note}")
                        elif coef < -0.01:
                            st.write(f"- ğŸ”´ **{action_desc}**: é€†ã«ã€Œ{target_label}ã€ã®ç™ºç”Ÿã‚’ **å¹³å‡ {abs(effect_pt):.1f} ãƒã‚¤ãƒ³ãƒˆä½ä¸‹** ã•ã›ã¦ã—ã¾ã†å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚{sig_note}")
                        else:
                            st.write(f"- âšª **{action_desc}**: ã€Œ{target_label}ã€ã«å¯¾ã™ã‚‹ç›´æ¥çš„ãªå¢—æ¸›åŠ¹æœã¯ã»ã¨ã‚“ã©è¦‹ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                            
                st.caption("â€»ã“ã®çµæœã¯ã€Œç¾åœ¨ã®äºˆå®šã®è©°ã¾ã‚Šå…·åˆã€ã‚„ã€Œä¼šè­°ä¸­ã‹ã©ã†ã‹ã€ã¨ã„ã£ãŸä»–ã®æ¡ä»¶ï¼ˆãƒã‚¤ã‚ºï¼‰ã‚’çµ±è¨ˆçš„ã«é™¤å»ã—ã€è¡Œå‹•ãã®ã‚‚ã®ã®ç´”ç²‹ãªåŠ¹æœã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚")
                
                # --- æ±ºå®šæœ¨åˆ†æã«ã‚ˆã‚‹ãƒã‚¤ãƒ«ãƒ¼ãƒ«æŠ½å‡º ---
                st.markdown("---")
                st.markdown("##### ğŸŒ³ æ¡ä»¶ã®çµ„ã¿åˆã‚ã›åˆ†æï¼ˆãƒã‚¤ãƒ»ãƒ«ãƒ¼ãƒ«æŠ½å‡ºï¼‰")
                st.write("æ±ºå®šæœ¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ã¦ã€è¤‡æ•°ã®æ¡ä»¶ï¼ˆäºˆå®šã®çŠ¶æ³ã¨è¡Œå‹•ï¼‰ãŒçµ„ã¿åˆã‚ã•ã£ãŸæ™‚ã«ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒã©ã†å¤‰åŒ–ã™ã‚‹ã‹ã‚’åˆ†æã—ã¾ã™ã€‚")
                
                # ãƒ„ãƒªãƒ¼ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ (åˆ†ã‹ã‚Šã‚„ã™ãã™ã‚‹ãŸã‚æ·±ã•ã‚’2ã«åˆ¶é™)
                from sklearn.tree import DecisionTreeRegressor, _tree, plot_tree
                tree_model = DecisionTreeRegressor(max_depth=2, min_samples_leaf=5, random_state=42)
                tree_model.fit(X, y)
                
                # ç‰¹å¾´é‡è¡¨ç¤ºåã¨çœŸå½å€¤åˆ¤å®šã®ãƒªã‚¹ãƒˆä½œæˆ
                feature_display_names = []
                feature_is_bool = []
                for col in X_cols:
                    feature_is_bool.append(reg_df[col].dropna().nunique() <= 2)
                    if col == 'is_meeting':
                        feature_display_names.append("ä¼šè­°ä¸­")
                    elif col == 'schedule_density_2h':
                        feature_display_names.append("äºˆå®šå¯†åº¦")
                    elif '_å‰' in col:
                        base = get_base_feature_name(col.replace('_å‰', ''))
                        feature_display_names.append(f"ç›´å‰ã®{base}")
                    else:
                        base = get_base_feature_name(col)
                        if col in ['ä¼‘æ†©åˆ¤å®š', 'çŸ­æ™‚é–“æ­©è¡Œ']:
                            feature_display_names.append(f"ç¾åœ¨ã®{base}")
                        else:
                            feature_display_names.append(jp_feat_name(col))

                def extract_rules(tree, feature_names, is_bool_list):
                    tree_ = tree.tree_
                    feature_name = [
                        feature_names[i] if i != _tree.TREE_UNDEFINED else "undefined!"
                        for i in tree_.feature
                    ]
                    rules = []
                    def recurse(node, current_rule):
                        if tree_.feature[node] != _tree.TREE_UNDEFINED:
                            name = feature_name[node]
                            threshold = tree_.threshold[node]
                            is_bool = is_bool_list[tree_.feature[node]]
                            
                            left_rule = current_rule.copy()
                            if is_bool:
                                left_rule.append(f"ã€{name}ï¼šãªã—ã€‘")
                            else:
                                left_rule.append(f"ã€{name}ãŒä½ã„ (â‰¦{threshold:.2f})ã€‘")
                            recurse(tree_.children_left[node], left_rule)
                            
                            right_rule = current_rule.copy()
                            if is_bool:
                                right_rule.append(f"ã€{name}ï¼šã‚ã‚Šã€‘")
                            else:
                                right_rule.append(f"ã€{name}ãŒé«˜ã„ (>{threshold:.2f})ã€‘")
                            recurse(tree_.children_right[node], right_rule)
                        else:
                            val = tree_.value[node][0][0]
                            samples = tree_.n_node_samples[node]
                            rules.append((" ï¼‹ ".join(current_rule), val, samples))
                    recurse(0, [])
                    return rules
                
                tree_rules = extract_rules(tree_model, feature_display_names, feature_is_bool)
                is_negative_target = target_col in ['NEMUKE_SCORE_NEW', 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š', 'çœ æ°—åˆ¤å®š', 'å¼·ã„çœ æ°—åˆ¤å®š']
                
                # ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå…¨ä½“ã®5%ä»¥ä¸Šã®ãƒ«ãƒ¼ãƒ«ã®ã¿æŠ½å‡º
                min_samples_threshold = max(3, int(len(reg_df) * 0.05))
                valid_rules = [r for r in tree_rules if r[2] >= min_samples_threshold]
                if not valid_rules:
                    valid_rules = tree_rules
                
                valid_rules.sort(key=lambda x: x[1], reverse=not is_negative_target)
                
                st.markdown(f"**ğŸ¯ ã‚ãªãŸã®ã€Œ{target_label}ã€ã«é–¢ã™ã‚‹ãƒ™ã‚¹ãƒˆæ¡ä»¶ãƒ‘ã‚¿ãƒ¼ãƒ³**")
                
                if is_negative_target:
                    st.write(f"â€»ã‚¹ã‚³ã‚¢ãŒ**ä½ã„**ï¼ˆç™ºç”Ÿç¢ºç‡ãŒä½ã„ï¼‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ™ã‚¹ãƒˆæ¡ä»¶ã¨ã—ã¦è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")
                else:
                    st.write(f"â€»ã‚¹ã‚³ã‚¢ãŒ**é«˜ã„**ï¼ˆç™ºç”Ÿç¢ºç‡ãŒé«˜ã„ï¼‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ™ã‚¹ãƒˆæ¡ä»¶ã¨ã—ã¦è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")

                if valid_rules:
                    rule_text, val, samples = valid_rules[0]
                    display_val = val * 100
                    st.markdown(f"ğŸ¥‡ **ç¬¬1ä½** (ãƒ‡ãƒ¼ã‚¿æ•°: {samples}ä»¶)")
                    st.markdown(f"ã€€æ¡ä»¶ï¼š {rule_text}")
                    st.markdown(f"ã€€ğŸ‘‰ äºˆæƒ³ã‚¹ã‚³ã‚¢: **{display_val:.1f} pt**")
                else:
                    st.write("æœ‰åŠ¹ãªãƒ«ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    
                # æ¨¹å½¢å›³ã®æç”»
                st.markdown("##### ğŸŒ¿ æ±ºå®šæœ¨ã®æ¨¹å½¢å›³")
                st.caption("â€» ä¸€ç•ªä¸Šã®ãƒã‚³ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã—ã€æ¡ä»¶ãŒã€ŒTrueï¼ˆå½“ã¦ã¯ã¾ã‚‹ï¼‰ã€ãªã‚‰å·¦ã¸ã€ã€ŒFalseï¼ˆå½“ã¦ã¯ã¾ã‚‰ãªã„ï¼‰ã€ãªã‚‰å³ã¸é€²ã¿ã¾ã™ã€‚è‰²ã®æ¿ƒã•ã¯ã‚¹ã‚³ã‚¢ã®é«˜ä½ã‚’è¡¨ã—ã¾ã™ã€‚")
                fig_tree, ax_tree = plt.subplots(figsize=(10, 6))
                plot_tree(tree_model, feature_names=feature_display_names, filled=True, rounded=True, ax=ax_tree, fontsize=12, precision=2)
                st.pyplot(fig_tree)
                
                # --- åˆ†æãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³è¿½åŠ  ---
                st.markdown("---")
                st.markdown("##### ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
                st.write("ã“ã®é‡å›å¸°åˆ†æã«å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ï¼ˆæ¬ æå€¤ç­‰ã‚’é™¤å»ã—ãŸã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚")
                
                # Excelã§é–‹ã„ãŸéš›ã®æ–‡å­—åŒ–ã‘ã‚’é˜²ããŸã‚ã« utf-8-sig (BOMä»˜ãUTF-8) ã‚’ä½¿ç”¨
                csv_data = reg_df[X_cols + [target_col]].to_csv().encode('utf-8-sig')
                st.download_button(
                    label="ğŸ“Š åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (.csv)",
                    data=csv_data,
                    file_name='regression_analysis_data.csv',
                    mime='text/csv',
                )

            else:
                st.write("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã‚‹ãŸã‚ã€çµ±è¨ˆåˆ†æã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã€‚")

    # =========================================================================
    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ (Real-time Focus)
    # =========================================================================
    st.header("âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ (Real-time Focus)")
    
    auc_eval = "ç®—å‡ºä¸å¯"
    if not np.isnan(auc_test):
        if auc_test >= 0.8: auc_eval = "ğŸŸ¢ éå¸¸ã«è‰¯ã„"
        elif auc_test >= 0.7: auc_eval = "ğŸ”µ è‰¯ã„ (å®Ÿç”¨ãƒ¬ãƒ™ãƒ«)"
        elif auc_test >= 0.6: auc_eval = "ğŸŸ¡ æ™®é€š"
        else: auc_eval = "ğŸ”´ æ”¹å–„ãŒå¿…è¦"

    loss_eval = "ç®—å‡ºä¸å¯"
    if not np.isnan(logloss_test):
        if logloss_test <= 0.4: loss_eval = "ğŸŸ¢ éå¸¸ã«è‰¯ã„"
        elif logloss_test <= 0.6: loss_eval = "ğŸŸ¡ æ™®é€š"
        else: loss_eval = "ğŸ”´ æ”¹å–„ãŒå¿…è¦"

    col_m1, col_m2 = st.columns(2)
    col_m1.info(f"**ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ (AUC-ROC)**: {auc_test:.3f} ğŸ‘‰ **{auc_eval}**\n\n*1.0ã«è¿‘ã„ã»ã©çŠ¶æ…‹ã®åˆ¤åˆ¥ãŒæ­£ç¢ºã«ã§ãã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ï¼ˆ0.7ä»¥ä¸ŠãŒå®Ÿç”¨ã®ç›®å®‰ï¼‰ã€‚*")
    col_m2.info(f"**äºˆæ¸¬ã®ç¢ºä¿¡åº¦ (Log Loss)**: {logloss_test:.3f} ğŸ‘‰ **{loss_eval}**\n\n*0.0ã«è¿‘ã„ã»ã©AIãŒã€Œè¿·ã„ãªãã€æ­£è§£ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ï¼ˆ0.6ä»¥ä¸‹ãŒç›®å®‰ï¼‰ã€‚*")
    
    with st.expander("ğŸ“Š ãƒ†ã‚¹ãƒˆæœŸé–“ã®äºˆæ¸¬ç¢ºç‡æ¨ç§»ã‚’è¡¨ç¤º"):
        fig_ts_plot = go.Figure()
        fig_ts_plot.add_trace(go.Scatter(
            x=test_df.index, y=y_test_class, mode='markers', name='å®Ÿéš›ã®çŠ¶æ…‹ (1=Yes, 0=No)',
            marker=dict(color='blue', opacity=0.6, size=6), hovertemplate="æ—¥æ™‚: %{x}<br>çŠ¶æ…‹: %{y}<extra></extra>"
        ))
        fig_ts_plot.add_trace(go.Scatter(
            x=test_df.index, y=preds_proba, mode='lines', name='LightGBM äºˆæ¸¬ç¢ºç‡',
            line=dict(color='red', width=2), opacity=0.8, hovertemplate="æ—¥æ™‚: %{x}<br>äºˆæ¸¬ç¢ºç‡: %{y:.2f}<extra></extra>"
        ))
        fig_ts_plot.update_layout(title=f"ãƒ†ã‚¹ãƒˆæœŸé–“ã® {selected_target_name} äºˆæ¸¬ç¢ºç‡ã®æ¨ç§»", hovermode="x unified", legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))
        st.plotly_chart(fig_ts_plot, use_container_width=True)

    st.subheader("ğŸ”® ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã¨è¦å› åˆ†æ")
    available_data_all = df_imp.drop(columns=drop_cols, errors='ignore')
    if TARGET_DATETIME is not None:
        try:
            target_dt = pd.to_datetime(TARGET_DATETIME)
            available_data = available_data_all[available_data_all.index <= target_dt]
            if len(available_data) == 0:
                st.warning("æŒ‡å®šã•ã‚ŒãŸåŸºæº–æ—¥æ™‚ä»¥å‰ã®ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                available_data = available_data_all
        except Exception as e:
            st.warning(f"æ—¥æ™‚ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆ{e}ï¼‰ã€‚æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    else:
        available_data = available_data_all

    target_data = available_data.iloc[-1:]
    current_time = target_data.index[0]
    current_val = float(target_data[target_col].values[0])
    current_state_bool = current_val >= target_threshold
    current_proba = model.predict_proba(target_data)[0, 1]
    predicted_state_bool = current_proba >= 0.5
    
    col_p1, col_p2, col_p3, col_p4 = st.columns(4)
    col_p1.metric("åŸºæº–æ—¥æ™‚", current_time.strftime('%Y-%m-%d %H:%M'))
    col_p2.metric(f"ç¾åœ¨ã® {selected_target_name} çŠ¶æ…‹", "Yes" if current_state_bool else "No")
    col_p3.metric(f"{PREDICT_AHEAD}å¾Œã®äºˆæ¸¬åˆ¤å®š", "Yes" if predicted_state_bool else "No")
    col_p4.metric(f"ç™ºç”Ÿç¢ºç‡", f"{current_proba * 100:.1f} %")
    st.caption(f"â€» **äºˆæ¸¬åˆ¤å®šã¨ç™ºç”Ÿç¢ºç‡ã«ã¤ã„ã¦**: {PREDICT_AHEAD}å¾Œã«ã‚ãªãŸãŒã€Œ{selected_target_name}ã€ã®çŠ¶æ…‹ã«ãªã£ã¦ã„ã‚‹ç¢ºç‡ã‚’AIãŒç®—å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚50%ä»¥ä¸Šã‚’ã€ŒYesã€ã¨åˆ¤å®šã—ã¦ã„ã¾ã™ã€‚")

    with st.spinner("SHAPã§è¦å› ã‚’åˆ†æã—ã¦ã„ã¾ã™..."):
        explainer = shap.TreeExplainer(model)
        shap_values_latest = explainer(target_data)
        if len(shap_values_latest.shape) == 3:
            shap_vals = shap_values_latest[0, :, 1].values
            shap_base_obj = shap_values_latest[0, :, 1]
        else:
            shap_vals = shap_values_latest[0].values
            shap_base_obj = shap_values_latest[0]
        
        def is_actionable(col: str) -> bool: return not (target_col in col or col in ["hour", "dayofweek"])
        exp_df = pd.DataFrame({'Feature': target_data.columns, 'Value': target_data.values[0], 'SHAP': shap_vals})
        exp_df['AbsSHAP'] = exp_df['SHAP'].abs()
        exp_df_action = exp_df[exp_df['Feature'].apply(is_actionable)].sort_values('AbsSHAP', ascending=False)
        
        fig2, ax2 = plt.subplots(figsize=(8, 4))
        shap.plots.waterfall(shap_base_obj, show=False)
        st.pyplot(fig2)

        st.markdown("**ã€è¦å› åˆ†æã®è§£èª¬ã€‘**")
        st.caption("â€» ä¸Šè¨˜ã®SHAPã‚°ãƒ©ãƒ•ã¯å°‚ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãŸã‚é™æ­¢ç”»åƒã§å‡ºåŠ›ã—ã¦ã„ã¾ã™ã€‚ä¸€ç•ªé•·ã„ãƒãƒ¼ï¼ˆèµ¤ã¾ãŸã¯é’ï¼‰ãŒç¢ºç‡ã«æœ€ã‚‚å½±éŸ¿ã‚’ä¸ãˆãŸè¦å› ã§ã™ã€‚")
        
        pos_factors = exp_df_action[exp_df_action['SHAP'] > 0]
        neg_factors = exp_df_action[exp_df_action['SHAP'] < 0]
        
        if target_col in ['NEMUKE_SCORE_NEW', 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š', 'çœ æ°—åˆ¤å®š', 'å¼·ã„çœ æ°—åˆ¤å®š']:
            pos_effect_text, neg_effect_text = "ç¢ºç‡ä¸Šæ˜‡ï¼ˆæ‚ªåŒ–æ–¹å‘ï¼‰", "ç¢ºç‡ä½ä¸‹ï¼ˆå¥½è»¢æ–¹å‘ï¼‰"
            bar_desc = f"â€»ã‚°ãƒ©ãƒ•ã®èµ¤ã„ãƒãƒ¼ãŒ{selected_target_name}ã®ç™ºç”Ÿç¢ºç‡ã‚’æŠ¼ã—ä¸Šã’ã‚‹ï¼ˆæ‚ªåŒ–ï¼‰è¦å› ã€é’ã„ãƒãƒ¼ãŒæŠ¼ã—ä¸‹ã’ã‚‹ï¼ˆå¥½è»¢ï¼‰è¦å› ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚"
        else:
            pos_effect_text, neg_effect_text = "ç¢ºç‡ä¸Šæ˜‡ï¼ˆå¥½è»¢æ–¹å‘ï¼‰", "ç¢ºç‡ä½ä¸‹ï¼ˆæ‚ªåŒ–æ–¹å‘ï¼‰"
            bar_desc = f"â€»ã‚°ãƒ©ãƒ•ã®èµ¤ã„ãƒãƒ¼ãŒ{selected_target_name}ã®ç™ºç”Ÿç¢ºç‡ã‚’æŠ¼ã—ä¸Šã’ã‚‹ï¼ˆå¥½è»¢ï¼‰è¦å› ã€é’ã„ãƒãƒ¼ãŒæŠ¼ã—ä¸‹ã’ã‚‹ï¼ˆæ‚ªåŒ–ï¼‰è¦å› ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚"

        base_pos = None
        if not pos_factors.empty:
            top_pos = pos_factors.iloc[0]
            desc_pos = get_factor_direction_text(top_pos['Feature'], top_pos['Value'], available_data_all)
            base_pos = get_base_feature_name(top_pos['Feature'])
            st.write(f"- ğŸ“ˆ **ç¢ºç‡ã‚’ä¸Šã’ã‚‹è¦å› **: **{desc_pos}** ãŒ{pos_effect_text}ã«åƒã„ã¦ã„ã¾ã™ (å½±éŸ¿åº¦: {top_pos['SHAP']:+.2f})ã€‚")
            
        if not neg_factors.empty:
            top_neg = neg_factors.iloc[0]
            if base_pos is not None and get_base_feature_name(top_neg['Feature']) == base_pos and len(neg_factors) > 1:
                top_neg = neg_factors.iloc[1]
            desc_neg = get_factor_direction_text(top_neg['Feature'], top_neg['Value'], available_data_all)
            st.write(f"- ğŸ“‰ **ç¢ºç‡ã‚’ä¸‹ã’ã‚‹è¦å› **: **{desc_neg}** ãŒ{neg_effect_text}ã«åƒã„ã¦ã„ã¾ã™ (å½±éŸ¿åº¦: {top_neg['SHAP']:+.2f})ã€‚")
            
        st.caption(bar_desc)

    schedule_density = float(target_data["schedule_density_2h"].values[0]) if "schedule_density_2h" in target_data.columns else 0
    time_to_next = float(target_data["time_to_next_event_min"].values[0]) if "time_to_next_event_min" in target_data.columns else np.nan
    is_meeting = float(target_data["is_meeting"].values[0]) if "is_meeting" in target_data.columns else 0
    
    state_trend_prob = 1.0 - current_proba if target_col in ['NEMUKE_SCORE_NEW', 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š', 'çœ æ°—åˆ¤å®š', 'å¼·ã„çœ æ°—åˆ¤å®š'] else current_proba

    reasons = []
    if is_meeting > 0: reasons.append("ç¾åœ¨ä¼šè­°ä¸­")
    if schedule_density >= 0.6: reasons.append("äºˆå®šå¯†åº¦ãŒé«˜ã„")
    if state_trend_prob >= 0.6: reasons.append(f"{selected_target_name}ã®å¥½ã¾ã—ã„ç¢ºç‡ãŒé«˜ã„")
    elif state_trend_prob <= 0.4: reasons.append(f"{selected_target_name}ã®å¥½ã¾ã—ããªã„ç¢ºç‡ãŒé«˜ã„")
    
    if is_meeting > 0:
        work_mode, advice = "E: æ®µå–ã‚Šï¼ˆä¼šè­°ãƒ¢ãƒ¼ãƒ‰ï¼‰", "è«–ç‚¹ã‚’1æšã«æ•´ç†ã—ã€æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ToDoåŒ–ã—ã¾ã—ã‚‡ã†ã€‚"
    elif state_trend_prob >= 0.6 and (np.isnan(time_to_next) or time_to_next >= 50) and schedule_density < 0.6:
        work_mode, advice = "C: ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆï¼ˆæ·±ï¼‰- ä¼ç”»ãƒ»æˆ¦ç•¥", "çŠ¶æ…‹ãŒå¥½è»¢ã™ã‚‹ç¢ºç‡ãŒé«˜ãã€ã¾ã¨ã¾ã£ãŸæ™‚é–“ã‚‚ã‚ã‚Šã¾ã™ã€‚è¨­è¨ˆãƒ»ä¼ç”»ã®éª¨æ ¼ã¥ãã‚Šãªã©ã€é‡ã„æ€è€ƒã‚¿ã‚¹ã‚¯ã‚’é€²ã‚ã‚‹ã®ãŒæœ€é©ã§ã™ã€‚"
    elif state_trend_prob <= 0.4 or schedule_density >= 0.6:
        work_mode, advice = "D: ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆï¼ˆè»½ï¼‰- æ•´ç†ãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼", "äºˆå®šãŒç´°åˆ‡ã‚Œã‹ã€çŠ¶æ…‹ãŒæ‚ªåŒ–ã™ã‚‹ç¢ºç‡ãŒé«˜ã„ã§ã™ã€‚10ã€œ20åˆ†ã§çµ‚ã‚ã‚‹ToDoæ¶ˆåŒ–ã‚„ã€è³‡æ–™ã®æ•´å½¢ãƒ»ãƒã‚§ãƒƒã‚¯ä½œæ¥­ã«æ™‚é–“ã‚’å½“ã¦ã¾ã—ã‚‡ã†ã€‚"
    else:
        if (np.isnan(time_to_next) or time_to_next >= 30) and schedule_density < 0.6:
            work_mode, advice = "A: ã‚¤ãƒ³ãƒ—ãƒƒãƒˆï¼ˆé‡ï¼‰ ã¾ãŸã¯ B: ã‚¤ãƒ³ãƒ—ãƒƒãƒˆï¼ˆè»½ï¼‰", "é›£ã—ã‚è³‡æ–™ã®èª­ã¿è¾¼ã¿ã‚„æƒ…å ±æ•´ç†ãªã©ã€æ¬¡ã®æ·±ã„æ€è€ƒã«å‘ã‘ãŸã‚¤ãƒ³ãƒ—ãƒƒãƒˆä½œæ¥­ã«é©ã—ã¦ã„ã¾ã™ã€‚"
        else:
            work_mode, advice = "E: æ®µå–ã‚Š", "æ¬¡ã®æ·±ã„ä½œæ¥­ã¸ã‚¹ãƒ ãƒ¼ã‚ºã«å…¥ã‚Œã‚‹ã‚ˆã†ã€è«–ç‚¹ã®åˆ—æŒ™ã‚„å„ªå…ˆé †ä½ä»˜ã‘ã€ç´ æã®æ´—ã„å‡ºã—ã‚’è¡Œã„ã¾ã—ã‚‡ã†ã€‚"

    st.subheader("ğŸ“ åˆ†æãƒ¬ãƒãƒ¼ãƒˆ (AIã«ã‚ˆã‚‹ææ¡ˆ)")
    main_factor_desc = get_factor_direction_text(exp_df_action.iloc[0]['Feature'], exp_df_action.iloc[0]['Value'], available_data_all) if not exp_df_action.empty else "ä¸æ˜"
    prompt_context = f"ç¾åœ¨æ™‚åˆ»: {current_time.strftime('%Y-%m-%d %H:%M')}\nç¾åœ¨ã®{selected_target_name}ã®çŠ¶æ…‹: {'Yes' if current_state_bool else 'No'}\n{PREDICT_AHEAD}å¾Œã®äºˆæ¸¬åˆ¤å®š: {'Yes' if predicted_state_bool else 'No'} (ç™ºç”Ÿç¢ºç‡: {current_proba * 100:.1f}%)\nç›´è¿‘ã®ä¸»è¦å› : {main_factor_desc} (SHAP: {exp_df_action.iloc[0]['SHAP']:+.2f})\nåˆ¤å®šã•ã‚ŒãŸåƒãæ–¹: {work_mode}\nç†ç”±: {', '.join(reasons) if reasons else 'ç‰¹ã«ãªã—'}"
    
    if use_gemini and api_key:
        with st.spinner("GeminiãŒãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­..."):
            try:
                genai.configure(api_key=api_key)
                model_llm = genai.GenerativeModel('gemini-2.5-flash')
                resp = model_llm.generate_content(f"ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€å®¢è¦³çš„ãªåƒãæ–¹ã‚¢ãƒ‰ãƒã‚¤ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n\n{prompt_context}\n\næ§‹æˆ:\n1. äºˆæ¸¬çµæœã¨ä¸»ãªè¦å› \n2. å¥¨åŠ±ã™ã‚‹åƒãæ–¹ã®å…·ä½“ä¾‹")
                st.write(resp.text)
            except Exception as e:
                st.error(f"Gemini APIã‚¨ãƒ©ãƒ¼: {e}")
    else:
        st.info("ğŸ’¡ Gemini APIã‚­ãƒ¼ãŒæœªå…¥åŠ›ã®ãŸã‚ã€ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
        st.markdown(f"#### 1. è¿‘ã„å°†æ¥ï¼ˆ{PREDICT_AHEAD}å¾Œï¼‰ã®äºˆæ¸¬çµæœ")
        st.write(f"åŸºæº–æ—¥æ™‚ï¼ˆ{current_time.strftime('%Y-%m-%d %H:%M')}ï¼‰ã® {selected_target_name} ã¯ **{'Yes' if current_state_bool else 'No'}** ã®çŠ¶æ…‹ã§ã™ã€‚")
        st.write(f"{PREDICT_AHEAD}å¾Œã¯ **{'Yes' if predicted_state_bool else 'No'}** ï¼ˆç™ºç”Ÿç¢ºç‡ **{current_proba * 100:.1f} %**ï¼‰ã¨äºˆæ¸¬ã•ã‚Œã¾ã™ã€‚\nã“ã®äºˆæ¸¬ã®ä¸»ãªè¦å› ã¨ã—ã¦ã€**{main_factor_desc}** ãŒå½±éŸ¿ã—ã¦ã„ã¾ã™ã€‚")
        st.markdown(f"#### 2. å¥¨åŠ±ã™ã‚‹åƒãæ–¹")
        st.write(f"ç¾åœ¨ã®äºˆæ¸¬ç¢ºç‡ã¨äºˆå®šçŠ¶æ³ï¼ˆ{', '.join(reasons) if reasons else 'é˜»å®³è¦å› ãªã—'}ï¼‰ã‹ã‚‰ã€**ã€Œ{work_mode}ã€**ã«å–ã‚Šçµ„ã‚€ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n**ğŸ’¡ é€²ã‚æ–¹ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹**: {advice}")

# --- UI ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ ---
st.write("### ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
col_file1, col_file2 = st.columns(2)
with col_file1:
    file_ts = st.file_uploader("1. ç”Ÿä½“ãƒ‡ãƒ¼ã‚¿ (CSVå½¢å¼)", type=['csv'])
with col_file2:
    file_sched = st.file_uploader("2. äºˆå®šè¡¨ãƒ‡ãƒ¼ã‚¿ (äºˆå®šè¡¨.CSV) â€»ä»»æ„", type=['csv'])

if st.button("ğŸš€ åˆ†æã‚’å®Ÿè¡Œã™ã‚‹", type="primary"):
    if file_ts is not None:
        # åˆ†æå®Ÿè¡Œãƒ•ãƒ©ã‚°ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ï¼ˆç”»é¢å†æç”»ã§æ¶ˆãˆãªã„ã‚ˆã†ã«ã™ã‚‹ï¼‰
        st.session_state['run_analysis'] = True
    else:
        st.warning("âš ï¸ ç”Ÿä½“ãƒ‡ãƒ¼ã‚¿ (CSVå½¢å¼) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ãƒ•ãƒ©ã‚°ãŒã‚ã‚‹å ´åˆã®ã¿åˆ†æã‚’å®Ÿè¡Œãƒ»è¡¨ç¤ºã—ç¶šã‘ã‚‹
if st.session_state.get('run_analysis', False) and file_ts is not None:
    try:
        # ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³å¤‰æ›´æ™‚ã®å†èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã‚’é˜²ããŸã‚ã«ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«æˆ»ã™
        file_ts.seek(0)
        df_ts = pd.read_csv(file_ts, skiprows=2)
        
        df_sched = None
        if file_sched is not None:
            file_sched.seek(0)
            df_sched = pd.read_csv(file_sched)
            
        run_analysis(df_ts, df_sched, use_gemini=True if api_key else False)
    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.session_state['run_analysis'] = False