# -*- coding: utf-8 -*-
"""
ã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ« + Outlookã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« é›†ä¸­ãƒ»ç–²åŠ´äºˆæ¸¬ã‚¢ãƒ—ãƒª (Streamlitç‰ˆ)
"""

import streamlit as st
import pandas as pd
import numpy as np

from pathlib import Path
import matplotlib as mpl
import matplotlib.font_manager as fm

def setup_japanese_font():
    # ãƒªãƒã‚¸ãƒˆãƒªã«åŒæ¢±ã™ã‚‹ãƒ•ã‚©ãƒ³ãƒˆï¼ˆæ¨å¥¨ï¼‰
    font_path = Path(__file__).parent / "assets" / "fonts" / "NotoSansCJKjp-Regular.otf"
    if font_path.exists():
        fm.fontManager.addfont(str(font_path))
        prop = fm.FontProperties(fname=str(font_path))
        mpl.rcParams["font.family"] = prop.get_name()
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆCloudã§ã¯åŠ¹ã‹ãªã„ã“ã¨ã‚‚å¤šã„ï¼‰
        mpl.rcParams["font.family"] = ["Noto Sans CJK JP", "IPAexGothic", "IPAGothic", "Yu Gothic", "MS Gothic"]

    mpl.rcParams["axes.unicode_minus"] = False  # ã€Œâˆ’ã€åŒ–ã‘å¯¾ç­–

setup_japanese_font()

# japanize-matplotlib ã¯ä½µç”¨OKï¼ˆã‚ã£ã¦ã‚‚ãªãã¦ã‚‚å‹•ãï¼‰
try:
    import japanize_matplotlib
    japanize_matplotlib.japanize()
except Exception as e:
    st.warning(f"âš ï¸ æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆjapanize-matplotlibï¼‰ã§å•é¡ŒãŒç™ºç”Ÿ: {e}")

import matplotlib.pyplot as plt
import lightgbm as lgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score, log_loss
from sklearn.model_selection import TimeSeriesSplit
import google.generativeai as genai
import shap
import warnings
import math

# --- Streamlit ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="é›†ä¸­ãƒ»ç–²åŠ´äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ", layout="wide")

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š (æ˜ç¤ºçš„ã«é©ç”¨ã™ã‚‹ã‚ˆã†ä¿®æ­£ã—ã€ã‚¨ãƒ©ãƒ¼æ¤œçŸ¥ã‚’è¿½åŠ )
try:
    import japanize_matplotlib
    japanize_matplotlib.japanize()
except ImportError:
    st.warning("âš ï¸ japanize-matplotlib ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã€‚ã‚°ãƒ©ãƒ•ã®æ—¥æœ¬èªãŒæ–‡å­—åŒ–ã‘ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚GitHubä¸Šã§ `requirements.txt` ãŒ `streamlit_app.py` ã¨å…¨ãåŒã˜ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆç¬¬ä¸€éšå±¤ï¼‰ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

warnings.filterwarnings('ignore')

st.title("ğŸ§  ã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ«Ã—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« é›†ä¸­äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ")
st.markdown("""
ã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ«ãƒ‡ãƒã‚¤ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã¨äºˆå®šè¡¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ„ã¿åˆã‚ã›ã¦ã€æ•°æ™‚é–“å¾Œã®é›†ä¸­ã‚¹ã‚³ã‚¢ã‚’äºˆæ¸¬ã—ã€æ¨å¥¨ã™ã‚‹åƒãæ–¹ã‚’ææ¡ˆã—ã¾ã™ã€‚
""")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ (è¨­å®š) ---
st.sidebar.header("âš™ï¸ è¨­å®š")
api_key = st.sidebar.text_input("Gemini APIã‚­ãƒ¼ (çœç•¥æ™‚ã¯å›ºå®šãƒ«ãƒ¼ãƒ«ã§å‡ºåŠ›)", type="password")

st.sidebar.subheader("åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
RESAMPLE_FREQ = st.sidebar.selectbox("åˆ†æå˜ä½", ['10T', '30T', '1H'], index=1)
PREDICT_AHEAD = st.sidebar.selectbox("äºˆæ¸¬å…ˆ", ['10T', '30T', '1H'], index=1)
LOOKBACK_PERIOD = st.sidebar.selectbox("éå»å‚ç…§", ['1H', '2H', '3H'], index=1)
INTERPOLATE_LIMIT = st.sidebar.selectbox("è£œå®Œé™ç•Œ", ['10T', '30T', '1H'], index=1)

st.sidebar.subheader("äºˆæ¸¬åŸºæº–æ—¥æ™‚ (ä»»æ„)")
TARGET_DATETIME_STR = st.sidebar.text_input("ä¾‹ï¼‰2025-12-18 16:00 (ç©ºæ¬„ã§æœ€æ–°ãƒ‡ãƒ¼ã‚¿)")
TARGET_DATETIME = TARGET_DATETIME_STR if TARGET_DATETIME_STR.strip() != "" else None

freq_td = pd.Timedelta(RESAMPLE_FREQ)
ahead_td = pd.Timedelta(PREDICT_AHEAD)
lookback_td = pd.Timedelta(LOOKBACK_PERIOD)
interp_td = pd.Timedelta(INTERPOLATE_LIMIT)

ahead_steps = max(1, int(ahead_td / freq_td))
lookback_steps = max(2, int(lookback_td / freq_td))
interp_steps = max(1, int(interp_td / freq_td))

# --- äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®é¸æŠæ©Ÿèƒ½ã‚’è¿½åŠ  ---
TARGET_OPTIONS = {
    'é›†ä¸­åˆ¤å®š': 'é›†ä¸­åˆ¤å®š(1=é›†ä¸­)',
    'ç–²åŠ´åˆ¤å®š': 'ç–²åŠ´åˆ¤å®š(1=ç–²åŠ´)'
}
st.sidebar.subheader("ğŸ¯ äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ")
selected_target_name = st.sidebar.selectbox("äºˆæ¸¬ã™ã‚‹æŒ‡æ¨™ã‚’é¸æŠ", list(TARGET_OPTIONS.values()), index=0)
target_col = [k for k, v in TARGET_OPTIONS.items() if v == selected_target_name][0]

# --- æ–°è¦: é•·æœŸåˆ†æç”¨ãƒ•ã‚£ãƒ«ã‚¿ ---
st.sidebar.subheader("ğŸ“… é•·æœŸåˆ†æãƒ•ã‚£ãƒ«ã‚¿ (ç‰¹æ€§ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”¨)")
dow_options = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]
selected_dows = st.sidebar.multiselect("å¯¾è±¡æ›œæ—¥", dow_options, default=dow_options)
time_range = st.sidebar.slider("å¯¾è±¡æ™‚é–“å¸¯", 0, 23, (9, 19))

# --- ç‰¹å¾´é‡åæ—¥æœ¬èªåŒ–ãƒ˜ãƒ«ãƒ‘ãƒ¼ ---
def jp_feat_name(col_name: str) -> str:
    mapping = {
        'CVRR_SCORE_NEW': 'é›†ä¸­ã‚¹ã‚³ã‚¢', 'SkinTemp': 'çš®è†šæ¸©åº¦', 'LP_SCORE_NEW': 'ãƒªãƒ©ãƒƒã‚¯ã‚¹ã‚¹ã‚³ã‚¢',
        'LFHF_SCORE_NEW': 'LF/HF(è‡ªå¾‹ç¥çµŒãƒãƒ©ãƒ³ã‚¹)', 'TP': 'TP(è‡ªå¾‹ç¥çµŒãƒˆãƒ¼ã‚¿ãƒ«ãƒ‘ãƒ¯ãƒ¼)', 'NEMUKE_SCORE_NEW': 'ä½è¦šé†’ã‚¹ã‚³ã‚¢',
        'PR_SCORE_NEW': 'è„ˆæ‹', 'RMSSD_SCORE_NEW': 'ç–²åŠ´ãƒ»å›å¾©ã‚¹ã‚³ã‚¢', '1åˆ†é–“æ­©æ•°': 'æ­©æ•°', 'accDeviation': 'æ´»å‹•é‡(åŠ é€Ÿåº¦)',
        'has_schedule': 'äºˆå®šã®æœ‰ç„¡', 'is_meeting': 'ä¼šè­°ä¸­ã‹ã©ã†ã‹', 'schedule_density_2h': 'æœ€è¿‘ã®äºˆå®šã®è©°ã¾ã‚Šå…·åˆ',
        'time_to_next_event_min': 'æ¬¡ã®äºˆå®šã¾ã§ã®æ™‚é–“', 'time_since_prev_event_min': 'å‰ã®äºˆå®šã‹ã‚‰ã®çµŒéæ™‚é–“',
        'daily_schedule_hours': '1æ—¥ã®ç·äºˆå®šæ™‚é–“', 'consecutive_schedules': 'é€£ç¶šäºˆå®šãƒ–ãƒ­ãƒƒã‚¯æ•°',
        'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°', 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“_åˆ†': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“(åˆ†)',
        'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“(åˆ†)', 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“(åˆ†)',
        'é›†ä¸­åˆ¤å®š': 'é›†ä¸­åˆ¤å®š', 'ç–²åŠ´åˆ¤å®š': 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š': 'å¼·ã„ç–²åŠ´åˆ¤å®š',
        'é›†ä¸­çŠ¶æ…‹': 'é›†ä¸­çŠ¶æ…‹', 'çœ æ°—çŠ¶æ…‹': 'çœ æ°—çŠ¶æ…‹', 'ç–²åŠ´çŠ¶æ…‹': 'ç–²åŠ´çŠ¶æ…‹',
        'é›†ä¸­ç¶™ç¶šæ™‚é–“': 'é›†ä¸­ç¶™ç¶šæ™‚é–“', 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“': 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“',
        'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'ç–²åŠ´ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“': 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“'
    }
    
    base_jp = col_name
    remainder = ""
    for k, v in mapping.items():
        if col_name.startswith(k):
            base_jp = v
            remainder = col_name[len(k):]
            break
            
    # ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆ_roll_meanãªã©ï¼‰ã‚’åˆ†ã‹ã‚Šã‚„ã™ã„è¨€è‘‰ã«å¤‰æ›
    if remainder == "":
        return base_jp
    elif remainder == "_roll_mean":
        return f"æœ€è¿‘ã®ã€Œ{base_jp}ã€ã®å¹³å‡çš„ãªé«˜ã•"
    elif remainder == "_roll_slope":
        return f"æœ€è¿‘ã®ã€Œ{base_jp}ã€ã®æ€¥ãªå¤‰åŒ–(ãƒˆãƒ¬ãƒ³ãƒ‰)"
    elif remainder == "_diff1":
        return f"å‰å›ã‹ã‚‰ã®ã€Œ{base_jp}ã€ã®å¤‰å‹•å¹…"
    elif remainder.startswith("_lag"):
        return f"å°‘ã—å‰ã®ã€Œ{base_jp}ã€ã®çŠ¶æ…‹"
    elif remainder == "_is_missing":
        return f"ã€Œ{base_jp}ã€ãŒæœªè¨ˆæ¸¬ã§ã‚ã‚‹ã“ã¨"
    else:
        return f"{base_jp}{remainder}"

# --- ãƒ™ãƒ¼ã‚¹æŒ‡æ¨™åï¼ˆã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãªã—ï¼‰ã‚’å–å¾—ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼ ---
def get_base_feature_name(feat: str) -> str:
    mapping = {
        'CVRR_SCORE_NEW': 'é›†ä¸­ã‚¹ã‚³ã‚¢', 'SkinTemp': 'çš®è†šæ¸©åº¦', 'LP_SCORE_NEW': 'ãƒªãƒ©ãƒƒã‚¯ã‚¹ã‚¹ã‚³ã‚¢',
        'LFHF_SCORE_NEW': 'LF/HF(è‡ªå¾‹ç¥çµŒãƒãƒ©ãƒ³ã‚¹)', 'TP': 'TP(è‡ªå¾‹ç¥çµŒãƒˆãƒ¼ã‚¿ãƒ«ãƒ‘ãƒ¯ãƒ¼)', 'NEMUKE_SCORE_NEW': 'ä½è¦šé†’ã‚¹ã‚³ã‚¢',
        'PR_SCORE_NEW': 'è„ˆæ‹', 'RMSSD_SCORE_NEW': 'ç–²åŠ´ãƒ»å›å¾©ã‚¹ã‚³ã‚¢', '1åˆ†é–“æ­©æ•°': 'æ­©æ•°', 'accDeviation': 'æ´»å‹•é‡(åŠ é€Ÿåº¦)',
        'has_schedule': 'äºˆå®š', 'is_meeting': 'ä¼šè­°', 'schedule_density_2h': 'äºˆå®šã®å¯†åº¦',
        'time_to_next_event_min': 'æ¬¡ã®äºˆå®šã¾ã§ã®æ™‚é–“', 'time_since_prev_event_min': 'å‰ã®äºˆå®šã‹ã‚‰ã®çµŒéæ™‚é–“',
        'daily_schedule_hours': '1æ—¥ã®ç·äºˆå®šæ™‚é–“', 'consecutive_schedules': 'é€£ç¶šäºˆå®šãƒ–ãƒ­ãƒƒã‚¯æ•°',
        'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°', 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“_åˆ†': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“',
        'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“', 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“',
        'é›†ä¸­åˆ¤å®š': 'é›†ä¸­åˆ¤å®š', 'ç–²åŠ´åˆ¤å®š': 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š': 'å¼·ã„ç–²åŠ´åˆ¤å®š',
        'é›†ä¸­çŠ¶æ…‹': 'é›†ä¸­çŠ¶æ…‹', 'çœ æ°—çŠ¶æ…‹': 'çœ æ°—çŠ¶æ…‹', 'ç–²åŠ´çŠ¶æ…‹': 'ç–²åŠ´çŠ¶æ…‹',
        'é›†ä¸­ç¶™ç¶šæ™‚é–“': 'é›†ä¸­ç¶™ç¶šæ™‚é–“', 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“': 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“',
        'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'ç–²åŠ´ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“': 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“'
    }
    for k, v in mapping.items():
        if feat.startswith(k):
            return v
    return feat

# --- ç‰¹å¾´é‡åã¨å€¤ã‹ã‚‰ã€Œå¢—åŠ ã€ã€Œä½ä¸‹ã€ã‚’åŠ å‘³ã—ãŸè¡¨ç¾ã‚’ç”Ÿæˆã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼ ---
def get_factor_direction_text(feat: str, val: float, df_all: pd.DataFrame) -> str:
    mapping = {
        'CVRR_SCORE_NEW': 'é›†ä¸­ã‚¹ã‚³ã‚¢', 'SkinTemp': 'çš®è†šæ¸©åº¦', 'LP_SCORE_NEW': 'ãƒªãƒ©ãƒƒã‚¯ã‚¹ã‚¹ã‚³ã‚¢',
        'LFHF_SCORE_NEW': 'LF/HF(è‡ªå¾‹ç¥çµŒãƒãƒ©ãƒ³ã‚¹)', 'TP': 'TP(è‡ªå¾‹ç¥çµŒãƒˆãƒ¼ã‚¿ãƒ«ãƒ‘ãƒ¯ãƒ¼)', 'NEMUKE_SCORE_NEW': 'ä½è¦šé†’ã‚¹ã‚³ã‚¢',
        'PR_SCORE_NEW': 'è„ˆæ‹', 'RMSSD_SCORE_NEW': 'ç–²åŠ´ãƒ»å›å¾©ã‚¹ã‚³ã‚¢', '1åˆ†é–“æ­©æ•°': 'æ­©æ•°', 'accDeviation': 'æ´»å‹•é‡(åŠ é€Ÿåº¦)',
        'has_schedule': 'äºˆå®š', 'is_meeting': 'ä¼šè­°', 'schedule_density_2h': 'äºˆå®šã®å¯†åº¦',
        'time_to_next_event_min': 'æ¬¡ã®äºˆå®šã¾ã§ã®æ™‚é–“', 'time_since_prev_event_min': 'å‰ã®äºˆå®šã‹ã‚‰ã®çµŒéæ™‚é–“',
        'daily_schedule_hours': '1æ—¥ã®ç·äºˆå®šæ™‚é–“', 'consecutive_schedules': 'é€£ç¶šäºˆå®šãƒ–ãƒ­ãƒƒã‚¯æ•°',
        'é›†ä¸­åˆ¤å®š': 'é›†ä¸­åˆ¤å®š', 'ç–²åŠ´åˆ¤å®š': 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š': 'å¼·ã„ç–²åŠ´åˆ¤å®š',
        'é›†ä¸­çŠ¶æ…‹': 'é›†ä¸­çŠ¶æ…‹', 'çœ æ°—çŠ¶æ…‹': 'çœ æ°—çŠ¶æ…‹', 'ç–²åŠ´çŠ¶æ…‹': 'ç–²åŠ´çŠ¶æ…‹',
        'é›†ä¸­ç¶™ç¶šæ™‚é–“': 'é›†ä¸­ç¶™ç¶šæ™‚é–“', 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“': 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“',
        'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'ç–²åŠ´ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“': 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“'
    }
    
    base_jp = feat
    remainder = ""
    for k, v in mapping.items():
        if feat.startswith(k):
            base_jp = v
            remainder = feat[len(k):]
            break

    # ã‚«ãƒ†ã‚´ãƒªã‚„ãƒ•ãƒ©ã‚°ç³»ã®å‡¦ç†
    if remainder == "_is_missing":
        return f"ã€Œ{base_jp}ã€ãŒæœªè¨ˆæ¸¬ã§ã‚ã‚‹ã“ã¨"
    elif feat in ["has_schedule", "is_meeting"]:
        if val > 0:
            return f"ã€Œ{base_jp}ã€ãŒå…¥ã£ã¦ã„ã‚‹ã“ã¨"
        else:
            return f"ã€Œ{base_jp}ã€ãŒå…¥ã£ã¦ã„ãªã„ã“ã¨"
    elif feat in ["é›†ä¸­çŠ¶æ…‹", "çœ æ°—çŠ¶æ…‹", "ç–²åŠ´çŠ¶æ…‹"]:
        return f"ã€Œ{base_jp}ã€ãŒã€Œ{val}ã€ã§ã‚ã‚‹ã“ã¨"
            
    # å¢—æ¸›ã‚’ã¿ã‚‹ã‚‚ã®ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ã‚„å·®åˆ†ï¼‰
    if remainder in ["_roll_slope", "_diff1"]:
        if val > 0:
            direction = "ã®å¢—åŠ "
        elif val < 0:
            direction = "ã®ä½ä¸‹"
        else:
            direction = "ã®å¤‰åŒ–ãªã—"
            
        if remainder == "_roll_slope":
            return f"æœ€è¿‘ã®ã€Œ{base_jp}ã€ã®ãƒˆãƒ¬ãƒ³ãƒ‰{direction}"
        elif remainder == "_diff1":
            return f"å‰å›ã‹ã‚‰ã®ã€Œ{base_jp}ã€{direction}"
            
    # æŒ‡æ¨™ãã®ã‚‚ã®ã‚’è¦‹ã‚‹å ´åˆï¼ˆå€¤ã®å¤§ãã•è‡ªä½“ãŒå½±éŸ¿ã™ã‚‹ãŸã‚ã€ç„¡ç†ã«ã€Œå¢—åŠ /ä½ä¸‹ã€ã‚’ä»˜ã‘ãªã„ï¼‰
    else:
        if remainder == "_roll_mean":
            return f"æœ€è¿‘ã®ã€Œ{base_jp}ã€"
        elif remainder.startswith("_lag"):
            return f"å°‘ã—å‰ã®ã€Œ{base_jp}ã€"
        else:
            return f"ã€Œ{base_jp}ã€"

# --- åˆ†æãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def run_analysis(df_ts, df_sched, use_gemini=False):
    # 1. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
    with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ãƒ»å‰å‡¦ç†ã—ã¦ã„ã¾ã™..."):
        # CSVã®timestampåˆ—ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æ–‡å­—åˆ—ï¼ˆ GMT+0900 ãªã©ï¼‰ã‚’é™¤å»ã—ã¦æ—¥æ™‚ã«å¤‰æ›
        if 'timestamp' in df_ts.columns:
            df_ts['timestamp_clean'] = df_ts['timestamp'].astype(str).str.split(' GMT').str[0]
            df_ts['datetime'] = pd.to_datetime(df_ts['timestamp_clean'], errors='coerce')
            df_ts = df_ts.dropna(subset=['datetime'])
            df_ts.set_index('datetime', inplace=True)
            df_ts.drop(columns=['timestamp', 'timestamp_clean'], inplace=True, errors='ignore')

        base_agg_dict = {
            'SkinTemp': 'mean', 'CVRR_SCORE_NEW': 'mean', 'LP_SCORE_NEW': 'mean',
            'LFHF_SCORE_NEW': 'mean', 'TP': 'mean', 'NEMUKE_SCORE_NEW': 'mean',
            'PR_SCORE_NEW': 'mean', 'RMSSD_SCORE_NEW': 'mean', '1åˆ†é–“æ­©æ•°': 'sum', 'accDeviation': 'mean',
            'é›†ä¸­åˆ¤å®š': 'mean', 'ç–²åŠ´åˆ¤å®š': 'mean', 'å¼·ã„ç–²åŠ´åˆ¤å®š': 'mean',
            'é›†ä¸­ç¶™ç¶šæ™‚é–“': 'mean', 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“': 'mean', 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“': 'mean', 'ç–²åŠ´ç¶™ç¶šæ™‚é–“': 'mean', 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“': 'mean'
        }
        
        cat_agg_dict = {
            'é›†ä¸­çŠ¶æ…‹': 'last',
            'çœ æ°—çŠ¶æ…‹': 'last',
            'ç–²åŠ´çŠ¶æ…‹': 'last'
        }
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã‚’æŠ½å‡ºã—ã¦é›†ç´„å¯¾è±¡ã«ã™ã‚‹ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        agg_dict = {col: func for col, func in base_agg_dict.items() if col in df_ts.columns}
        for col, func in cat_agg_dict.items():
            if col in df_ts.columns:
                agg_dict[col] = func
        
        # äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’å‡ºã—ã¦çµ‚äº†
        if target_col not in agg_dict:
            st.error(f"ã‚¨ãƒ©ãƒ¼: äºˆæ¸¬ã«å¿…è¦ãªç›®çš„å¤‰æ•°ã€Œ{selected_target_name}ï¼ˆåˆ—å: {target_col}ï¼‰ã€ãŒãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return

        # é›†ç´„å‰ã«ç¢ºå®Ÿã«æ•°å€¤å‹ã«å¤‰æ›ã—ã¦ãŠã (æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®ã¿)
        for col in agg_dict.keys():
            if col in base_agg_dict.keys():
                df_ts[col] = pd.to_numeric(df_ts[col], errors='coerce')

        df_resampled = df_ts.resample(RESAMPLE_FREQ).agg(agg_dict)

        if df_sched is not None:
            # äºˆå®šè¡¨ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
            df_sched = df_sched[df_sched['çµ‚æ—¥ã‚¤ãƒ™ãƒ³ãƒˆ'].astype(str).str.upper() != 'TRUE']
            df_sched['start_dt'] = pd.to_datetime(df_sched['é–‹å§‹æ—¥'].astype(str) + ' ' + df_sched['é–‹å§‹æ™‚åˆ»'].astype(str), errors='coerce')
            df_sched['end_dt']   = pd.to_datetime(df_sched['çµ‚äº†æ—¥'].astype(str) + ' ' + df_sched['çµ‚äº†æ™‚åˆ»'].astype(str), errors='coerce')
            df_sched = df_sched.dropna(subset=['start_dt', 'end_dt']).sort_values('start_dt')

            df_resampled['has_schedule'] = 0
            df_resampled['is_meeting'] = 0
            meeting_keywords = ['ä¼šè­°', 'æ‰“åˆã›', 'æ‰“ã¡åˆã‚ã›', 'MTG', 'é¢è«‡', 'å•†è«‡', 'æ¥å®¢', 'è¨ªå•']

            for _, row in df_sched.iterrows():
                mask = (df_resampled.index < row['end_dt']) & ((df_resampled.index + freq_td) > row['start_dt'])
                df_resampled.loc[mask, 'has_schedule'] = 1
                subject = str(row.get('ä»¶å', ''))
                if any(kw in subject for kw in meeting_keywords):
                    df_resampled.loc[mask, 'is_meeting'] = 1

            s = df_resampled['has_schedule']
            df_resampled['consecutive_schedules'] = s.groupby((s != s.shift()).cumsum()).cumsum()

            # æ—¥æ¬¡ç‰¹å¾´é‡
            df_resampled['date'] = df_resampled.index.date
            df_resampled = df_resampled.join(df_resampled.groupby('date')['has_schedule'].sum().rename('daily_schedule_hours'), on='date').fillna({'daily_schedule_hours': 0})
            df_resampled.drop(columns=['date'], inplace=True)

            # æ¬¡ãƒ»å‰ã®äºˆå®šã¾ã§ã®æ™‚é–“ (Safe Lookup)
            event_starts = df_sched['start_dt'].to_numpy(dtype='datetime64[ns]')
            event_ends   = df_sched['end_dt'].to_numpy(dtype='datetime64[ns]')
            t = df_resampled.index.to_numpy(dtype='datetime64[ns]')

            next_start_idx = np.searchsorted(event_starts, t, side='left')
            has_next = next_start_idx < len(event_starts)
            next_idx_safe = np.clip(next_start_idx, 0, max(len(event_starts) - 1, 0))
            next_start = np.full(t.shape, np.datetime64('NaT'), dtype='datetime64[ns]')
            if len(event_starts) > 0: next_start[has_next] = event_starts[next_idx_safe[has_next]]

            prev_end_idx = np.searchsorted(event_ends, t, side='right') - 1
            has_prev = prev_end_idx >= 0
            prev_idx_safe = np.clip(prev_end_idx, 0, max(len(event_ends) - 1, 0))
            prev_end = np.full(t.shape, np.datetime64('NaT'), dtype='datetime64[ns]')
            if len(event_ends) > 0: prev_end[has_prev] = event_ends[prev_idx_safe[has_prev]]

            df_resampled['time_to_next_event_min'] = (next_start - t) / np.timedelta64(1, 'm')
            df_resampled['time_since_prev_event_min'] = (t - prev_end) / np.timedelta64(1, 'm')

            win_steps = max(1, int(pd.Timedelta('2H') / freq_td))
            df_resampled['schedule_density_2h'] = df_resampled['has_schedule'].rolling(win_steps, min_periods=1).mean()

    # 2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    with st.spinner("ç‰¹å¾´é‡ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™..."):
        df_features = df_resampled.copy()
        df_features['hour'] = df_features.index.hour.astype('category')
        df_features['dayofweek'] = df_features.index.dayofweek.astype('category')
        
        for c in ['é›†ä¸­çŠ¶æ…‹', 'çœ æ°—çŠ¶æ…‹', 'ç–²åŠ´çŠ¶æ…‹']:
            if c in df_features.columns:
                df_features[c] = df_features[c].astype('category')
        
        numeric_cols = df_resampled.select_dtypes(include=[np.number]).columns.tolist()
        
        win = lookback_steps
        x = np.arange(win, dtype=float)
        x_mean = x.mean()
        x_var = ((x - x_mean) ** 2).sum()
        def rolling_slope(arr):
            y = arr.astype(float)
            if x_var == 0: return 0.0
            return ((x - x_mean) * (y - y.mean())).sum() / x_var

        for col in numeric_cols:
            df_features[f'{col}_is_missing'] = df_resampled[col].isna().astype(int)
            r = df_features[col].rolling(win, min_periods=win)
            df_features[f'{col}_roll_mean'] = r.mean()
            df_features[f'{col}_roll_slope'] = r.apply(rolling_slope, raw=True)
            df_features[f'{col}_diff1'] = df_features[col] - df_features[col].shift(1)

        # -- Step 2: è¿½åŠ ç‰¹å¾´é‡ (ç´¯ç©è² è·, ç¶™ç¶šæ™‚é–“) ã®è¨ˆç®— --
        df_features['date'] = df_features.index.date
        if '1åˆ†é–“æ­©æ•°' in df_features.columns:
            df_features['ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°'] = df_features.groupby('date')['1åˆ†é–“æ­©æ•°'].cumsum()
        if 'is_meeting' in df_features.columns:
            df_features['ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“_åˆ†'] = df_features.groupby('date')['is_meeting'].cumsum() * (freq_td.total_seconds() / 60)
        
        def calc_duration_mins(series):
            group_id = (series != series.shift()).cumsum()
            return (series.groupby(group_id).cumcount() + 1) * (freq_td.total_seconds() / 60)
            
        if 'é›†ä¸­åˆ¤å®š' in df_features.columns:
            focus_mask = (df_features['é›†ä¸­åˆ¤å®š'] >= 0.5).astype(int)
            df_features['ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“_åˆ†'] = calc_duration_mins(focus_mask) * focus_mask
            
        if 'ç–²åŠ´åˆ¤å®š' in df_features.columns:
            fatigue_mask = (df_features['ç–²åŠ´åˆ¤å®š'] >= 0.5).astype(int)
            df_features['ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“_åˆ†'] = calc_duration_mins(fatigue_mask) * fatigue_mask
            
        df_features.drop(columns=['date'], inplace=True)

        # Step 1: åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã¸ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®ç”Ÿæˆ
        target_threshold = 0.5 if target_col in ['é›†ä¸­åˆ¤å®š', 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š'] else df_features[target_col].median()
        # äºˆæ¸¬å…ˆã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã§ã®å¹³å‡å€¤ãŒé–¾å€¤ä»¥ä¸Šã‹ã©ã†ã‹ã‚’åˆ†é¡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã™ã‚‹ï¼ˆæœªæ¥ã®çµ¶å¯¾çŠ¶æ…‹ï¼‰
        df_features['target_ahead_class'] = (df_features[target_col].shift(-ahead_steps) >= target_threshold).astype(int)

    # 3. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    with st.spinner("LightGBMåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¦ã„ã¾ã™..."):
        drop_cols = ['target_ahead_class']
        df_all = df_features.copy()
        split_idx = int(len(df_all) * 0.8)
        
        # ç°¡æ˜“æ¬ æè£œå®Œ (ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚‚å«ã‚ã¦è£œå®Œã™ã‚‹ã‚ˆã†ä¿®æ­£)
        df_imp = df_all.copy()
        for col in df_imp.columns:
            if col not in drop_cols:
                df_imp[col] = df_imp[col].ffill(limit=interp_steps).bfill(limit=interp_steps)
        
        train_df = df_imp.iloc[:split_idx].dropna(subset=drop_cols + [target_col])
        test_df  = df_imp.iloc[split_idx:].dropna(subset=drop_cols + [target_col])

        X_train = train_df.drop(columns=drop_cols)
        y_train_class = train_df['target_ahead_class']
        X_test  = test_df.drop(columns=drop_cols)
        y_test_class = test_df['target_ahead_class']
        
        cat_cols = [c for c in X_train.columns if str(X_train[c].dtype) == 'category']
        
        model = lgb.LGBMClassifier(
            objective='binary', n_estimators=500, learning_rate=0.03, random_state=42
        )
        # ç°¡æ˜“çš„ã«å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
        model.fit(X_train, y_train_class, categorical_feature=cat_cols if cat_cols else 'auto')
        
        preds_proba = model.predict_proba(X_test)[:, 1]
        
        try:
            auc_test = roc_auc_score(y_test_class, preds_proba)
            logloss_test = log_loss(y_test_class, preds_proba)
        except ValueError:
            auc_test = np.nan
            logloss_test = np.nan

    # === ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ç‰¹æ€§ã‚¤ãƒ³ã‚µã‚¤ãƒˆ (é•·æœŸåˆ†æ) ã‚’å…ˆã«è¡¨ç¤º ===
    st.header("ğŸ‘¤ ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ç‰¹æ€§ã‚¤ãƒ³ã‚µã‚¤ãƒˆ (é•·æœŸåˆ†æ)")
    
    # é•·æœŸåˆ†æç”¨ãƒ•ã‚£ãƒ«ã‚¿ã®é©ç”¨
    df_insight = df_imp.copy()
    selected_dow_indices = [dow_options.index(d) for d in selected_dows]
    df_insight = df_insight[df_insight.index.dayofweek.isin(selected_dow_indices)]
    df_insight = df_insight[(df_insight.index.hour >= time_range[0]) & (df_insight.index.hour <= time_range[1])]

    # çŠ¶æ…‹é–‹å§‹ã®ãƒ•ãƒ©ã‚°ä½œæˆ (ã‚¤ãƒ³ã‚µã‚¤ãƒˆå…¨ä½“ã§åˆ©ç”¨)
    if 'é›†ä¸­åˆ¤å®š' in df_insight.columns:
        df_insight['focus_start'] = (df_insight['é›†ä¸­åˆ¤å®š'] >= 0.5) & (df_insight['é›†ä¸­åˆ¤å®š'].shift(1) < 0.5)
    if 'ç–²åŠ´åˆ¤å®š' in df_insight.columns:
        df_insight['fatigue_start'] = (df_insight['ç–²åŠ´åˆ¤å®š'] >= 0.5) & (df_insight['ç–²åŠ´åˆ¤å®š'].shift(1) < 0.5)

    # --- ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç®—å‡º ---
    def get_peak_time(metric_col):
        if metric_col not in df_insight.columns:
            return None, None
        pivot_df = df_insight.pivot_table(
            values=metric_col, 
            index=df_insight.index.hour, 
            columns=df_insight.index.dayofweek, 
            aggfunc='mean'
        )
        start_hour_hm = time_range[0]
        end_hour_hm = time_range[1]
        
        # é¸æŠã•ã‚ŒãŸæ™‚é–“å¸¯ãƒ»æ›œæ—¥ã®ã¿æŠ½å‡º
        daytime_pivot = pivot_df.loc[start_hour_hm:end_hour_hm, selected_dow_indices]
        if not daytime_pivot.isna().all().all():
            best_hour, best_dow = daytime_pivot.stack().idxmax()
            return int(best_hour), dow_options[int(best_dow)]
        return None, None

    f_hour, f_dow = get_peak_time('é›†ä¸­åˆ¤å®š')
    fat_hour, fat_dow = get_peak_time('ç–²åŠ´åˆ¤å®š')

    avg_focus_duration_str = "ç®—å‡ºã§ãã¾ã›ã‚“"
    daily_focus_count_str = "ç®—å‡ºã§ãã¾ã›ã‚“"
    focus_durations = pd.Series(dtype=float)
    
    if 'é›†ä¸­åˆ¤å®š' in df_ts.columns:
        # ç¶™ç¶šæ™‚é–“ã‚’ã‚ˆã‚Šæ­£ç¢ºã«è¨ˆç®—ã™ã‚‹ãŸã‚ã€å…ƒãƒ‡ãƒ¼ã‚¿ã‚’1åˆ†å˜ä½ã§ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦è¨ˆç®—
        df_1min = df_ts[['é›†ä¸­åˆ¤å®š']].resample('1T').mean()
        df_1min = df_1min.ffill(limit=5)
        df_1min = df_1min[df_1min.index.dayofweek.isin(selected_dow_indices)]
        df_1min = df_1min[(df_1min.index.hour >= time_range[0]) & (df_1min.index.hour <= time_range[1])]
        
        focus_mask = df_1min['é›†ä¸­åˆ¤å®š'] >= 0.5
        focus_blocks = focus_mask.groupby((focus_mask != focus_mask.shift()).cumsum())
        focus_durations = focus_blocks.sum() 
        focus_durations = focus_durations[focus_durations > 0]
        
        if not focus_durations.empty:
            avg_focus_duration_str = f"{focus_durations.mean():.0f}"
            total_focus_count = len(focus_durations)
            num_days = df_1min.index.normalize().nunique()
            daily_focus_count = total_focus_count / num_days if num_days > 0 else 0
            daily_focus_count_str = f"{daily_focus_count:.1f}"

    focus_actions = []
    if '1åˆ†é–“æ­©æ•°' in df_insight.columns and 'focus_start' in df_insight.columns:
        walk_before_focus = df_insight['1åˆ†é–“æ­©æ•°'].shift(1)[df_insight['focus_start']].dropna()
        avg_walk_overall = df_insight['1åˆ†é–“æ­©æ•°'].mean()
        if not walk_before_focus.empty and avg_walk_overall > 0:
            avg_walk_before = walk_before_focus.mean()
            if avg_walk_before > avg_walk_overall * 1.2:
                focus_actions.append("äº‹å‰ã«ä½“ã‚’å‹•ã‹ã™ã“ã¨ï¼ˆå°‘ã—æ­©ããªã©ï¼‰")
            elif avg_walk_before < avg_walk_overall * 0.8:
                focus_actions.append("äº‹å‰ã«é™ã‹ãªç’°å¢ƒã§è½ã¡ç€ã„ã¦éã”ã™ã“ã¨")

    if 'has_schedule' in df_insight.columns and 'é›†ä¸­åˆ¤å®š' in df_insight.columns:
        sched_mask = df_insight['has_schedule'] >= 0.5
        sched_blocks = (sched_mask != sched_mask.shift()).cumsum()
        sched_df = df_insight[sched_mask]
        
        focus_scores_rested = []
        focus_scores_rushed = []
        for _, group in sched_df.groupby(sched_blocks):
            if len(group) > 1 and 'time_since_prev_event_min' in group.columns:
                rest_before = group['time_since_prev_event_min'].iloc[0]
                avg_focus = group['é›†ä¸­åˆ¤å®š'].mean()
                if not np.isnan(rest_before):
                    if rest_before >= 30:
                        focus_scores_rested.append(avg_focus)
                    else:
                        focus_scores_rushed.append(avg_focus)
        
        if len(focus_scores_rested) > 0 and len(focus_scores_rushed) > 0:
            diff_focus = (np.mean(focus_scores_rested) - np.mean(focus_scores_rushed)) * 100
            if diff_focus > 0:
                focus_actions.append("äºˆå®šã®å‰ã«30åˆ†ä»¥ä¸Šã®ç©ºãæ™‚é–“ï¼ˆä¼‘æ†©ï¼‰ã‚’ã¨ã‚‹ã“ã¨")
            elif diff_focus < 0:
                focus_actions.append("äºˆå®šã¨äºˆå®šã®é–“ã‚’ç©ºã‘ãšã«é€£ç¶šã—ã¦æ´»å‹•ã™ã‚‹ã“ã¨")

    focus_actions_str = "ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ç‰¹å®šã§ãã¾ã›ã‚“" if not focus_actions else "ã€".join(focus_actions)

    fatigue_actions = []
    if 'ç–²åŠ´åˆ¤å®š' in df_insight.columns and 'has_schedule' in df_insight.columns:
        sched_mask = df_insight['has_schedule'] >= 0.5
        sched_blocks = (sched_mask != sched_mask.shift()).cumsum()
        sched_df = df_insight[sched_mask]
        
        fatigue_diffs = []
        for _, group in sched_df.groupby(sched_blocks):
            if len(group) > 1:
                fatigue_start_val = group['ç–²åŠ´åˆ¤å®š'].iloc[0]
                fatigue_end_val = group['ç–²åŠ´åˆ¤å®š'].iloc[-1]
                duration_hours = len(group) * (freq_td.total_seconds() / 3600)
                if duration_hours > 0:
                    fatigue_diffs.append((fatigue_end_val - fatigue_start_val) / duration_hours)

        if len(fatigue_diffs) > 0:
            avg_fatigue_diff = np.mean(fatigue_diffs) * 100
            if avg_fatigue_diff > 0:
                fatigue_actions.append("1æ™‚é–“ä»¥ä¸Šã®äºˆå®šã‚’ã“ãªã™ã“ã¨")

    if 'fatigue_start' in df_insight.columns and 'focus_start' in df_insight.columns:
        recovery_consecutive = [] 
        recovery_single = []      
        fatigue_times = df_insight[df_insight['fatigue_start']].index
        focus_times = df_insight[df_insight['focus_start']].index
        for fat_time in fatigue_times:
            future_focus = focus_times[focus_times > fat_time]
            if len(future_focus) > 0:
                first_focus = future_focus[0]
                if first_focus.date() == fat_time.date():
                    rec_time = (first_focus - fat_time).total_seconds() / 60
                    if 'consecutive_schedules' in df_insight.columns:
                        cons_sched = df_insight.loc[fat_time, 'consecutive_schedules']
                        if cons_sched >= 2:
                            recovery_consecutive.append(rec_time)
                        else:
                            recovery_single.append(rec_time)
                            
        if len(recovery_consecutive) > 0 and len(recovery_single) > 0:
            delay = np.mean(recovery_consecutive) - np.mean(recovery_single)
            if delay > 0:
                fatigue_actions.append("äºˆå®šã‚’é€£ç¶šã—ã¦å…¥ã‚Œã‚‹ã“ã¨")

    fatigue_actions_str = "ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ç‰¹å®šã§ãã¾ã›ã‚“" if not fatigue_actions else "ã€".join(fatigue_actions)

    # --- UI è¡¨ç¤º ---
    st.markdown("### ğŸ¯ ã‚ãªãŸã®é›†ä¸­ç‰¹æ€§")
    if f_dow and f_hour is not None:
        st.write(f"- **{f_dow}æ›œæ—¥**ã® **{f_hour}æ™‚å°** ã«æœ€ã‚‚é›†ä¸­ã—ã‚„ã™ã„å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚")
    else:
        st.write("- é›†ä¸­ã—ã‚„ã™ã„æ™‚é–“å¸¯ã¯ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«ã‚ˆã‚Šç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.write(f"- å¹³å‡é›†ä¸­æŒç¶šæ™‚é–“ã¯ **{avg_focus_duration_str}** åˆ†ã§ã™ã€‚")
    st.write(f"- 1æ—¥ã« **{daily_focus_count_str}** å›é›†ä¸­ã¨ç·©å’Œã®ãƒªã‚ºãƒ ã‚’ç¹°ã‚Šè¿”ã—ã¦ã„ã¾ã™ã€‚")
    st.write(f"- é›†ä¸­ã«å…¥ã‚Šã‚„ã™ã„è¡Œå‹•ã¯ **{focus_actions_str}** ã§ã™ã€‚")

    st.markdown("### ğŸ”‹ ã‚ãªãŸã®ç–²åŠ´ç‰¹æ€§")
    if fat_dow and fat_hour is not None:
        st.write(f"- **{fat_dow}æ›œæ—¥**ã® **{fat_hour}æ™‚å°** ã«æœ€ã‚‚ç–²åŠ´ã—ã‚„ã™ã„å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚")
    else:
        st.write("- ç–²åŠ´ã—ã‚„ã™ã„æ™‚é–“å¸¯ã¯ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«ã‚ˆã‚Šç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.write(f"- ç–²åŠ´ã—ã‚„ã™ã„è¡Œå‹•ã¯ **{fatigue_actions_str}** ã§ã™ã€‚")

    # åˆ†å¸ƒã‚°ãƒ©ãƒ•ã®è¡¨ç¤º
    if not focus_durations.empty:
        st.markdown("<br>", unsafe_allow_html=True)
        st.markdown("#### é›†ä¸­æŒç¶šæ™‚é–“ã®åˆ†å¸ƒ")
        fig_dist, ax_dist = plt.subplots(figsize=(8, 3))
        max_duration = int(focus_durations.max())
        max_bin = math.ceil(max_duration / 10) * 10
        bins = np.arange(0, max_bin + 20, 10) 
        
        counts, edges, patches = ax_dist.hist(focus_durations, bins=bins, color='#4A90E2', edgecolor='white', alpha=0.8)
        
        bin_centers = edges[:-1] + 5
        xtick_labels = [f"{int(edges[i])}-{int(edges[i+1])-1}" for i in range(len(edges)-1)]
        
        ax_dist.set_xticks(bin_centers)
        ax_dist.set_xticklabels(xtick_labels, rotation=45, ha='right', fontsize=9)
        
        ax_dist.set_xlabel("é›†ä¸­æŒç¶šæ™‚é–“ (åˆ†)")
        ax_dist.set_ylabel("å›æ•°")
        ax_dist.set_title("é›†ä¸­æŒç¶šæ™‚é–“ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  (10åˆ†åˆ»ã¿)")
        ax_dist.spines['top'].set_visible(False)
        ax_dist.spines['right'].set_visible(False)
        fig_dist.tight_layout()
        st.pyplot(fig_dist)

    # --- ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®è¡¨ç¤º ---
    st.markdown("<br>", unsafe_allow_html=True)
    st.markdown("### ğŸ“Š æ™‚é–“å¸¯ãƒ»æ›œæ—¥åˆ¥ã®å‚¾å‘ (ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—)")
    
    def plot_heatmap(target_metric, title_prefix, cmap_name):
        if target_metric not in df_imp.columns:
            return None
            
        pivot_df = df_imp.pivot_table(
            values=target_metric, 
            index=df_imp.index.hour, 
            columns=df_imp.index.dayofweek, 
            aggfunc='mean'
        )
        
        start_hour_hm = time_range[0]
        end_hour_hm = time_range[1]
        num_hours_hm = end_hour_hm - start_hour_hm + 1
        
        fig_hm, ax_hm = plt.subplots(figsize=(6, 4))
        heatmap_data = np.full((num_hours_hm, 7), np.nan)
        
        for h in pivot_df.index:
            if start_hour_hm <= h <= end_hour_hm:
                for d in pivot_df.columns:
                    if d in selected_dow_indices:
                        heatmap_data[int(h) - start_hour_hm, int(d)] = pivot_df.loc[h, d]
        
        im = ax_hm.imshow(heatmap_data, cmap=cmap_name, aspect='auto')
        
        ax_hm.set_xticks(np.arange(7))
        ax_hm.set_xticklabels(dow_options)
        
        yticks = []
        yticklabels = []
        for i, h in enumerate(range(start_hour_hm, end_hour_hm + 1)):
            yticks.append(i)
            yticklabels.append(str(h))
            
        ax_hm.set_yticks(yticks)
        ax_hm.set_yticklabels(yticklabels)
        
        ax_hm.set_xlabel("æ›œæ—¥")
        ax_hm.set_ylabel("æ™‚é–“å¸¯ (æ™‚)")
        ax_hm.set_title(f"{title_prefix} ({start_hour_hm}æ™‚ã€œ{end_hour_hm}æ™‚)")
        
        cbar = plt.colorbar(im, ax=ax_hm)
        cbar.set_label("ç¢ºç‡")
        
        return fig_hm

    col_h1, col_h2 = st.columns(2)
    with col_h1:
        st.markdown("#### ğŸ¯ é›†ä¸­ã—ã‚„ã™ã„æ™‚é–“å¸¯")
        fig_focus = plot_heatmap('é›†ä¸­åˆ¤å®š', "æ›œæ—¥ãƒ»æ™‚é–“å¸¯åˆ¥ã®é›†ä¸­ç¢ºç‡", 'Blues')
        if fig_focus:
            st.pyplot(fig_focus)
        else:
            st.write("ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")
            
    with col_h2:
        st.markdown("#### ğŸ”‹ ç–²åŠ´ã—ã‚„ã™ã„æ™‚é–“å¸¯")
        fig_fatigue = plot_heatmap('ç–²åŠ´åˆ¤å®š', "æ›œæ—¥ãƒ»æ™‚é–“å¸¯åˆ¥ã®ç–²åŠ´ç¢ºç‡", 'Reds')
        if fig_fatigue:
            st.pyplot(fig_fatigue)
        else:
            st.write("ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")

    # === ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã‚’å¾Œã‚ã«ç§»å‹• ===
    st.header("âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ (Real-time Focus)")
    
    # --- ç²¾åº¦è©•ä¾¡ã®è‡ªå‹•åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ ---
    auc_eval = "ç®—å‡ºä¸å¯"
    if not np.isnan(auc_test):
        if auc_test >= 0.8: auc_eval = "ğŸŸ¢ éå¸¸ã«è‰¯ã„"
        elif auc_test >= 0.7: auc_eval = "ğŸ”µ è‰¯ã„ (å®Ÿç”¨ãƒ¬ãƒ™ãƒ«)"
        elif auc_test >= 0.6: auc_eval = "ğŸŸ¡ æ™®é€š"
        else: auc_eval = "ğŸ”´ æ”¹å–„ãŒå¿…è¦"

    loss_eval = "ç®—å‡ºä¸å¯"
    if not np.isnan(logloss_test):
        if logloss_test <= 0.4: loss_eval = "ğŸŸ¢ éå¸¸ã«è‰¯ã„"
        elif logloss_test <= 0.6: loss_eval = "ğŸŸ¡ æ™®é€š"
        else: loss_eval = "ğŸ”´ æ”¹å–„ãŒå¿…è¦"

    # æŒ‡æ¨™ã‚’ä¸¦ã¹ã¦è¦‹ã‚„ã™ãè¡¨ç¤º
    col_m1, col_m2 = st.columns(2)
    col_m1.info(f"**ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ (AUC-ROC)**: {auc_test:.3f} ğŸ‘‰ **{auc_eval}**\n\n*1.0ã«è¿‘ã„ã»ã©çŠ¶æ…‹ã®åˆ¤åˆ¥ãŒæ­£ç¢ºã«ã§ãã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ï¼ˆ0.7ä»¥ä¸ŠãŒå®Ÿç”¨ã®ç›®å®‰ï¼‰ã€‚*")
    col_m2.info(f"**äºˆæ¸¬ã®ç¢ºä¿¡åº¦ (Log Loss)**: {logloss_test:.3f} ğŸ‘‰ **{loss_eval}**\n\n*0.0ã«è¿‘ã„ã»ã©AIãŒã€Œè¿·ã„ãªãã€æ­£è§£ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ï¼ˆ0.6ä»¥ä¸‹ãŒç›®å®‰ï¼‰ã€‚*")
    
    with st.expander("ğŸ“Š ãƒ†ã‚¹ãƒˆæœŸé–“ã®äºˆæ¸¬ç¢ºç‡æ¨ç§»ã‚’è¡¨ç¤º"):
        fig, ax = plt.subplots(figsize=(10, 4))
        ax.plot(test_df.index, y_test_class, label='å®Ÿéš›ã®çŠ¶æ…‹ (1=Yes, 0=No)', color='blue', alpha=0.6, marker='o', linestyle='None')
        ax.plot(test_df.index, preds_proba, label='LightGBM äºˆæ¸¬ç¢ºç‡', color='red', linestyle='-', alpha=0.8)
        ax.set_title(f"ãƒ†ã‚¹ãƒˆæœŸé–“ã® {selected_target_name} äºˆæ¸¬ç¢ºç‡ã®æ¨ç§»")
        ax.set_ylabel("ç¢ºç‡ / çŠ¶æ…‹")
        ax.legend()
        st.pyplot(fig)
        
        # ã‚ºãƒ¼ãƒ ã‚°ãƒ©ãƒ•ã®è¿½åŠ 
        if TARGET_DATETIME is not None:
            try:
                plot_date = pd.to_datetime(TARGET_DATETIME).date()
            except:
                plot_date = test_df.index[-1].date()
        else:
            plot_date = test_df.index[-1].date()
        
        if plot_date not in test_df.index.date:
            plot_date = test_df.index[-1].date()

        target_indices = test_df[test_df.index.date == plot_date].index
        if len(target_indices) > 0:
            fig_zoom, ax_zoom = plt.subplots(figsize=(10, 4))
            ax_zoom.plot(target_indices, y_test_class.loc[target_indices], label='å®Ÿéš›ã®çŠ¶æ…‹ (1=Yes, 0=No)', color='blue', marker='o', linestyle='None', alpha=0.6)
            
            preds_series = pd.Series(preds_proba, index=test_df.index)
            ax_zoom.plot(target_indices, preds_series.loc[target_indices], label='LightGBM äºˆæ¸¬ç¢ºç‡', color='red', linestyle='-', marker='x', alpha=0.8)
            
            ax_zoom.set_title(f"äºˆæ¸¬ã‚ºãƒ¼ãƒ ï¼ˆ{plot_date}ï¼‰")
            ax_zoom.set_ylabel("ç¢ºç‡ / çŠ¶æ…‹")
            ax_zoom.legend()
            st.pyplot(fig_zoom)

    # 4. ç›´è¿‘ã®äºˆæ¸¬ã¨SHAP
    st.subheader("ğŸ”® ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã¨è¦å› åˆ†æ")
    
    available_data_all = df_imp.drop(columns=drop_cols, errors='ignore')
    
    # --- åŸºæº–æ—¥æ™‚ã®é©ç”¨ ---
    if TARGET_DATETIME is not None:
        try:
            target_dt = pd.to_datetime(TARGET_DATETIME)
            available_data = available_data_all[available_data_all.index <= target_dt]
            if len(available_data) == 0:
                st.warning("æŒ‡å®šã•ã‚ŒãŸåŸºæº–æ—¥æ™‚ä»¥å‰ã®ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                available_data = available_data_all
        except Exception as e:
            st.warning(f"æ—¥æ™‚ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆ{e}ï¼‰ã€‚æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    else:
        available_data = available_data_all

    # æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®æœ€æ–°è¡Œã‚’å–å¾—
    target_data = available_data.iloc[-1:]
    current_time = target_data.index[0]
    current_val = float(target_data[target_col].values[0])
    current_state_bool = current_val >= target_threshold
    
    current_proba = model.predict_proba(target_data)[0, 1]
    predicted_state_bool = current_proba >= 0.5
    
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("åŸºæº–æ—¥æ™‚", current_time.strftime('%Y-%m-%d %H:%M'))
    col2.metric(f"ç¾åœ¨ã® {selected_target_name} çŠ¶æ…‹", "Yes" if current_state_bool else "No")
    col3.metric(f"{PREDICT_AHEAD}å¾Œã®äºˆæ¸¬åˆ¤å®š", "Yes" if predicted_state_bool else "No")
    col4.metric(f"ç™ºç”Ÿç¢ºç‡", f"{current_proba * 100:.1f} %")
    
    st.caption(f"â€» **äºˆæ¸¬åˆ¤å®šã¨ç™ºç”Ÿç¢ºç‡ã«ã¤ã„ã¦**: {PREDICT_AHEAD}å¾Œã«ã‚ãªãŸãŒã€Œ{selected_target_name}ã€ã®çŠ¶æ…‹ã«ãªã£ã¦ã„ã‚‹ç¢ºç‡ã‚’AIãŒç®—å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚50%ä»¥ä¸Šã‚’ã€ŒYesã€ã¨åˆ¤å®šã—ã¦ã„ã¾ã™ã€‚")

    with st.spinner("SHAPã§è¦å› ã‚’åˆ†æã—ã¦ã„ã¾ã™..."):
        explainer = shap.TreeExplainer(model)
        shap_values_latest = explainer(target_data)
        
        # äºŒå€¤åˆ†é¡ã®å ´åˆã®SHAPå€¤ã®å–ã‚Šå‡ºã—
        if len(shap_values_latest.shape) == 3:
            shap_vals = shap_values_latest[0, :, 1].values
            shap_base_obj = shap_values_latest[0, :, 1]
        else:
            shap_vals = shap_values_latest[0].values
            shap_base_obj = shap_values_latest[0]
        
        # ä»‹å…¥å¯èƒ½è¦å› ã®æŠ½å‡º
        def is_actionable(col: str) -> bool:
            # äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè‡ªèº«ã«é–¢ã™ã‚‹ç‰¹å¾´é‡ã€ãŠã‚ˆã³æ™‚åˆ»ãƒ»æ›œæ—¥ãªã©ã®ç›´æ¥åˆ¶å¾¡ã§ããªã„è¦å› ã‚’é™¤å¤–
            return not (target_col in col or col in ["hour", "dayofweek"])
            
        exp_df = pd.DataFrame({
            'Feature': target_data.columns,
            'Value': target_data.values[0],
            'SHAP': shap_vals
        })
        exp_df['AbsSHAP'] = exp_df['SHAP'].abs()
        exp_df_action = exp_df[exp_df['Feature'].apply(is_actionable)].sort_values('AbsSHAP', ascending=False)
        
        fig2, ax2 = plt.subplots(figsize=(8, 4))
        shap.plots.waterfall(shap_base_obj, show=False)
        st.pyplot(fig2)

        # è¦å› åˆ†æã‚°ãƒ©ãƒ•ã¸ã®ã‚³ãƒ¡ãƒ³ãƒˆè¿½åŠ 
        st.markdown("**ã€è¦å› åˆ†æã®è§£èª¬ã€‘**")
        
        pos_factors = exp_df_action[exp_df_action['SHAP'] > 0]
        neg_factors = exp_df_action[exp_df_action['SHAP'] < 0]
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæŒ‡æ¨™ã«ã‚ˆã£ã¦ã€Œç¢ºç‡ä¸Šæ˜‡ã€ãŒè‰¯ã„ã‹æ‚ªã„ã‹åˆ†å²ã•ã›ã‚‹
        if target_col in ['NEMUKE_SCORE_NEW', 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š']:
            pos_effect_text = "ç¢ºç‡ä¸Šæ˜‡ï¼ˆæ‚ªåŒ–æ–¹å‘ï¼‰"
            neg_effect_text = "ç¢ºç‡ä½ä¸‹ï¼ˆå¥½è»¢æ–¹å‘ï¼‰"
            bar_desc = f"â€»ã‚°ãƒ©ãƒ•ã®èµ¤ã„ãƒãƒ¼ãŒ{selected_target_name}ã®ç™ºç”Ÿç¢ºç‡ã‚’æŠ¼ã—ä¸Šã’ã‚‹ï¼ˆæ‚ªåŒ–ï¼‰è¦å› ã€é’ã„ãƒãƒ¼ãŒæŠ¼ã—ä¸‹ã’ã‚‹ï¼ˆå¥½è»¢ï¼‰è¦å› ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚"
        else:
            pos_effect_text = "ç¢ºç‡ä¸Šæ˜‡ï¼ˆå¥½è»¢æ–¹å‘ï¼‰"
            neg_effect_text = "ç¢ºç‡ä½ä¸‹ï¼ˆæ‚ªåŒ–æ–¹å‘ï¼‰"
            bar_desc = f"â€»ã‚°ãƒ©ãƒ•ã®èµ¤ã„ãƒãƒ¼ãŒ{selected_target_name}ã®ç™ºç”Ÿç¢ºç‡ã‚’æŠ¼ã—ä¸Šã’ã‚‹ï¼ˆå¥½è»¢ï¼‰è¦å› ã€é’ã„ãƒãƒ¼ãŒæŠ¼ã—ä¸‹ã’ã‚‹ï¼ˆæ‚ªåŒ–ï¼‰è¦å› ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚"

        base_pos = None
        if not pos_factors.empty:
            top_pos = pos_factors.iloc[0]
            desc_pos = get_factor_direction_text(top_pos['Feature'], top_pos['Value'], available_data_all)
            base_pos = get_base_feature_name(top_pos['Feature'])
            st.write(f"- ğŸ“ˆ **ç¢ºç‡ã‚’ä¸Šã’ã‚‹è¦å› **: **{desc_pos}** ãŒ{pos_effect_text}ã«åƒã„ã¦ã„ã¾ã™ (å½±éŸ¿åº¦: {top_pos['SHAP']:+.2f})ã€‚")
            
        if not neg_factors.empty:
            top_neg = neg_factors.iloc[0]
            # ãƒ—ãƒ©ã‚¹è¦å› ã¨ãƒ™ãƒ¼ã‚¹æŒ‡æ¨™ãŒè¢«ã£ãŸå ´åˆã¯ã€æ¬¡ç‚¹ã®è¦å› ã‚’æ¡ç”¨ã™ã‚‹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ··ä¹±ã‚’é˜²ããŸã‚ï¼‰
            if base_pos is not None and get_base_feature_name(top_neg['Feature']) == base_pos and len(neg_factors) > 1:
                top_neg = neg_factors.iloc[1]
                
            desc_neg = get_factor_direction_text(top_neg['Feature'], top_neg['Value'], available_data_all)
            st.write(f"- ğŸ“‰ **ç¢ºç‡ã‚’ä¸‹ã’ã‚‹è¦å› **: **{desc_neg}** ãŒ{neg_effect_text}ã«åƒã„ã¦ã„ã¾ã™ (å½±éŸ¿åº¦: {top_neg['SHAP']:+.2f})ã€‚")
            
        st.caption(bar_desc)

    # 5. ãƒ­ã‚¸ãƒƒã‚¯ã«ã‚ˆã‚‹åƒãæ–¹åˆ¤å®š
    schedule_density = float(target_data["schedule_density_2h"].values[0]) if "schedule_density_2h" in target_data else 0
    time_to_next = float(target_data["time_to_next_event_min"].values[0]) if "time_to_next_event_min" in target_data else np.nan
    is_meeting = float(target_data["is_meeting"].values[0]) if "is_meeting" in target_data else 0
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæŒ‡æ¨™ã®å¢—æ¸›ã‚’ã€Œå¥½è»¢(ãƒ—ãƒ©ã‚¹)ã€ã€Œæ‚ªåŒ–(ãƒã‚¤ãƒŠã‚¹)ã€ã®å…±é€šè»¸ã«å¤‰æ›ã™ã‚‹
    if target_col in ['NEMUKE_SCORE_NEW', 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š']:
        state_trend_prob = 1.0 - current_proba  # æ‚ªåŒ–ç¢ºç‡ãŒä½ã„(å¥½è»¢)ã»ã©é«˜ã„å€¤
    else:
        state_trend_prob = current_proba   # ç™ºç”Ÿç¢ºç‡ãŒé«˜ã„(å¥½è»¢)ã»ã©é«˜ã„å€¤

    reasons = []
    if is_meeting > 0: reasons.append("ç¾åœ¨ä¼šè­°ä¸­")
    if schedule_density >= 0.6: reasons.append("äºˆå®šå¯†åº¦ãŒé«˜ã„")
    
    if state_trend_prob >= 0.6: 
        reasons.append(f"{selected_target_name}ã®å¥½ã¾ã—ã„ç¢ºç‡ãŒé«˜ã„")
    elif state_trend_prob <= 0.4: 
        reasons.append(f"{selected_target_name}ã®å¥½ã¾ã—ããªã„ç¢ºç‡ãŒé«˜ã„")
    
    if is_meeting > 0:
        work_mode = "E: æ®µå–ã‚Šï¼ˆä¼šè­°ãƒ¢ãƒ¼ãƒ‰ï¼‰"
        advice = "è«–ç‚¹ã‚’1æšã«æ•´ç†ã—ã€æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ToDoåŒ–ã—ã¾ã—ã‚‡ã†ã€‚"
    elif state_trend_prob >= 0.6 and (np.isnan(time_to_next) or time_to_next >= 50) and schedule_density < 0.6:
        work_mode = "C: ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆï¼ˆæ·±ï¼‰- ä¼ç”»ãƒ»æˆ¦ç•¥"
        advice = "çŠ¶æ…‹ãŒå¥½è»¢ã™ã‚‹ç¢ºç‡ãŒé«˜ãã€ã¾ã¨ã¾ã£ãŸæ™‚é–“ã‚‚ã‚ã‚Šã¾ã™ã€‚è¨­è¨ˆãƒ»ä¼ç”»ã®éª¨æ ¼ã¥ãã‚Šãªã©ã€é‡ã„æ€è€ƒã‚¿ã‚¹ã‚¯ã‚’é€²ã‚ã‚‹ã®ãŒæœ€é©ã§ã™ã€‚"
    elif state_trend_prob <= 0.4 or schedule_density >= 0.6:
        work_mode = "D: ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆï¼ˆè»½ï¼‰- æ•´ç†ãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼"
        advice = "äºˆå®šãŒç´°åˆ‡ã‚Œã‹ã€çŠ¶æ…‹ãŒæ‚ªåŒ–ã™ã‚‹ç¢ºç‡ãŒé«˜ã„ã§ã™ã€‚10ã€œ20åˆ†ã§çµ‚ã‚ã‚‹ToDoæ¶ˆåŒ–ã‚„ã€è³‡æ–™ã®æ•´å½¢ãƒ»ãƒã‚§ãƒƒã‚¯ä½œæ¥­ã«æ™‚é–“ã‚’å½“ã¦ã¾ã—ã‚‡ã†ã€‚"
    else:
        if (np.isnan(time_to_next) or time_to_next >= 30) and schedule_density < 0.6:
            work_mode = "A: ã‚¤ãƒ³ãƒ—ãƒƒãƒˆï¼ˆé‡ï¼‰ ã¾ãŸã¯ B: ã‚¤ãƒ³ãƒ—ãƒƒãƒˆï¼ˆè»½ï¼‰"
            advice = "é›£ã—ã‚è³‡æ–™ã®èª­ã¿è¾¼ã¿ã‚„æƒ…å ±æ•´ç†ãªã©ã€æ¬¡ã®æ·±ã„æ€è€ƒã«å‘ã‘ãŸã‚¤ãƒ³ãƒ—ãƒƒãƒˆä½œæ¥­ã«é©ã—ã¦ã„ã¾ã™ã€‚"
        else:
            work_mode = "E: æ®µå–ã‚Š"
            advice = "æ¬¡ã®æ·±ã„ä½œæ¥­ã¸ã‚¹ãƒ ãƒ¼ã‚ºã«å…¥ã‚Œã‚‹ã‚ˆã†ã€è«–ç‚¹ã®åˆ—æŒ™ã‚„å„ªå…ˆé †ä½ä»˜ã‘ã€ç´ æã®æ´—ã„å‡ºã—ã‚’è¡Œã„ã¾ã—ã‚‡ã†ã€‚"

    # 6. ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    st.subheader("ğŸ“ åˆ†æãƒ¬ãƒãƒ¼ãƒˆ (AIã«ã‚ˆã‚‹ææ¡ˆ)")
    
    main_factor_desc = get_factor_direction_text(exp_df_action.iloc[0]['Feature'], exp_df_action.iloc[0]['Value'], available_data_all) if not exp_df_action.empty else "ä¸æ˜"

    prompt_context = f"""
    ç¾åœ¨æ™‚åˆ»: {current_time.strftime('%Y-%m-%d %H:%M')}
    ç¾åœ¨ã®{selected_target_name}ã®çŠ¶æ…‹: {'Yes' if current_state_bool else 'No'}
    {PREDICT_AHEAD}å¾Œã®äºˆæ¸¬åˆ¤å®š: {'Yes' if predicted_state_bool else 'No'} (ç™ºç”Ÿç¢ºç‡: {current_proba * 100:.1f}%)
    ç›´è¿‘ã®ä¸»è¦å› : {main_factor_desc} (SHAP: {exp_df_action.iloc[0]['SHAP']:+.2f})
    åˆ¤å®šã•ã‚ŒãŸåƒãæ–¹: {work_mode}
    ç†ç”±: {', '.join(reasons) if reasons else 'ç‰¹ã«ãªã—'}
    """
    
    if use_gemini and api_key:
        with st.spinner("GeminiãŒãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­..."):
            try:
                genai.configure(api_key=api_key)
                model_llm = genai.GenerativeModel('gemini-2.5-flash')
                prompt = f"ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€ã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å®¢è¦³çš„ãªåƒãæ–¹ã‚¢ãƒ‰ãƒã‚¤ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n\n{prompt_context}\n\næ§‹æˆ:\n1. äºˆæ¸¬çµæœã¨ä¸»ãªè¦å› \n2. å¥¨åŠ±ã™ã‚‹åƒãæ–¹ã®å…·ä½“ä¾‹"
                resp = model_llm.generate_content(prompt)
                st.write(resp.text)
            except Exception as e:
                st.error(f"Gemini APIã‚¨ãƒ©ãƒ¼: {e}")
    else:
        st.info("ğŸ’¡ Gemini APIã‚­ãƒ¼ãŒæœªå…¥åŠ›ã®ãŸã‚ã€ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
        st.markdown(f"#### 1. è¿‘ã„å°†æ¥ï¼ˆ{PREDICT_AHEAD}å¾Œï¼‰ã®äºˆæ¸¬çµæœ")
        st.write(f"åŸºæº–æ—¥æ™‚ï¼ˆ{current_time.strftime('%Y-%m-%d %H:%M')}ï¼‰ã® {selected_target_name} ã¯ **{'Yes' if current_state_bool else 'No'}** ã®çŠ¶æ…‹ã§ã™ã€‚")
        st.write(f"{PREDICT_AHEAD}å¾Œã¯ **{'Yes' if predicted_state_bool else 'No'}** ï¼ˆç™ºç”Ÿç¢ºç‡ **{current_proba * 100:.1f} %**ï¼‰ã¨äºˆæ¸¬ã•ã‚Œã¾ã™ã€‚")
        
        st.write(f"ã“ã®äºˆæ¸¬ã®ä¸»ãªè¦å› ã¨ã—ã¦ã€**{main_factor_desc}** ãŒå½±éŸ¿ã—ã¦ã„ã¾ã™ã€‚")

        st.markdown(f"#### 2. å¥¨åŠ±ã™ã‚‹åƒãæ–¹")
        st.write(f"ç¾åœ¨ã®äºˆæ¸¬ç¢ºç‡ã¨äºˆå®šçŠ¶æ³ï¼ˆ{', '.join(reasons) if reasons else 'é˜»å®³è¦å› ãªã—'}ï¼‰ã‹ã‚‰ã€**ã€Œ{work_mode}ã€**ã«å–ã‚Šçµ„ã‚€ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
        st.write(f"**ğŸ’¡ é€²ã‚æ–¹ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹**: {advice}")

# --- UI ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ ---
st.write("### ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
col_file1, col_file2 = st.columns(2)
with col_file1:
    file_ts = st.file_uploader("1. ç”Ÿä½“ãƒ‡ãƒ¼ã‚¿ (CSVå½¢å¼)", type=['csv'])
with col_file2:
    file_sched = st.file_uploader("2. äºˆå®šè¡¨ãƒ‡ãƒ¼ã‚¿ (äºˆå®šè¡¨.CSV) â€»ä»»æ„", type=['csv'])

if st.button("ğŸš€ åˆ†æã‚’å®Ÿè¡Œã™ã‚‹", type="primary"):
    if file_ts is not None:
        try:
            # 3è¡Œç›®ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹2ï¼‰ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ©ãƒ™ãƒ«ã¨ã—ã¦èª­ã¿è¾¼ã‚€
            df_ts = pd.read_csv(file_ts, skiprows=2)
            df_sched = pd.read_csv(file_sched) if file_sched is not None else None
            
            # åˆ†æå‡¦ç†ã®å®Ÿè¡Œ
            run_analysis(df_ts, df_sched, use_gemini=True if api_key else False)
            
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    else:
        st.warning("âš ï¸ ç”Ÿä½“ãƒ‡ãƒ¼ã‚¿ (CSVå½¢å¼) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")