# -*- coding: utf-8 -*-
"""
ã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ« + Outlookã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« é›†ä¸­ãƒ»ç–²åŠ´äºˆæ¸¬ã‚¢ãƒ—ãƒª (Streamlitç‰ˆ)
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import lightgbm as lgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score, log_loss
from sklearn.model_selection import TimeSeriesSplit
import google.generativeai as genai
import shap
import warnings
import math
import plotly.graph_objects as go
import plotly.express as px

# --- Streamlit ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="é›†ä¸­ãƒ»ç–²åŠ´äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ", layout="wide")

st.title("ğŸ§  ã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ«Ã—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« é›†ä¸­äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ")
st.markdown("""
ã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ«ãƒ‡ãƒã‚¤ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã¨äºˆå®šè¡¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ„ã¿åˆã‚ã›ã¦ã€æ•°æ™‚é–“å¾Œã®é›†ä¸­ã‚¹ã‚³ã‚¢ã‚’äºˆæ¸¬ã—ã€æ¨å¥¨ã™ã‚‹åƒãæ–¹ã‚’ææ¡ˆã—ã¾ã™ã€‚
""")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ (è¨­å®š) ---
st.sidebar.header("âš™ï¸ è¨­å®š")
api_key = st.sidebar.text_input("Gemini APIã‚­ãƒ¼ (çœç•¥æ™‚ã¯å›ºå®šãƒ«ãƒ¼ãƒ«ã§å‡ºåŠ›)", type="password")

st.sidebar.subheader("åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
RESAMPLE_FREQ = st.sidebar.selectbox("åˆ†æå˜ä½", ['10T', '30T', '1H'], index=1)
PREDICT_AHEAD = st.sidebar.selectbox("äºˆæ¸¬å…ˆ", ['10T', '30T', '1H'], index=1)
LOOKBACK_PERIOD = st.sidebar.selectbox("éå»å‚ç…§", ['1H', '2H', '3H'], index=1)
INTERPOLATE_LIMIT = st.sidebar.selectbox("è£œå®Œé™ç•Œ", ['10T', '30T', '1H'], index=1)

st.sidebar.subheader("äºˆæ¸¬åŸºæº–æ—¥æ™‚ (ä»»æ„)")
TARGET_DATETIME_STR = st.sidebar.text_input("ä¾‹ï¼‰2026-01-01 16:00 (ç©ºæ¬„ã§æœ€æ–°ãƒ‡ãƒ¼ã‚¿)")
TARGET_DATETIME = TARGET_DATETIME_STR if TARGET_DATETIME_STR.strip() != "" else None

freq_td = pd.Timedelta(RESAMPLE_FREQ)
ahead_td = pd.Timedelta(PREDICT_AHEAD)
lookback_td = pd.Timedelta(LOOKBACK_PERIOD)
interp_td = pd.Timedelta(INTERPOLATE_LIMIT)

ahead_steps = max(1, int(ahead_td / freq_td))
lookback_steps = max(2, int(lookback_td / freq_td))
interp_steps = max(1, int(interp_td / freq_td))

# --- äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®é¸æŠæ©Ÿèƒ½ã‚’è¿½åŠ  ---
TARGET_OPTIONS = {
    'é›†ä¸­åˆ¤å®š': 'é›†ä¸­åˆ¤å®š(1=é›†ä¸­)',
    'ç–²åŠ´åˆ¤å®š': 'ç–²åŠ´åˆ¤å®š(1=ç–²åŠ´)'
}
st.sidebar.subheader("ğŸ¯ äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ")
selected_target_name = st.sidebar.selectbox("äºˆæ¸¬ã™ã‚‹æŒ‡æ¨™ã‚’é¸æŠ", list(TARGET_OPTIONS.values()), index=0)
target_col = [k for k, v in TARGET_OPTIONS.items() if v == selected_target_name][0]

# --- æ–°è¦: é•·æœŸåˆ†æç”¨ãƒ•ã‚£ãƒ«ã‚¿ ---
st.sidebar.subheader("ğŸ“… é•·æœŸåˆ†æãƒ•ã‚£ãƒ«ã‚¿ (ç‰¹æ€§ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”¨)")
dow_options = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]
selected_dows = st.sidebar.multiselect("å¯¾è±¡æ›œæ—¥", dow_options, default=dow_options)
time_range = st.sidebar.slider("å¯¾è±¡æ™‚é–“å¸¯", 0, 23, (9, 19))

# --- ç‰¹å¾´é‡åæ—¥æœ¬èªåŒ–ãƒ˜ãƒ«ãƒ‘ãƒ¼ ---
def jp_feat_name(col_name: str) -> str:
    mapping = {
        'CVRR_SCORE_NEW': 'é›†ä¸­ã‚¹ã‚³ã‚¢', 'SkinTemp': 'çš®è†šæ¸©åº¦', 'LP_SCORE_NEW': 'ãƒªãƒ©ãƒƒã‚¯ã‚¹ã‚¹ã‚³ã‚¢',
        'LFHF_SCORE_NEW': 'LF/HF(è‡ªå¾‹ç¥çµŒãƒãƒ©ãƒ³ã‚¹)', 'TP': 'TP(è‡ªå¾‹ç¥çµŒãƒˆãƒ¼ã‚¿ãƒ«ãƒ‘ãƒ¯ãƒ¼)', 'NEMUKE_SCORE_NEW': 'ä½è¦šé†’ã‚¹ã‚³ã‚¢',
        'PR_SCORE_NEW': 'è„ˆæ‹', 'RMSSD_SCORE_NEW': 'ç–²åŠ´ãƒ»å›å¾©ã‚¹ã‚³ã‚¢', '1åˆ†é–“æ­©æ•°': 'æ­©æ•°', 'accDeviation': 'æ´»å‹•é‡(åŠ é€Ÿåº¦)',
        'has_schedule': 'äºˆå®šã®æœ‰ç„¡', 'is_meeting': 'ä¼šè­°ä¸­ã‹ã©ã†ã‹', 'schedule_density_2h': 'æœ€è¿‘ã®äºˆå®šã®è©°ã¾ã‚Šå…·åˆ',
        'time_to_next_event_min': 'æ¬¡ã®äºˆå®šã¾ã§ã®æ™‚é–“', 'time_since_prev_event_min': 'å‰ã®äºˆå®šã‹ã‚‰ã®çµŒéæ™‚é–“',
        'daily_schedule_hours': '1æ—¥ã®ç·äºˆå®šæ™‚é–“', 'consecutive_schedules': 'é€£ç¶šäºˆå®šãƒ–ãƒ­ãƒƒã‚¯æ•°',
        'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°', 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“_åˆ†': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“(åˆ†)',
        'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“(åˆ†)', 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“(åˆ†)',
        'é›†ä¸­åˆ¤å®š': 'é›†ä¸­åˆ¤å®š', 'ç–²åŠ´åˆ¤å®š': 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š': 'å¼·ã„ç–²åŠ´åˆ¤å®š',
        'é›†ä¸­çŠ¶æ…‹': 'é›†ä¸­çŠ¶æ…‹', 'çœ æ°—çŠ¶æ…‹': 'çœ æ°—çŠ¶æ…‹', 'ç–²åŠ´çŠ¶æ…‹': 'ç–²åŠ´çŠ¶æ…‹',
        'é›†ä¸­ç¶™ç¶šæ™‚é–“': 'é›†ä¸­ç¶™ç¶šæ™‚é–“', 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“': 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“',
        'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'ç–²åŠ´ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“': 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“'
    }
    
    base_jp = col_name
    remainder = ""
    for k, v in mapping.items():
        if col_name.startswith(k):
            base_jp = v
            remainder = col_name[len(k):]
            break
            
    if remainder == "": return base_jp
    elif remainder == "_roll_mean": return f"æœ€è¿‘ã®ã€Œ{base_jp}ã€ã®å¹³å‡çš„ãªé«˜ã•"
    elif remainder == "_roll_slope": return f"æœ€è¿‘ã®ã€Œ{base_jp}ã€ã®æ€¥ãªå¤‰åŒ–(ãƒˆãƒ¬ãƒ³ãƒ‰)"
    elif remainder == "_diff1": return f"å‰å›ã‹ã‚‰ã®ã€Œ{base_jp}ã€ã®å¤‰å‹•å¹…"
    elif remainder.startswith("_lag"): return f"å°‘ã—å‰ã®ã€Œ{base_jp}ã€ã®çŠ¶æ…‹"
    elif remainder == "_is_missing": return f"ã€Œ{base_jp}ã€ãŒæœªè¨ˆæ¸¬ã§ã‚ã‚‹ã“ã¨"
    else: return f"{base_jp}{remainder}"

def get_base_feature_name(feat: str) -> str:
    mapping = {
        'CVRR_SCORE_NEW': 'é›†ä¸­ã‚¹ã‚³ã‚¢', 'SkinTemp': 'çš®è†šæ¸©åº¦', 'LP_SCORE_NEW': 'ãƒªãƒ©ãƒƒã‚¯ã‚¹ã‚¹ã‚³ã‚¢',
        'LFHF_SCORE_NEW': 'LF/HF(è‡ªå¾‹ç¥çµŒãƒãƒ©ãƒ³ã‚¹)', 'TP': 'TP(è‡ªå¾‹ç¥çµŒãƒˆãƒ¼ã‚¿ãƒ«ãƒ‘ãƒ¯ãƒ¼)', 'NEMUKE_SCORE_NEW': 'ä½è¦šé†’ã‚¹ã‚³ã‚¢',
        'PR_SCORE_NEW': 'è„ˆæ‹', 'RMSSD_SCORE_NEW': 'ç–²åŠ´ãƒ»å›å¾©ã‚¹ã‚³ã‚¢', '1åˆ†é–“æ­©æ•°': 'æ­©æ•°', 'accDeviation': 'æ´»å‹•é‡(åŠ é€Ÿåº¦)',
        'has_schedule': 'äºˆå®š', 'is_meeting': 'ä¼šè­°', 'schedule_density_2h': 'äºˆå®šã®å¯†åº¦',
        'time_to_next_event_min': 'æ¬¡ã®äºˆå®šã¾ã§ã®æ™‚é–“', 'time_since_prev_event_min': 'å‰ã®äºˆå®šã‹ã‚‰ã®çµŒéæ™‚é–“',
        'daily_schedule_hours': '1æ—¥ã®ç·äºˆå®šæ™‚é–“', 'consecutive_schedules': 'é€£ç¶šäºˆå®šãƒ–ãƒ­ãƒƒã‚¯æ•°',
        'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°', 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“_åˆ†': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“',
        'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“', 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“',
        'é›†ä¸­åˆ¤å®š': 'é›†ä¸­åˆ¤å®š', 'ç–²åŠ´åˆ¤å®š': 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š': 'å¼·ã„ç–²åŠ´åˆ¤å®š',
        'é›†ä¸­çŠ¶æ…‹': 'é›†ä¸­çŠ¶æ…‹', 'çœ æ°—çŠ¶æ…‹': 'çœ æ°—çŠ¶æ…‹', 'ç–²åŠ´çŠ¶æ…‹': 'ç–²åŠ´çŠ¶æ…‹',
        'é›†ä¸­ç¶™ç¶šæ™‚é–“': 'é›†ä¸­ç¶™ç¶šæ™‚é–“', 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“': 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“',
        'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'ç–²åŠ´ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“': 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“'
    }
    for k, v in mapping.items():
        if feat.startswith(k): return v
    return feat

def get_factor_direction_text(feat: str, val: float, df_all: pd.DataFrame) -> str:
    mapping = {
        'CVRR_SCORE_NEW': 'é›†ä¸­ã‚¹ã‚³ã‚¢', 'SkinTemp': 'çš®è†šæ¸©åº¦', 'LP_SCORE_NEW': 'ãƒªãƒ©ãƒƒã‚¯ã‚¹ã‚¹ã‚³ã‚¢',
        'LFHF_SCORE_NEW': 'LF/HF(è‡ªå¾‹ç¥çµŒãƒãƒ©ãƒ³ã‚¹)', 'TP': 'TP(è‡ªå¾‹ç¥çµŒãƒˆãƒ¼ã‚¿ãƒ«ãƒ‘ãƒ¯ãƒ¼)', 'NEMUKE_SCORE_NEW': 'ä½è¦šé†’ã‚¹ã‚³ã‚¢',
        'PR_SCORE_NEW': 'è„ˆæ‹', 'RMSSD_SCORE_NEW': 'ç–²åŠ´ãƒ»å›å¾©ã‚¹ã‚³ã‚¢', '1åˆ†é–“æ­©æ•°': 'æ­©æ•°', 'accDeviation': 'æ´»å‹•é‡(åŠ é€Ÿåº¦)',
        'has_schedule': 'äºˆå®š', 'is_meeting': 'ä¼šè­°', 'schedule_density_2h': 'äºˆå®šã®å¯†åº¦',
        'time_to_next_event_min': 'æ¬¡ã®äºˆå®šã¾ã§ã®æ™‚é–“', 'time_since_prev_event_min': 'å‰ã®äºˆå®šã‹ã‚‰ã®çµŒéæ™‚é–“',
        'daily_schedule_hours': '1æ—¥ã®ç·äºˆå®šæ™‚é–“', 'consecutive_schedules': 'é€£ç¶šäºˆå®šãƒ–ãƒ­ãƒƒã‚¯æ•°',
        'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°', 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“_åˆ†': 'ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“',
        'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“', 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“_åˆ†': 'ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“',
        'é›†ä¸­åˆ¤å®š': 'é›†ä¸­åˆ¤å®š', 'ç–²åŠ´åˆ¤å®š': 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š': 'å¼·ã„ç–²åŠ´åˆ¤å®š',
        'é›†ä¸­çŠ¶æ…‹': 'é›†ä¸­çŠ¶æ…‹', 'çœ æ°—çŠ¶æ…‹': 'çœ æ°—çŠ¶æ…‹', 'ç–²åŠ´çŠ¶æ…‹': 'ç–²åŠ´çŠ¶æ…‹',
        'é›†ä¸­ç¶™ç¶šæ™‚é–“': 'é›†ä¸­ç¶™ç¶šæ™‚é–“', 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“': 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“',
        'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'ç–²åŠ´ç¶™ç¶šæ™‚é–“': 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“', 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“': 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“'
    }
    
    base_jp = feat
    remainder = ""
    for k, v in mapping.items():
        if feat.startswith(k):
            base_jp = v
            remainder = feat[len(k):]
            break
            
    if "_is_missing" in feat: return f"ã€Œ{base_jp}ã€ãŒæœªè¨ˆæ¸¬ã§ã‚ã‚‹ã“ã¨"
    elif feat in ["has_schedule", "is_meeting"]: return f"ã€Œ{base_jp}ã€ãŒå…¥ã£ã¦ã„ã‚‹ã“ã¨" if val > 0 else f"ã€Œ{base_jp}ã€ãŒå…¥ã£ã¦ã„ãªã„ã“ã¨"
    elif feat in ["é›†ä¸­çŠ¶æ…‹", "çœ æ°—çŠ¶æ…‹", "ç–²åŠ´çŠ¶æ…‹"]: return f"ã€Œ{base_jp}ã€ãŒã€Œ{val}ã€ã§ã‚ã‚‹ã“ã¨"
            
    if "_roll_slope" in feat or "_diff1" in feat:
        direction = "ã®å¢—åŠ " if val > 0 else "ã®ä½ä¸‹" if val < 0 else "ã®å¤‰åŒ–ãªã—"
        return f"æœ€è¿‘ã®ã€Œ{base_jp}ã€ã®ãƒˆãƒ¬ãƒ³ãƒ‰{direction}" if "_roll_slope" in feat else f"å‰å›ã‹ã‚‰ã®ã€Œ{base_jp}ã€{direction}"
    else:
        if "_roll_mean" in feat: return f"æœ€è¿‘ã®ã€Œ{base_jp}ã€"
        elif "_lag" in feat: return f"å°‘ã—å‰ã®ã€Œ{base_jp}ã€"
        else: return f"ã€Œ{base_jp}ã€"

# --- åˆ†æãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def run_analysis(df_ts, df_sched, use_gemini=False):
    with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ãƒ»å‰å‡¦ç†ã—ã¦ã„ã¾ã™..."):
        if 'timestamp' in df_ts.columns:
            df_ts['timestamp_clean'] = df_ts['timestamp'].astype(str).str.split(' GMT').str[0]
            df_ts['datetime'] = pd.to_datetime(df_ts['timestamp_clean'], errors='coerce')
            df_ts = df_ts.dropna(subset=['datetime'])
            df_ts.set_index('datetime', inplace=True)
            df_ts.drop(columns=['timestamp', 'timestamp_clean'], inplace=True, errors='ignore')
            df_ts = df_ts.sort_index()

        base_agg_dict = {
            'SkinTemp': 'mean', 'CVRR_SCORE_NEW': 'mean', 'LP_SCORE_NEW': 'mean',
            'LFHF_SCORE_NEW': 'mean', 'TP': 'mean', 'NEMUKE_SCORE_NEW': 'mean',
            'PR_SCORE_NEW': 'mean', 'RMSSD_SCORE_NEW': 'mean', '1åˆ†é–“æ­©æ•°': 'sum', 'accDeviation': 'mean',
            'é›†ä¸­åˆ¤å®š': 'mean', 'ç–²åŠ´åˆ¤å®š': 'mean', 'å¼·ã„ç–²åŠ´åˆ¤å®š': 'mean',
            'é›†ä¸­ç¶™ç¶šæ™‚é–“': 'mean', 'æ·±ã„é›†ä¸­ç¶™ç¶šæ™‚é–“': 'mean', 'ç–²åŠ´çŠ¶æ…‹ç¶™ç¶šæ™‚é–“': 'mean', 'ç–²åŠ´ç¶™ç¶šæ™‚é–“': 'mean', 'é«˜å¿ƒæ‹ç¶™ç¶šæ™‚é–“': 'mean'
        }
        
        cat_agg_dict = {'é›†ä¸­çŠ¶æ…‹': 'last', 'çœ æ°—çŠ¶æ…‹': 'last', 'ç–²åŠ´çŠ¶æ…‹': 'last'}
        
        agg_dict = {col: func for col, func in base_agg_dict.items() if col in df_ts.columns}
        for col, func in cat_agg_dict.items():
            if col in df_ts.columns: agg_dict[col] = func
        
        if target_col not in agg_dict:
            st.error(f"ã‚¨ãƒ©ãƒ¼: äºˆæ¸¬ã«å¿…è¦ãªç›®çš„å¤‰æ•°ã€Œ{selected_target_name}ã€ãŒãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return

        for col in agg_dict.keys():
            if col in base_agg_dict.keys():
                df_ts[col] = pd.to_numeric(df_ts[col], errors='coerce')

        df_resampled = df_ts.resample(RESAMPLE_FREQ).agg(agg_dict)

        if df_sched is not None:
            df_sched = df_sched[df_sched['çµ‚æ—¥ã‚¤ãƒ™ãƒ³ãƒˆ'].astype(str).str.upper() != 'TRUE']
            df_sched['start_dt'] = pd.to_datetime(df_sched['é–‹å§‹æ—¥'].astype(str) + ' ' + df_sched['é–‹å§‹æ™‚åˆ»'].astype(str), errors='coerce')
            df_sched['end_dt']   = pd.to_datetime(df_sched['çµ‚äº†æ—¥'].astype(str) + ' ' + df_sched['çµ‚äº†æ™‚åˆ»'].astype(str), errors='coerce')
            df_sched = df_sched.dropna(subset=['start_dt', 'end_dt']).sort_values('start_dt')

            df_resampled['has_schedule'] = 0
            df_resampled['is_meeting'] = 0
            meeting_keywords = ['ä¼šè­°', 'æ‰“åˆã›', 'æ‰“ã¡åˆã‚ã›', 'MTG', 'é¢è«‡', 'å•†è«‡', 'æ¥å®¢', 'è¨ªå•']

            for _, row in df_sched.iterrows():
                mask = (df_resampled.index < row['end_dt']) & ((df_resampled.index + freq_td) > row['start_dt'])
                df_resampled.loc[mask, 'has_schedule'] = 1
                subject = str(row.get('ä»¶å', ''))
                if any(kw in subject for kw in meeting_keywords):
                    df_resampled.loc[mask, 'is_meeting'] = 1

            s = df_resampled['has_schedule']
            df_resampled['consecutive_schedules'] = s.groupby((s != s.shift()).cumsum()).cumsum()

            df_resampled['date'] = df_resampled.index.date
            df_resampled = df_resampled.join(df_resampled.groupby('date')['has_schedule'].sum().rename('daily_schedule_hours'), on='date').fillna({'daily_schedule_hours': 0})
            df_resampled.drop(columns=['date'], inplace=True)

            event_starts = df_sched['start_dt'].to_numpy(dtype='datetime64[ns]')
            event_ends   = df_sched['end_dt'].to_numpy(dtype='datetime64[ns]')
            t = df_resampled.index.to_numpy(dtype='datetime64[ns]')

            next_start_idx = np.searchsorted(event_starts, t, side='left')
            has_next = next_start_idx < len(event_starts)
            next_idx_safe = np.clip(next_start_idx, 0, max(len(event_starts) - 1, 0))
            next_start = np.full(t.shape, np.datetime64('NaT'), dtype='datetime64[ns]')
            if len(event_starts) > 0: next_start[has_next] = event_starts[next_idx_safe[has_next]]

            prev_end_idx = np.searchsorted(event_ends, t, side='right') - 1
            has_prev = prev_end_idx >= 0
            prev_idx_safe = np.clip(prev_end_idx, 0, max(len(event_ends) - 1, 0))
            prev_end = np.full(t.shape, np.datetime64('NaT'), dtype='datetime64[ns]')
            if len(event_ends) > 0: prev_end[has_prev] = event_ends[prev_idx_safe[has_prev]]

            df_resampled['time_to_next_event_min'] = (next_start - t) / np.timedelta64(1, 'm')
            df_resampled['time_since_prev_event_min'] = (t - prev_end) / np.timedelta64(1, 'm')

            win_steps = max(1, int(pd.Timedelta('2H') / freq_td))
            df_resampled['schedule_density_2h'] = df_resampled['has_schedule'].rolling(win_steps, min_periods=1).mean()

    with st.spinner("ç‰¹å¾´é‡ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™..."):
        df_features = df_resampled.copy()
        df_features['hour'] = df_features.index.hour.astype('category')
        df_features['dayofweek'] = df_features.index.dayofweek.astype('category')
        
        for c in ['é›†ä¸­çŠ¶æ…‹', 'çœ æ°—çŠ¶æ…‹', 'ç–²åŠ´çŠ¶æ…‹']:
            if c in df_features.columns: df_features[c] = df_features[c].astype('category')
        
        numeric_cols = df_resampled.select_dtypes(include=[np.number]).columns.tolist()
        win = lookback_steps
        x = np.arange(win, dtype=float)
        x_mean = x.mean()
        x_var = ((x - x_mean) ** 2).sum()
        def rolling_slope(arr):
            y = arr.astype(float)
            if x_var == 0: return 0.0
            return ((x - x_mean) * (y - y.mean())).sum() / x_var

        for col in numeric_cols:
            df_features[f'{col}_is_missing'] = df_resampled[col].isna().astype(int)
            r = df_features[col].rolling(win, min_periods=win)
            df_features[f'{col}_roll_mean'] = r.mean()
            df_features[f'{col}_roll_slope'] = r.apply(rolling_slope, raw=True)
            df_features[f'{col}_diff1'] = df_features[col] - df_features[col].shift(1)

        df_features['date'] = df_features.index.date
        if '1åˆ†é–“æ­©æ•°' in df_features.columns:
            df_features['ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©æ­©æ•°'] = df_features.groupby('date')['1åˆ†é–“æ­©æ•°'].cumsum()
        if 'is_meeting' in df_features.columns:
            df_features['ä»Šæ—¥ã‹ã‚‰ã®ç´¯ç©ä¼šè­°æ™‚é–“_åˆ†'] = df_features.groupby('date')['is_meeting'].cumsum() * (freq_td.total_seconds() / 60)
        
        def calc_duration_mins(series):
            group_id = (series != series.shift()).cumsum()
            return (series.groupby(group_id).cumcount() + 1) * (freq_td.total_seconds() / 60)
            
        if 'é›†ä¸­åˆ¤å®š' in df_features.columns:
            focus_mask = (df_features['é›†ä¸­åˆ¤å®š'] >= 0.5).astype(int)
            df_features['ç¾åœ¨ã®é›†ä¸­ç¶™ç¶šæ™‚é–“_åˆ†'] = calc_duration_mins(focus_mask) * focus_mask
        if 'ç–²åŠ´åˆ¤å®š' in df_features.columns:
            fatigue_mask = (df_features['ç–²åŠ´åˆ¤å®š'] >= 0.5).astype(int)
            df_features['ç¾åœ¨ã®ç–²åŠ´ç¶™ç¶šæ™‚é–“_åˆ†'] = calc_duration_mins(fatigue_mask) * fatigue_mask
            
        df_features.drop(columns=['date'], inplace=True)

        target_threshold = 0.5 if target_col in ['é›†ä¸­åˆ¤å®š', 'ç–²åŠ´åˆ¤å®š', 'å¼·ã„ç–²åŠ´åˆ¤å®š'] else df_features[target_col].median()
        df_features['target_ahead_class'] = (df_features[target_col].shift(-ahead_steps) >= target_threshold).astype(int)

    with st.spinner("LightGBMåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¦ã„ã¾ã™..."):
        drop_cols = ['target_ahead_class']
        df_all = df_features.copy()
        split_idx = int(len(df_all) * 0.8)
        
        df_imp = df_all.copy()
        for col in df_imp.columns:
            if col not in drop_cols:
                df_imp[col] = df_imp[col].ffill(limit=interp_steps).bfill(limit=interp_steps)
        
        train_df = df_imp.iloc[:split_idx].dropna(subset=drop_cols + [target_col])
        test_df  = df_imp.iloc[split_idx:].dropna(subset=drop_cols + [target_col])

        X_train = train_df.drop(columns=drop_cols)
        y_train_class = train_df['target_ahead_class']
        X_test  = test_df.drop(columns=drop_cols)
        y_test_class = test_df['target_ahead_class']
        
        cat_cols = [c for c in X_train.columns if str(X_train[c].dtype) == 'category']
        model = lgb.LGBMClassifier(objective='binary', n_estimators=500, learning_rate=0.03, random_state=42)
        model.fit(X_train, y_train_class, categorical_feature=cat_cols if cat_cols else 'auto')
        preds_proba = model.predict_proba(X_test)[:, 1]
        try:
            auc_test = roc_auc_score(y_test_class, preds_proba)
            logloss_test = log_loss(y_test_class, preds_proba)
        except ValueError:
            auc_test = np.nan
            logloss_test = np.nan


    # =========================================================================
    # ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ç‰¹æ€§ã‚¤ãƒ³ã‚µã‚¤ãƒˆï¼ˆé•·æœŸãƒ»æœˆæ¬¡ãƒ»æ—¥æ¬¡ï¼‰
    # =========================================================================
    st.header("ğŸ‘¤ ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ç‰¹æ€§ã‚¤ãƒ³ã‚µã‚¤ãƒˆ")
    
    # å…±é€šãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
    df_insight = df_imp.copy()
    selected_dow_indices = [dow_options.index(d) for d in selected_dows]
    df_insight = df_insight[df_insight.index.dayofweek.isin(selected_dow_indices)]
    df_insight = df_insight[(df_insight.index.hour >= time_range[0]) & (df_insight.index.hour <= time_range[1])]

    if 'é›†ä¸­åˆ¤å®š' in df_insight.columns:
        df_insight['focus_start'] = (df_insight['é›†ä¸­åˆ¤å®š'] >= 0.5) & (df_insight['é›†ä¸­åˆ¤å®š'].shift(1) < 0.5)
    if 'ç–²åŠ´åˆ¤å®š' in df_insight.columns:
        df_insight['fatigue_start'] = (df_insight['ç–²åŠ´åˆ¤å®š'] >= 0.5) & (df_insight['ç–²åŠ´åˆ¤å®š'].shift(1) < 0.5)

    def get_peak_time(metric_col):
        if metric_col not in df_insight.columns: return "ä¸æ˜", "ä¸æ˜"
        pivot_df = df_insight.pivot_table(values=metric_col, index=df_insight.index.hour, columns=df_insight.index.dayofweek, aggfunc='mean')
        daytime_pivot = pivot_df.loc[time_range[0]:time_range[1], selected_dow_indices]
        if not daytime_pivot.isna().all().all():
            best_hour, best_dow = daytime_pivot.stack().idxmax()
            return dow_options[int(best_dow)], str(int(best_hour))
        return "ä¸æ˜", "ä¸æ˜"

    f_dow, f_hour = get_peak_time('é›†ä¸­åˆ¤å®š')
    fat_dow, fat_hour = get_peak_time('ç–²åŠ´åˆ¤å®š')

    avg_focus_duration_str = "ä¸æ˜"
    daily_focus_count_str = "ä¸æ˜"
    daily_total_focus_time_str = "ä¸æ˜"
    focus_durations = pd.Series(dtype=float)
    
    if 'é›†ä¸­åˆ¤å®š' in df_ts.columns:
        df_1min = df_ts[['é›†ä¸­åˆ¤å®š']].resample('1T').mean().ffill(limit=5)
        df_1min = df_1min[df_1min.index.dayofweek.isin(selected_dow_indices)]
        df_1min = df_1min[(df_1min.index.hour >= time_range[0]) & (df_1min.index.hour <= time_range[1])]
        
        focus_mask = df_1min['é›†ä¸­åˆ¤å®š'] >= 0.5
        focus_blocks = focus_mask.groupby((focus_mask != focus_mask.shift()).cumsum())
        focus_durations = focus_blocks.sum() 
        focus_durations = focus_durations[focus_durations > 0]
        
        if not focus_durations.empty:
            avg_focus_duration_str = f"{focus_durations.mean():.0f}"
            total_focus_count = len(focus_durations)
            num_days = df_1min.index.normalize().nunique()
            daily_focus_count_str = f"{(total_focus_count / num_days if num_days > 0 else 0):.1f}"
            total_focus_minutes = focus_mask.sum()
            daily_total_focus_time_str = f"{(total_focus_minutes / num_days if num_days > 0 else 0):.0f}"

    focus_actions = []
    if '1åˆ†é–“æ­©æ•°' in df_insight.columns and 'focus_start' in df_insight.columns:
        walk_before_focus = df_insight['1åˆ†é–“æ­©æ•°'].shift(1)[df_insight['focus_start']].dropna()
        avg_walk_overall = df_insight['1åˆ†é–“æ­©æ•°'].mean()
        if not walk_before_focus.empty and avg_walk_overall > 0:
            avg_walk_before = walk_before_focus.mean()
            if avg_walk_before > avg_walk_overall * 1.2: focus_actions.append("äº‹å‰ã«ä½“ã‚’å‹•ã‹ã™ã“ã¨ï¼ˆå°‘ã—æ­©ããªã©ï¼‰")
            elif avg_walk_before < avg_walk_overall * 0.8: focus_actions.append("äº‹å‰ã«é™ã‹ãªç’°å¢ƒã§è½ã¡ç€ã„ã¦éã”ã™ã“ã¨")

    if 'has_schedule' in df_insight.columns and 'é›†ä¸­åˆ¤å®š' in df_insight.columns:
        sched_mask = df_insight['has_schedule'] >= 0.5
        sched_blocks = (sched_mask != sched_mask.shift()).cumsum()
        sched_df = df_insight[sched_mask]
        focus_scores_rested, focus_scores_rushed = [], []
        for _, group in sched_df.groupby(sched_blocks):
            if len(group) > 1 and 'time_since_prev_event_min' in group.columns:
                rest_before = group['time_since_prev_event_min'].iloc[0]
                if not np.isnan(rest_before):
                    if rest_before >= 30: focus_scores_rested.append(group['é›†ä¸­åˆ¤å®š'].mean())
                    else: focus_scores_rushed.append(group['é›†ä¸­åˆ¤å®š'].mean())
        if focus_scores_rested and focus_scores_rushed:
            diff_focus = (np.mean(focus_scores_rested) - np.mean(focus_scores_rushed)) * 100
            if diff_focus > 0: focus_actions.append("äºˆå®šã®å‰ã«30åˆ†ä»¥ä¸Šã®ç©ºãæ™‚é–“ï¼ˆä¼‘æ†©ï¼‰ã‚’ã¨ã‚‹ã“ã¨")
            elif diff_focus < 0: focus_actions.append("äºˆå®šã¨äºˆå®šã®é–“ã‚’ç©ºã‘ãšã«é€£ç¶šã—ã¦æ´»å‹•ã™ã‚‹ã“ã¨")

    focus_actions_str = "ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ç‰¹å®šã§ãã¾ã›ã‚“" if not focus_actions else "ã€".join(focus_actions)

    fatigue_actions = []
    if 'ç–²åŠ´åˆ¤å®š' in df_insight.columns and 'has_schedule' in df_insight.columns:
        sched_mask = df_insight['has_schedule'] >= 0.5
        sched_blocks = (sched_mask != sched_mask.shift()).cumsum()
        sched_df = df_insight[sched_mask]
        fatigue_diffs = []
        for _, group in sched_df.groupby(sched_blocks):
            if len(group) > 1:
                duration_hours = len(group) * (freq_td.total_seconds() / 3600)
                if duration_hours > 0:
                    fatigue_diffs.append((group['ç–²åŠ´åˆ¤å®š'].iloc[-1] - group['ç–²åŠ´åˆ¤å®š'].iloc[0]) / duration_hours)
        if fatigue_diffs and np.mean(fatigue_diffs) > 0:
            fatigue_actions.append("1æ™‚é–“ä»¥ä¸Šã®äºˆå®šã‚’ã“ãªã™ã“ã¨")

    if 'fatigue_start' in df_insight.columns and 'focus_start' in df_insight.columns:
        recovery_consecutive, recovery_single = [], []
        fatigue_times, focus_times = df_insight[df_insight['fatigue_start']].index, df_insight[df_insight['focus_start']].index
        for fat_time in fatigue_times:
            future_focus = focus_times[focus_times > fat_time]
            if len(future_focus) > 0 and future_focus[0].date() == fat_time.date():
                if 'consecutive_schedules' in df_insight.columns:
                    if df_insight.loc[fat_time, 'consecutive_schedules'] >= 2: recovery_consecutive.append(1)
                    else: recovery_single.append(1)
        if recovery_consecutive and recovery_single and (np.mean(recovery_consecutive) - np.mean(recovery_single)) > 0:
            fatigue_actions.append("äºˆå®šã‚’é€£ç¶šã—ã¦å…¥ã‚Œã‚‹ã“ã¨")

    fatigue_actions_str = "ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ç‰¹å®šã§ãã¾ã›ã‚“" if not fatigue_actions else "ã€".join(fatigue_actions)

    # --- 3ã¤ã®ã‚¿ãƒ–ã‚’ä½œæˆ ---
    tab1, tab2, tab3 = st.tabs(["ğŸ“ ãƒã‚¤ãƒ»ã‚¹ãƒšãƒƒã‚¯", "ğŸ“… ãƒãƒ³ã‚¹ãƒªãƒ¼ã‚¤ãƒ³ã‚µã‚¤ãƒˆ", "â˜€ï¸ ãƒ‡ã‚¤ãƒªãƒ¼ã‚¤ãƒ³ã‚µã‚¤ãƒˆ"])
    
    with tab1:
        st.markdown("#### ã‚ãªãŸã®é›†ä¸­ç‰¹æ€§")
        st.markdown(f"ã€€{f_dow}æ›œæ—¥ã®{f_hour}æ™‚å°ã«æœ€ã‚‚é›†ä¸­ã—ã‚„ã™ã„å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚<br>"
                    f"ã€€å¹³å‡é›†ä¸­æŒç¶šæ™‚é–“ã¯{avg_focus_duration_str}åˆ†ã§ã™ã€‚<br>"
                    f"ã€€1æ—¥ã®å¹³å‡é›†ä¸­æ™‚é–“ã¯{daily_total_focus_time_str}åˆ†ã§ã™ã€‚<br>"
                    f"ã€€1æ—¥ã«{daily_focus_count_str}å›é›†ä¸­ã¨ç·©å’Œã®ãƒªã‚ºãƒ ã‚’ç¹°ã‚Šè¿”ã—ã¦ã„ã¾ã™ã€‚<br>"
                    f"ã€€é›†ä¸­ã«å…¥ã‚Šã‚„ã™ã„è¡Œå‹•ã¯{focus_actions_str}", unsafe_allow_html=True)

        st.markdown("<br>", unsafe_allow_html=True)
        st.markdown("#### ã‚ãªãŸã®ç–²åŠ´ç‰¹æ€§")
        st.markdown(f"ã€€{fat_dow}æ›œæ—¥ã®{fat_hour}æ™‚å°ã«æœ€ã‚‚ç–²åŠ´ã—ã‚„ã™ã„å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚<br>"
                    f"ã€€ç–²åŠ´ã—ã‚„ã™ã„è¡Œå‹•ã¯{fatigue_actions_str}", unsafe_allow_html=True)

        if not focus_durations.empty:
            st.markdown("<br>##### é›†ä¸­æŒç¶šæ™‚é–“ã®åˆ†å¸ƒ", unsafe_allow_html=True)
            max_duration = int(focus_durations.max())
            max_bin = math.ceil(max_duration / 10) * 10
            bins = np.arange(0, max_bin + 20, 10) 
            counts, edges = np.histogram(focus_durations, bins=bins)
            xtick_labels = [f"{int(edges[i])}-{int(edges[i+1])-1}" for i in range(len(edges)-1)]
            
            fig_dist = go.Figure(data=[go.Bar(
                x=xtick_labels, y=counts, marker_color='#4A90E2', opacity=0.8,
                hovertemplate="é›†ä¸­æ™‚é–“: %{x}åˆ†<br>å›æ•°: %{y}å›<extra></extra>"
            )])
            fig_dist.update_layout(
                xaxis_title="é›†ä¸­æŒç¶šæ™‚é–“ (åˆ†)", yaxis_title="å›æ•°", height=300,
                margin=dict(l=20, r=20, t=20, b=20), plot_bgcolor='rgba(0,0,0,0)', bargap=0.1
            )
            st.plotly_chart(fig_dist, use_container_width=True)

        st.markdown("##### æ™‚é–“å¸¯ãƒ»æ›œæ—¥åˆ¥ã®å‚¾å‘ (ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—)")
        def plot_heatmap_plotly(target_metric, colorscale_name):
            if target_metric not in df_imp.columns: return None
            pivot_df = df_imp.pivot_table(values=target_metric, index=df_imp.index.hour, columns=df_imp.index.dayofweek, aggfunc='mean')
            heatmap_data = np.full((time_range[1] - time_range[0] + 1, 7), np.nan)
            for h in pivot_df.index:
                if time_range[0] <= h <= time_range[1]:
                    for d in pivot_df.columns:
                        if d in selected_dow_indices: heatmap_data[int(h) - time_range[0], int(d)] = pivot_df.loc[h, d]
            
            fig_hm = go.Figure(data=go.Heatmap(
                z=heatmap_data, x=dow_options, y=[f"{h}:00" for h in range(time_range[0], time_range[1] + 1)],
                colorscale=colorscale_name, hoverongaps=False, hovertemplate="æ›œæ—¥: %{x}<br>æ™‚é–“å¸¯: %{y}<br>ç¢ºç‡: %{z:.2f}<extra></extra>"
            ))
            fig_hm.update_layout(yaxis_autorange='reversed', height=350, margin=dict(l=20, r=20, t=20, b=20))
            return fig_hm

        col_h1, col_h2 = st.columns(2)
        with col_h1:
            st.markdown("**ğŸ¯ é›†ä¸­ç¢ºç‡**")
            fig_focus = plot_heatmap_plotly('é›†ä¸­åˆ¤å®š', 'Blues')
            if fig_focus: st.plotly_chart(fig_focus, use_container_width=True)
        with col_h2:
            st.markdown("**ğŸ”‹ ç–²åŠ´ç¢ºç‡**")
            fig_fatigue = plot_heatmap_plotly('ç–²åŠ´åˆ¤å®š', 'Reds')
            if fig_fatigue: st.plotly_chart(fig_fatigue, use_container_width=True)

    with tab2:
        df_ts['year_month'] = df_ts.index.to_period('M').astype(str)
        available_months = sorted(df_ts['year_month'].unique().tolist(), reverse=True)
        
        if not available_months:
            st.write("åˆ†æå¯èƒ½ãªæœˆã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            selected_month = st.selectbox("åˆ†æå¯¾è±¡ã¨ã™ã‚‹å¹´æœˆã‚’é¸æŠã—ã¦ãã ã•ã„", available_months)
            df_month = df_ts[df_ts['year_month'] == selected_month]
            
            if 'é›†ä¸­åˆ¤å®š' in df_month.columns:
                # 1æ™‚é–“å˜ä½ã§ã®é›†ä¸­å›æ•°ï¼ˆåˆ†æ•°ã«ç›¸å½“ï¼‰ã‚’é›†è¨ˆ
                df_m_1t = df_month[['é›†ä¸­åˆ¤å®š']].resample('1T').mean()
                df_m_1t['é›†ä¸­åˆ¤å®š_ãƒ•ãƒ©ã‚°'] = (df_m_1t['é›†ä¸­åˆ¤å®š'] >= 0.5).astype(int)
                
                df_m_hourly = df_m_1t.resample('1H').sum()
                df_m_hourly['day'] = df_m_hourly.index.day
                df_m_hourly['hour'] = df_m_hourly.index.hour
                df_m_hourly['dow'] = df_m_hourly.index.dayofweek
                
                # è¨­å®šã•ã‚ŒãŸæ™‚é–“å¸¯ï¼ˆtime_rangeï¼‰ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
                target_hours = list(range(time_range[0], time_range[1] + 1))
                
                # ã‚°ãƒ©ãƒ•: æ›œæ—¥åˆ¥ãƒ»æ™‚é–“å¸¯åˆ¥
                col_m1, col_m2 = st.columns(2)
                
                with col_m1:
                    dow_sum = df_m_hourly.groupby('dow')['é›†ä¸­åˆ¤å®š_ãƒ•ãƒ©ã‚°'].sum().reindex(range(7), fill_value=0)
                    fig_dow = px.bar(x=dow_options, y=dow_sum.values, labels={'x': 'æ›œæ—¥', 'y': 'é›†ä¸­åˆ¤å®šå›æ•°'}, title="æ›œæ—¥åˆ¥ é›†ä¸­åˆ¤å®šå›æ•°")
                    fig_dow.update_traces(marker_color='#4A90E2')
                    st.plotly_chart(fig_dow, use_container_width=True)
                    
                with col_m2:
                    # æ™‚é–“å¸¯ã‚’å¯¾è±¡æ™‚é–“å¸¯ã®ã¿ã«çµã‚Šè¾¼ã‚€
                    hour_sum = df_m_hourly.groupby('hour')['é›†ä¸­åˆ¤å®š_ãƒ•ãƒ©ã‚°'].sum().reindex(target_hours, fill_value=0)
                    fig_hour = px.bar(x=[f"{h}:00" for h in target_hours], y=hour_sum.values, labels={'x': 'æ™‚é–“å¸¯', 'y': 'é›†ä¸­åˆ¤å®šå›æ•°'}, title="æ™‚é–“å¸¯åˆ¥ é›†ä¸­åˆ¤å®šå›æ•°")
                    fig_hour.update_traces(marker_color='#4A90E2')
                    st.plotly_chart(fig_hour, use_container_width=True)
                
                # ã‚°ãƒ©ãƒ•: æ—¥Ã—æ™‚é–“ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
                st.markdown("#### æ—¥ä»˜Ã—æ™‚é–“å¸¯ã®é›†ä¸­åˆ¤å®šå›æ•° (èµ¤æ ã¯äºˆå®šã‚ã‚Š)")
                hm_pivot = df_m_hourly.pivot_table(index='day', columns='hour', values='é›†ä¸­åˆ¤å®š_ãƒ•ãƒ©ã‚°', aggfunc='sum').fillna(0)
                # æ¬ ã‘ã¦ã„ã‚‹æ—¥ãƒ»æ™‚é–“ã‚’è£œå®Œã—ã€å¯¾è±¡æ™‚é–“å¸¯ã®ã¿ã«çµã‚Šè¾¼ã‚€
                all_days = list(range(1, df_month.index.days_in_month[0] + 1))
                hm_pivot = hm_pivot.reindex(index=all_days, columns=target_hours, fill_value=0)
                
                fig_hm_month = go.Figure(data=go.Heatmap(
                    z=hm_pivot.values,
                    x=[f"{h}:00" for h in target_hours],
                    y=[f"{d}æ—¥" for d in all_days],
                    colorscale='Blues',
                    hovertemplate="æ—¥ä»˜: %{y}<br>æ™‚é–“å¸¯: %{x}<br>é›†ä¸­å›æ•°: %{z}<extra></extra>"
      